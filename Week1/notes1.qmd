---
title: Week 1
jupyter: python3
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
    theme: litera
---

## Syllabus and class structure

Welcome to the course *Decoding the Brain*. We will cover how to process electrical signals generated by neural activity, use machine learning approaches to infer the presentation of stimuli or an intended motor action from that activity, and evaluate the performance of these inferences.

This course is designed so that students gain a practical understanding of how to design the algorithms used for brain-computer interfaces (BCI). By the end of the course, you should understand enough to program your own BCI algorithms for non-invasive (scalp) and invasive (electrodes implanted in the brain) measures of brain activity. These activities can be weak electrical activity recorded from the scalp (EEG), high-frequency oscillations recorded on the brain surface (ECoG), and the firing of individual neurons (units). Decoding each of these types of signals requires several steps we will cover. First, you will be shown how to process and clean them for subsequent decoding. Then, we will cover the theory behind how the decoding algorithms work and code basic versions of them from scratch in using the Python packages Numpy or PyTorch. Next, we will expand their capabilities to handle more complex patterns of brain activity or decode multiple events. Finally, we will evaluate the decoder's performance.

Except for cursory discussion, we will not cover the hardware, surgical, and biocompatibility issues of recording brain activity. Those topics depend upon fundamentally different skills from the design of BCI algorithms.

The two principal algorithms covered in this course, logistic regression and naive Bayes, are useful outside of BCIs. Thus, the understanding you get of them here will apply to many other domains.

## Mixing code, theory, and practice

Lectures will introduce the theory behind what we do, practical details of how to go about doing it, and code that does the doing. The astounding thing about BCI design is how accessible the algorithms and plentiful the data sources are. Of course, a real BCI system uses specialized hardware implanted in or on a person and is run in near real-time, which is not feasible in a classroom setting.

We will use Python throughout this course, and publicly available data sets.

```{python}
# This will be the standard set of packages we import for most lectures. 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.signal as sig
from IPython.display import display, Math, Latex    
```

## General principles of BCI design

The goal of BCI is to attribute electrical activity in a subject's brain to stimuli they received or actions they intended to produce. Grounding this is the fundamental assumption in neuroscience that all we experience and do arises from patterns of activity in the brain. These patterns are manifested at multiple levels, from single neurons to large portions of the cortical surface. Every stimulus produces a chain of activities starting at peripheral receptors, ascending through chains of neurons that eventually reach the neocortex, where they are processed and elaborated, linked with your expectations, memories, and goals. At every level specific groups of neurons are activated reflecting the properties of the stimuli or your cognitive state. Sometimes these trigger actions - e.g. moving your arm - that depends on a chain of activities stretching back from the muscles to your spinal cord, then brain stem, then cortex. Remarkably, in a mouse it only takes the activation of just over a dozen neurons to [influence perception](https://elifesciences.org/articles/58889).

Given that there is a physical correlate in the brain of what we perceive and do, then it is possible to 'tap' them. The limitations are technical such as how well can be extract neural activities and our computational resources for attributing them to the external world. Much work has been spent on these problems. This is often conceptualized in two different ways.

![Encoding and decoding](./images/DecodingEncoding.png).

One is treat it as an *encoding* problem, where we try to understand how events in the environment are reflected in neural activities. The other is the *decoding* approach, where we use neural activities to infer what is happening in the environment. BCIs make extensive use of decoding analyses.

![BCI schema](./images/BCI_Schema.png)

A BCI involves measuring some activity in the brain and then transforming it so that it can be used to infer whether a certain action was intended or stimulus occurred. The transformation, referred to as the decoding step in the above figure, involves three steps. 1. Signal preprocesing: The raw neural activity measurements are cleaned and filtered to remove non-neural artifacts and highlight those aspects of the signal that reflect the processing we care about. 2. Feature extraction: Patterns are identified in the activities that are related to the events we wish to decode. 3. Classification: Based on the detected patterns, a decision is made about whether, or what kind, of event occurred.

For each type of brain signal involves different preprocessing and feature extraction steps. Generally the classification step can be used interchangeably between signal types, but we will explore two fundamentally different approaches.

## Neuroscience basics

The brain is an electrochemical machine that allows vertebrates to interact with a dynamic complex environment. It does this using specialized cells called *neurons*. They have the ability to receive chemical signals, *neurotransmitters*, that affect their electrical activity. This activity is transmitted to other neurons via an *axon*. When multiple neurons are connected together they form a network that supports the transformation of incoming activity and spontaneous generation of new activities.

### Brain anatomy

The brain is composed of numerous regions, but its cortex is generally divided into four principal areas: ![brain](./images/brain-lobes-anatomy-johnshopkins.jpg)

1.  Frontal lobe: Reasoning and motor control
2.  Parietal lobe: Touch and visual spatial processing
3.  Occipital lobe: Low level visual processing
4.  Temporal lobe: Audition, high level visual processing, memory

Keep in mind these are gross generalizations regarding the function of these areas.

![Brain cross section](./images/AnatomyCrosssection.png)

The brain is situated in the skull and is separated by a couple centimeters from the surface of the scalp. Above it lies several layers of connective tissue (pia, dura, arachnoid) and cerebrospinal fluid that act as a protective cushion. The skull itself has multiple layers, followed by fatty tissue and the skin.

### Neurons

![Neuron schematic](./images/NeuronSchematic-Hammond.png)

Neurons are the principal cell type in the brain for processing information. Neurons are tiny, \~10 microns, which is 5-10 times smaller than the diameter of a human hair. They are comprised of a soma (cell body), dendrites, and an axon. The dendrites take in signals from other neurons, integrate them, and convey them to the soma. When these signals exceed a threshold, the soma can generate an *action potential* that is conveyed down the axon towards other neurons.

### Membranes and ions

![Structure of the neuron membrane](./images/FluidMosaic-Hammond.png) Like all eukaryotic cells, neurons have a lipid membrane that seals in their internal, or *intracellular*, fluid (also known as cytoplasm) and organelles from the *extracellular* environment. This membrane also acts as an electrical insulator, blocking the flow of electrical charges across it. Electrical charge in the brain is carried by *ions* in the intracellular and extracellular fluids (cerebrospinal fluid). Ions are atoms or molecules that are either positively or negatively charged depending on whether they have a fewer or greater number of electrons than protons. The ones that are most determinative of neural activity are Na<sup>+</sup>, K<sup>+</sup>, Cl<sup>-</sup>, and Ca<sup>2+</sup>. Between the intracellular and extracellular fluids, the concentrations of these ions are different. Na<sup>+</sup>,Cl<sup>-</sup>, and Ca<sup>2+</sup> have higher concentrations in the extracellular fluid, while K<sup>+</sup> ions have a higher concentration in the intracellular fluid.

![Membrane and ions](./images/Nernst_USMLE.jpg)

```{python}
ion_props = pd.DataFrame({'Ion': ['Na', 'K', 'Cl', 'Ca'],
                          'Out_mM': [140, 3, 140, 1.5],
                          'In_mM': [7, 140, 7, 0.0001],
                          'Charge': [1, 1, -1, 2]})
ion_props.set_index('Ion', inplace=True)

print(ion_props)
```

This unequal distribution of ions across the membrane is unstable because ions tend to flow from where they are highly concentrated to where they are less concentrated. To cross the membrane, ions flow through channels, unsurprisingly called *ion channels*. These are proteins that span the inside and outside of the neuron with a central hole, known as a pore, through which the ions flow. The pore's shape and charge determine its selectivity for specific ionic species. Additional factors can control the opening of the pore. Some ion channel pores are controlled by the voltage across the membrane, others by substances such as neurotransmitters or hormones that bind to them, or even other ions like Mg<sup>2+</sup>.

![Na channel](./images/NaChannel_Sula2017.jpg)

Since ions move from high to low concentrations, Na<sup>+</sup>, Cl<sup>-</sup>, and Ca<sup>2+</sup> are inclined to flow into neurons. K<sup>+</sup>, on the other hand, wants to flow out of the neuron. As these ions flow through the membrane they build up on its surface. This create a force that opposes further flow from diffusion, because ions with the same sign to their charge tend to push away from each other. The work needed to move a charge from one place to another is measured as a *voltage* or *potential*. The potential at which an ion's electrical forces from the buildup of ions balance out the diffusion force is referred to as the *reversal* or *equilibrium potential*, E. In the case here, we are measuring potential between the inside and outside of the neuron. We can calculate the strength of the potentials that reflect these drives using the Nernst equation.

$$E = \frac{RT}{zF}ln\frac{[Out]}{[In]}$$

$R$ is the gas constant and $F$ is Faraday's constant, while $T$ is temperature in Kelvin and $z$ is the charge of the ion. $[Out]$ and $[In]$ are the extracellular and intracellular concentrations of that ion, respectively.

Most of the time when an equation is introduced, we will try to translate it into Python. A method that calculates the equilibrium potential of a given ion can be given as:

```{python}
# calculate Nernst equilibrium potential for an ion
def nernst(conco=3, conci=140, z=1, t=310):
    """Calculate Nernst potential for an ion.
    
    Parameters
    ----------
    conco : float
        Concentration of ion outside cell (mM). Default is for K+.
    conci : float
        Concentration of ion inside cell (mM). Default is for K+.
    z : int
        Charge of ion.  Default is for K+.
    t : float
        Temperature (Kelvin). Default is body temperature.
    
    Returns
    -------
    float
        Nernst potential for ion (mV).
    """
    r = 8.314 # J/mol/K, gas constant
    f = 96485 # C/mol, Faraday's constant
    return (r*t)/(z*f) * np.log(conco/conci) * 1000
```

Using this function and the dataframe of ionic concentrations above, we can calculate the equilibrium potentials for each of the major ionic species found in the nervous system.

```{python}
ion_props['E'] = ion_props.apply(lambda ion: nernst(ion['Out_mM'], ion['In_mM'], ion['Charge']), axis=1)
print(ion_props)
```

Several insights are readily apparent from cursory inspection of the calculated equilibrium potentials and consideration of the Nernst equation. First, and most importantly, positive ions that have a higher concentration outside the neuron will have positive potentials, while those more concentrated inside the neuron have negative potentials. This is stands out when comparing the potential of Na<sup>+</sup>, whose potential is 80 mV and K<sup>+</sup>, whose potential is -80 mV. Second, since we are taking the ratio between $[Out]$ and $[In]$, the overall amount of ions in our system does not affect the potential, only their relative proportion. Put another way, you could decrease the concentration of Na<sup>+</sup> ions inside and outside the neuron by ten times and as long as they have the same proportion, the potential will be unchanged. Indeed, Ca<sup>2+</sup> has a much lower overall concentration inside and outside the membrane, but its reversal potential is higher than that for Na<sup>+</sup>. Third, if an ion has a negative charge, $z$, it flips the sign of the potential. For instance, Cl<sup>-</sup> ions have a higher concentration outside the neuron, just like Na<sup>+</sup>, but their potential is negative.

The potential across a neuron's membrane is the mean of all these potentials, each weighted by how easy it is for the ion to cross the membrane, which is related to the number of open ion channels allowing that ion to flow. In a neuron at rest, mostly K<sup>+</sup> ion channels are open, so its reversal potential dominates. Neurons in the cerebral cortex usually have a resting potential around -65 mV.

## Electrical model of the neuronal membrane

The electrical behavior of the membrane can be approximated by a relatively simple circuit. While this is not an electronics course, I will briefly describe this model and use it to construct a simulated neuron. This should allow us to explore some of the response properties of neurons and how they are reflected in the electrical activities detected with electrodes.

### Equilibrium potential as a battery and conductor

The equilibrium potential, $E_{r}$, behaves like a battery. The ionic current it produces flows through open ion channels, which we will refer to as conductors. As we discussed above, when a neuron is at rest it primarily allows K<sup>+</sup> ions to flow. This passive leakage of ions across the membrane has given the conductance the name *leak conductance*. This can be schematized below as a battery and conductor in series that bridge the intracellular and extracellular spaces.

![Membrane with battery](./images/MemBatRes.png)

By itself, this circuit will fix the voltage across the neuronal membrane (measured between the inside and outside) at the resting potential. Normally neurons do not remain at rest, but have a ongoing fluctuations in their membrane voltage due to the thousands of synaptic inputs they recieve. This means that the membrane potential, $V_{m}$, can be different from $E_{r}$. When $V_{m}$ is different from $E_{r}$, there is a net flow of current across the membrane, since the electrical and diffusion forces are no longer balanced (think back to the Nernst equation). The ionic current can be described using Ohm's law:

$$ V = IR $$

where $V$ is voltage (volts, V), $I$ is current (amperes, A), and $R$ is resistance (ohms, $\Omega$). Rewriting the equation to solve for $I$ gives us: $$ I = \frac{V}{R} $$ The inverse of resistance is conductance (siemens, S), so this equation can be rewritten as: $$ I = gV $$ Here $g$ is the ease with which our conductor allows current to flow. Now, if we include the effect of the reversal potential, $E_{r}$ acting as a battery, then the equation can be written as: $$ I_{leak} = g_{leak}(V_{m}-E_{r}) \tag{1} $$ Let's consider the behavior of this equation. It shows that as we increase the ion channel's conductance, $g_{leak}$, the current will increase as well. It also shows that the magnitude and direction of the current will depend on the membrane voltage, $V_{m}$. If $V_{m}$ is below $E_{r}$, current will be negative. If it is above, then it will be positive. As $V_{m}$ approaches $E_{r}$, the current gets smaller. When they are equal, no current flows because the electrical forces perfectly balance the concentration gradient.

Let's graph these relationships between $V_{m}$ and $I_{leak}$ by systematically varying $E_{r}$ and $g_{leak}$.

```{python}
# Here are somme realistic parameters for a neuron
mem_r = 25e3 # ohm*cm^2, taken from Egger et al 2020
cell_radius = 30e-4 # cm, 30 microns, this is larger than the cell body itself to account for membrane from dendrites
cell_area = 4*np.pi*cell_radius**2 # cm^2
g_leak = (1/mem_r) * cell_area # S/cm^2
e_rest = -0.065 # V, 65 mV

# code for solving for leak current
def ionic_current(v=0, g=g_leak, e=e_rest):
    """Calculate ionic current.
    
    Parameters
    ----------
    v : float
        Membrane potential (V). Default is 0.
    g : float
        Conductance (S). Default is leak conductance.
    e : float
        Equilibrium potential (V). Default is resting potential.
    
    Returns
    -------
    float
        Ionic current (A).
    """
    return g * (v - e)
```

```{python}
# Examine how varying the conductance affects the I-V curve
v = np.linspace(-0.1, 0.1, 100)
g_factors = [0.1, 0.5, 1, 2, 5]
for ind, curr_factor in enumerate(g_factors):
    curr_g = g_leak*curr_factor
    plt.plot(v*1e3, ionic_current(v, g=curr_g)*1e12, 
             label=round(curr_g*1e12), color=[ind/len(g_factors), 0,0])
    plt.text(v[-1]*1e3+2, ionic_current(v[-1], g=curr_g)*1e12,
                str(round(curr_g*1e12))+' pS', color=[ind/len(g_factors), 0,0])
plt.xlim(-100, 135)
plt.xlabel('Membrane potential (mV)')
plt.ylabel('Ionic current (pA)')
plt.grid()
plt.title('Effect of conductance on ionic current')
```

It is evident that increasing the conductance dramatically increases the ionic current. Now let's examine the effect of equilibrium potential.

```{python}
# Examine how varying the equilibrium potential affects the I-V curve
v = np.linspace(-0.1, 0.1, 100)
e_values = [-0.070, -0.035, 0, 0.035, 0.070]
for ind, curr_e in enumerate(e_values):
    plt.plot(v*1e3, ionic_current(v, e=curr_e)*1e12, 
             label=curr_e*1e3, color=[0,0,ind/len(e_values)])
    plt.text(v[-1]*1e3+2, ionic_current(v[-1], e=curr_e)*1e12,
                str(round(curr_e*1e3))+' mV', color=[0,0,ind/len(e_values)])
plt.xlim([-100, 135])
plt.xlabel('Membrane potential (mV)')
plt.ylabel('Ionic current (pA)')
plt.grid()
plt.title('Effect of reversal potential on ionic current')
```

As we shift the value of the equilibrium potential, the voltage at which the ionic current shifts from negative to positive shifts in tandem.

By itself, this circuit does not produce any interesting temporal dynamics or integrative abilities. To begin to capture those, we need to incorporate another detail about the electrical properties of the neuronal membrane.

### Membrane as a capacitor

The neuron's membrane is conceived of as a capacitor (see [this paper](https://doi.org/10.1016/S0006-3495(00)76293-X) for more info). A capacitor is composed of an insulator sandwiched between two conductors, in our case the lipid membrane acts as the insulator since charge cannot cross it, and the extracellular and intracellular ionic solutions are the conductors. Since each charge puts out an electric field attracts the opposite charge and repels the same charge, and this field decays with distance from the charge, the thinner the insulator is the stronger charges on either side of it can influence each other. If positive charges build up on one side of the capacitor, then they will draw negative charges to build up on the opposite side. The ability of a capacitor to store charge is quantified by its capacitance:

$$ C = \frac{\epsilon\epsilon_{0}A}{d} $$

where $\epsilon$ is the dielectric constant of the insulation material, $\epsilon_{0}$ is the polarizability of free space, $A$ is the surface area of the capacitor, and $d$ is the thickness of the insulator. Capacitance is measured in the unit Farads (F). This equation tells us that to increase capacitance one should enlarge the surface area of the capacitor, allowing more space to accommodate charge, and shrink the thickness of the insulator, making it easier for charges on either side to interact.

What value does the capacitance take for neurons? Since all membranes are composed of a lipid bilayer, they do not differ in their material composition so their $\epsilon$ stays the same, and $\epsilon_{0}$ is a physical constant that does not change. The thickness of the neuronal membrane, $d$, our insulator, is also consistent across neurons, with a value of \~9 nm (for comparison, that is about eleven thousand times thinner than the thickness of printer paper). But, since neurons can vary in size, $A$ varies greatly across neurons. So, we often use 'specific capacitance', which is the ratio between capacitance and area. The specific capacitance for neurons is generally around 1 𝜇F/cm<sup>2</sup>. If we approximate a neuron as a sphere, then its capacitance can be calculated using its radius to calculate the area of the sphere ($A=4 \pi r^2$), and multiplying that by the 1 𝜇F/cm<sup>2</sup>. For instance, a neuron with a radius of 10 um has a capacitance of 12.5 pF.

### Passive electrical model of the membrane

![membrane as capacitor](./images/MemBatResCap.png)

Expanding our electrical schematic we add the membrane capacitor in parallel with the resting potential battery and conductor. This is the *passive* electrical model of the membrane. Many classic phenomena of neural integration arise from the passive electrical behavior of its membrane.

![membrane rest](./images/MemRest.png)

The battery charges up the capacitor, forcing it to adopt a potential equal to the resting potential. Once it has reached this stable state, what if we were to artificially inject a current into the neuron? To calculate this, we can use a rule from electrical theory called Kirchoff's current law (see [Kirchoff's current law](https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws#Kirchhoff's_current_law "Wikipedia link")). It states that the current flowing into a node of an electrical circuit must equal the current flowing out of it. We already have an equation (Ohm's law) that describes the current produced by our ionic leak current, now we need to know how to calculate the current produced by the capacitor.

The relationship between current and voltage for a capacitor is described by the equation:

$$ I = C\frac{dV}{dt} \tag{2}$$

What this means is that the current produced across the capacitor is proportional to the change in voltage.

![membrane charging with resistance](./images/MemCharging.png)

The passive electrical circuit model is also known as an *RC circuit*, because it contains a resistor and capacitor. What is its behavior? When it is first put together, there is no voltage across the capacitor, so the battery will charge it up. However, the resistor places a limit on the current, meaning the capacitor will not fully charge instantly. To determine the trajectory the voltage will take, we can model this with an equation by combining equations 1 and 2 using Kirchoff's current law. For the circuit above, this yields the equation: $$ \begin{align} 
    \notag
    0 &= I_{rest} + I_{C} \\ \notag
    0 &=g_{m}(V_{m}-E_{rest}) + C_{m}\frac{dV_{m}}{dt} \\ \notag
    -C\frac{dV_{m}}{dt}&=g_{m}(V_{m}-E_{rest}) \\ \notag
    \frac{dV_{m}}{dt}&=-\frac{g_{m}}{C}(V_{m}-E_{rest})
    \end{align}
$$

This is a differential equation that can be solved to give voltage as a function of time: $$ V_{m}(t) = E_{rest}(1-e^{\frac{-t}{\frac{g_{m}}{C_{m}}}}) $$

Rewriting $g_{m}$, as its inverse, referred to as membrane resistance, $R_{m}$, we get the equation: $$ V_{m}(t) = E_{rest}(1-e^{\frac{-t}{R_{m}C_{m}}}) \tag{3}$$

The product of $R_{m}$ and $C_{m}$ sets how fast the membrane charges. Larger it is, the slower the membrane capacitor will charge, and the smaller it is, the faster it charges. Given realistic values of $R_{m}$ and $C_{m}$, what would be a reasonable time constant to expect from a neuron?

```{python}
# Calculate membrane resistance and capacitance
mem_cap = 1e-6 # F/cm^2
cell_c = mem_cap * cell_area # F
cell_r = mem_r / cell_area # ohm

cell_tau = cell_r * cell_c # sec

print('Membrane capacitance: {:.0f} pF'.format(cell_c*1e12))
print('Membrane resistance: {:.0f} MOhm'.format(cell_r*1e-6))
print('Membrane time constant: {:.0f} ms'.format(cell_tau*1e3))
```

```{python}
# Plot examples of V changing over time for different RC time constants
tau_factors = [0.1, 0.5, 1, 2, 5]
t = np.linspace(0, 0.1, 100)
for ind, curr_factor in enumerate(tau_factors):
    curr_tau = cell_tau*curr_factor
    plt.plot(t*1e3, e_rest*(1-np.exp(-t/curr_tau))*1e3, 
             label=curr_tau*1e3, color=[0,0,ind/len(tau_factors)])
    plt.text(t[-1]*1e3+2, e_rest*(1-np.exp(-t[-1]/curr_tau))*1e3,
                str(round(curr_tau*1e3,2))+' ms', color=[0,0,ind/len(tau_factors)])
    
    
plt.xlim([-5, 125])
plt.xlabel('Time (ms)')
plt.ylabel('Membrane potential (mV)')
plt.grid()
plt.title('Effect of RC time constant on membrane potential charging')
```

What this shows is that longer membrane time constants, $\tau$, slow the rate of change in the membrane potential.

### Creating a passive neuron model

Equation 3 showed us the relationship between time and membrane potential at the moment the membrane capacitance was connected with the leak current. This is, of course, not realistic. Instead, neurons start at the resting potential and recieve occasional inputs that inject currents into the neuron. To capture this, we can use the following equation:

$$ \begin{align} 
    \notag
    0 &= I_{rest} + I_{C} + I_{in}\\ \notag
    0 &=g_{m}(V_{m}-E_{rest}) + C_{m}\frac{dV_{m}}{dt} + I_{in}\\ \notag
    -C_{m}\frac{dV_{m}}{dt}&=g_{m}(V_{m}-E_{rest}) + I_{in}\\ \notag
    \frac{dV_{m}}{dt}&=-\frac{1}{C_{m}}({g_{m}}(V_{m}-E_{rest}) + I_{in})  \tag{4}
    \end{align}
$$

Since we know how the membrane voltage will change, $\frac{dV_{m}}{dt}$, based on its present value, $V_{m}$, and the current injected, $I_{in}$, we can simulate its response. To do this, we will create a class, `PassiveNeuron`. This class will encapsulate the data and methods needed to simulate the passive properties of a neuron's membrane.

```{python}
class PassiveNeuron:
    def __init__(self, v_rest=-65, c_m=1, r_m=25, radius=30):
        self._vrest = v_rest/1000 # mV
        self._cm = c_m * 1e-6 # uF/cm^2
        self._rm = r_m * 1e3 # kOhm*cm^2
        self._radius = radius * 1e-4 # cm

        self._area = 4 * np.pi * self._radius**2 # cm^2
        self._c = self._cm * self._area # F  
        self._gleak = 1 / (self._rm / self._area) # S

        self._vm = self._vrest
        self._im = 0
        self._add = 0 # holds additional current to deliver to the neuron
        self._dt = 0.0001 # sec

    def get_tau(self):
        return self._cm * self._rm
    
    def set_input(self, inp=0):
        self._add = inp
    
    def reset_state(self):
        self._vm = self._vrest
        self._im = 0
        self._add = 0 # holds additional current to deliver to the neuron
    
    def get_state(self):
        # return membrane potential in mV, membrane current in nA, and spike status
        return self._vm*1e3, self._im*1e9
    
    def get_t_vec(self, dur = 0.1, dt = 0.0001):
        return np.arange(0, dur, dt)
    
    def update(self):
        # solve for transmembrane currents
        self._im = (self._gleak * (self._vm - self._vrest)) + self._add
        
        # update membrane potential
        self._vm = self._vm + -(self._im / self._c) * self._dt
    
    def run(self, dur=0.1, dt=0.0001, inp=0):

        self.reset_state() # reset state
        
        # initialize arrays to store values
        t = self.get_t_vec(dur, dt) # time array
        v = np.zeros(len(t)) # voltage array
        i = np.zeros(len(t)) # current array
        self._dt = dt # set time step

        # if input is scalar, make it an array
        if isinstance(inp, (int, float)):
            inp = np.ones(len(t)) * inp
        
        # run simulation
        for ind, _ in enumerate(t):
            self.set_input(inp[ind]) # set input current
            self.update() # update membrane potential and current
            v[ind], i[ind] = self.get_state() # store values

        return v, i

```

```{python}
# Create a neuron with default parameters
pas_nrn = PassiveNeuron()
sim_dur = 0.2 # sec
sim_dt = 0.0001 # sec
t_pts = pas_nrn.get_t_vec(dur=sim_dur, dt=sim_dt)

# Create a step current input
def step_current(t, start=0.025, end=0.125, amp=-1e-10):
    step = np.zeros(len(t))
    step[np.where((t>=start)&(t<end))[0]] = amp
    return step

in_step = step_current(t_pts)
pas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)

def plot_sim(t, v, i, inp, title=''):
    plt.subplot(3,1,1)
    plt.plot(t*1e3, v, color='b')
    plt.yticks(color='b')
    plt.ylabel('mV', color='b')
    plt.grid()
    plt.title('Membrane potential')

    plt.subplot(3,1,2)
    plt.plot(t*1e3, i, color='r')
    plt.yticks(color='r')
    plt.ylabel('nA', color='r')
    plt.title('Membrane current')
    plt.grid()

    plt.subplot(3,1,3)
    plt.plot(t*1e3, inp*1e9, color='k')
    plt.xlabel('Time (ms)')
    plt.yticks(color='k')
    plt.ylabel('nA', color='k')
    plt.title('Input current')
    plt.grid()
    plt.suptitle(title)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

plot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron')
```

When we inject a current step into the neuron, the membrane begins to depolarize from its resting potential towards a new voltage. This is accompanied by an strong current crossing the membrane, which reflects the injected current charging up the membrane capacitance. As the membrane capacitance charges up, this current decays because it is offset by the leak current, which flows in the opposite direction of the injected current. The leak current is driven by the difference between $V_{m}$ and $E_{r}$. For the leak current to balance out the injected current, the membrane potential must reach:

$$ V_{m} = \frac{I_{in}}{g_{leak}}+E_{r}$$

Thus, a increasing $g_{leak}$ (or lowering membrane resistance, $R_{m}$), lowers the change in membrane potential to an injected current. We can see this here:

```{python}
pas_nrn_hig = PassiveNeuron(v_rest=-65, c_m=1, r_m=5, radius=30) # we lowered the membrane resistance, r_m, which increased the membrane conductance, g_m
pas_v_hig, pas_i_hig = pas_nrn_hig.run(dur=sim_dur, dt=sim_dt, inp=in_step)
plot_sim(t_pts, pas_v_hig, pas_i_hig, in_step, title='Passive neuron with higher $g_{leak}$')

# Get values of g_leak for each model
print('g_leak for regular model: {:.0f} nS'.format(pas_nrn._gleak*1e9))
print('g_leak for high g model: {:.0f} nS'.format(pas_nrn_hig._gleak*1e9))
```

Increasing $g_{leak}$ lowered the change in membrane potential from around 12 mV to 3 mV. Notice also that $V_{m}$ changes much faster now, because the membrane time constant is shorter.

Increasing the amount of current we inject also increases the change in $V_{m}$.

```{python}
in_step = step_current(t_pts, amp=-2e-10) # increase input current by 2x
pas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)
plot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron with $2xI_{in}$')
```

So far we have been giving negative current injections, which pushes the membrane towards positive values. If $V_{m}$ moves towards positive values, we say it is *depolarizing*. By contrast, if it is pushed towards negative values, it is *hyperpolarizing*. To hyperpolarize, we inject a positive current.

```{python}
# hyperpolarize the membrane potential
in_step = step_current(t_pts, amp=1e-10)
pas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)
plot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron with being hyperpolarized')
```

All the activity we have seen so far is slow changes in membrane potential. These are not conveyed to downstream neurons. Instead, neurons have to fire *action potentials* to influence their neighbors.

### Action potentials

Action potentials are large positive excursions of the membrane potential. They last less than 2 ms and propagate from the cell body down the axon, eventually triggering release of neurotransmitter at the synapse. They are the principal output of neurons and, for the most part, the only information about the state of a neuron that is conveyed to the rest of the network.

![membrane with action potential](./images/MemAPGeneration.png)

To generate an action potential, a neuron is first depolarized to a sufficient level that starts to open voltage-gated Na<sup>+</sup> ion channels. Recalling that Na<sup>+</sup> has a positive reversal potential, their opening will depolarize the membrane voltage. This depolarization recruits more voltage-gated Na<sup>+</sup> ion channels, reinforcing the depolarization and driving the membrane potential towards the Na<sup>+</sup> reversal potential (\~40 mV). Then, these channels begin to *inactivate*, closing and bringing the membrane back towards its resting potential. This fall back towards rest is accelerated by voltage-dependent K<sup>+</sup> channels, which open during the depolarization of the action potential and drive the membrane towards the K<sup>+</sup> reversal potential (\~-70 mV). During this period, the neuron enters a *refractory period*. The first phase of this is absolute, where the neuron cannot fire an action potential due to the inactivation of Na<sup>+</sup> channels. This is followed by a relative period, where the an action potential can be generated again, but the K<sup>+</sup> current counteracts this, effectively increasing the amount of current required to cross the voltage threshold.

The voltage-gated Na<sup>+</sup> and K<sup>+</sup> currents are similar in principal to the leak current discussed earlier. They are a battery, the equilibrium potential of Na<sup>+</sup> or K<sup>+</sup>, in series with a conductor. The difference is that the conductor depends on $V_{m}$ and time. We can expand our electrical model of the passive neuron to incorporate these voltage-gated currents, thus allowing us to model action potentials.

The equation that captures this is:

$$ \frac{dV_{m}}{dt}=-\frac{1}{C}(g_{m}(V_{m}-E_{rest}) + g_{Na_{V,t}}(V_{m}-E_{Na}) + g_{K_{V,t}}(V_{m}-E_{K}) + I_{in}) \tag{5} $$

Here $g_{Na_{V,t}}$ is the voltage and time dependent conductance for Na<sup>+</sup>, and similar for $g_{K_{V,t}}$. The driving force for those currents depends on how far $V_{m}$ is from the respective equilibrium potentials of those ions. In essence, increasing $g_{Na_{V,t}}$ or $g_{K_{V,t}}$ pulls $V_{m}$ towards the corresponding equilibrium potential.

::: callout-note
## A note about coding and neuronal firing

Action potentials are often described as all-or-none events. Either they occur or they do not, and when they do, they tend to always have the same strength and duration. Since they are stereotyped, it is thought that they convey information by their timing. Several different terms are often used to describe the codes offered by action potentials. A *time code* is when the exact time of an action potential indicates the occurrence of some event (whether it be environmental or behavioral). This is often counterposed to a *rate code*, where the informational signal is how many action potentials occurred in some short amount of time, with a higher rate indicating a 'stronger' signal. Alternatively, a *population code* uses the set of neurons firing action potentials at any given moment to encode information. Different ensembles of activated neurons signal different events.
:::

### Creating a simplified active neuron model

When we include voltage-gated ion channels in a model neuron, it goes from being a passive model to an *active model*. We can build upon our existing `PassiveNeuron` class to create an `ActiveNeuron` class that generates an approximation of action potentials. To do this, we will add the voltage-dependent Na<sup>+</sup> and K<sup>+</sup> currents. This requires specifying the equilibrium potential for those ions, a $V_{m}$ threshold that initiates the action potential, and designing voltage- and time-dependent conductances to gate the active currents.

```{python}
# create active neuron class that inheriting from the passive neuron model
class ActiveNeuron(PassiveNeuron):
    def __init__(self, v_thresh=-50, ena=50, ek=-90, gna=8e-8, gk=4e-8, **kwargs):
        super().__init__(**kwargs) # allows us to pass arguments for the passive properties of the neuron
        self._vthresh = v_thresh / 1000 # mV
        self._ena = ena / 1000 # mV
        self._gna = gna # S
        self._ek = ek / 1000 # mV
        self._gk = gk # S
        self._spk_timer = 0
        self._spk = False
    
    def reset_state(self):
        super().reset_state()
        self._spk_timer = 0
        self._spk = False

    def gen_ap(self):
        # action potential mechanism
        if (self._vm > self._vthresh) & (self._spk_timer <= 0):
            self._spk_timer = 0.004 # start countdown timer for duration of action potential
            self._spk = True
        elif self._spk_timer > 0.003: # open up sodium conductance for first 1 ms
            self._add = self._add + self._gna * (self._vm - self._ena)
            self._spk = False
            self._spk_timer -= self._dt
        elif self._spk_timer > 0: # open up potassium conductance for next 3 ms
            self._add = self._add + self._gk * (self._vm - self._ek)
            self._spk = False
            self._spk_timer -= self._dt

    def run(self, dur=0.1, dt=0.0001, inp=0):

        self.reset_state() # reset state
        
        # initialize arrays to store values
        t = self.get_t_vec(dur, dt) # time array
        v = np.zeros(len(t)) # voltage array
        i = np.zeros(len(t)) # current array
        self._dt = dt # set time step

        # if input is scalar, make it an array
        if isinstance(inp, (int, float)):
            inp = np.ones(len(t)) * inp
        
        # run simulation
        for ind, _ in enumerate(t):
            self.set_input(inp[ind]) # set input current
            self.gen_ap() # generate action potential <-- NEW
            self.update() # update membrane potential and current
            v[ind], i[ind] = self.get_state() # store values

        return v, i
```

```{python}
# Create an active neuron and deliver a subthreshold and over threshold current
act_nrn = ActiveNeuron()
sim_dur_act = 0.5
t_pts = act_nrn.get_t_vec(dur=sim_dur_act, dt=sim_dt)
in_step_act = step_current(t_pts, start=0.1, end=0.2, amp=-5e-11) + \
                step_current(t_pts, start=0.3, end=0.4, amp=-1e-10)

act_v, act_i = act_nrn.run(dur=sim_dur_act, dt=sim_dt, inp=in_step_act)
plot_sim(t_pts, act_v, act_i, in_step_act, title='Active neuron with subthreshold and overthreshold current')
```

For this model we set the action potential threshold to -50 mV, and if we depolarize below that level we get the passive subthreshold depolarization that we saw in the previous model. If the current step is increased, then the membrane depolarizes passively until the threshold is reached, at which point an action potential is triggered. The action potential voltage approaches the equilibrium potential for Na<sup>+</sup>, and then quickly reverses towards the equilibrium potential of K<sup>+</sup>. The transmembrane currents associated with the action potential are substantially greater than those from the subthreshold depolarization. This is due to the very high conductance of the voltage-gated channels (Na<sup>+</sup> channel: 80 nS vs. $g_{leak}$: 5 nS).

More than one action potential is emitted during the 100 ms over-threshold current step. The time between action potentials depends on two factors. First is the refractory period, with a stronger or longer refractory period delaying the time to the next action potential. The other is the strength of the injected current, with a stronger current able to overcome the refractory effect and elicit another action potential sooner. We can explore the effect of current strength by delivering a series of current pulses to our model with increasing amplitudes.

```{python}
# Examine how firing rate varies with curent injected
act_nrn = ActiveNeuron()
sim_dur_act = 1.1
t_pts = act_nrn.get_t_vec(dur=sim_dur_act, dt=sim_dt)

# create a series of step currents with increasing amplitudes
in_step_act = step_current(t_pts, start=0.1, end=0.2, amp=-5e-11) + \
                step_current(t_pts, start=0.3, end=0.4, amp=-8e-11) + \
                step_current(t_pts, start=0.5, end=0.6, amp=-1.6e-10) + \
                step_current(t_pts, start=0.7, end=0.8, amp=-2.4e-10) + \
                step_current(t_pts, start=0.9, end=1.0, amp=-3.2e-10)

act_v, act_i = act_nrn.run(dur=sim_dur_act, dt=sim_dt, inp=in_step_act)
plot_sim(t_pts, act_v, act_i, in_step_act, title='Reponse to increasing current steps')
```

Increasing the current step amplitude increased the number of action potentials emitted, and shortened the time between them. Thus, the rate of action potential generation is proportional to the excitatory drive the neuron receives.

However, it is important to remember that neurons are not normally driven by artificial current steps. Instead, they have synapses that are driven by the action potentials from other neurons.

### Synapses

Since neurons form networks that share electrical signals, there must be a means for these signals to be passed from one neuron to another. This exchange occurs at synapses, where the axon from the *presynaptic* neuron forms a terminal on the dendrite of the *postsynaptic* neuron. Between them is a narrow space called the *synaptic cleft*, where neurotransmitters released by the presynaptic terminal crosses to binds to ion channels on the postsynaptic neuron. When the neurotransmitter binds it opens an ion channel, which allows a current composed of the ions that channel is permeable to to flow. Channels that are found at excitatory synapses, which drive the membrane potential towards positive values (depolarize), tend to be permeable positive ions, Na<sup>+</sup>, K<sup>+</sup>, and Ca<sup>2+</sup>. Inhibitory synapses rely on ion channels selective for the negative Cl<sup>-</sup> ion and can counteract the excitatory depolarization.

Cortical neurons are normally bombarded by an ongoing stream of excitatory and inhibitory synaptic activity. Usually these are balanced, so that the membrane potential shows only small changes in its level. However, if the excitatory synaptic drive overwhelms the inhibition it can push the membrane potential towards positive values and potentially trigger an action potential.

The ion channels that support synaptic transmission can be incorporated in our circuit model as just another branch, with an equilibrium potential reflecting their ionic permeability and a conductance that depends on number of open ion channels.

![Membrane with synapse](./images/MemBatResCapActSyn.png)

As for the leak and action potential related currents, the ionic current arising from synaptic transmission has the form: $$ I_{syn} = g_{syn}r(V_{m}-E_{syn})$$

Here $g_{Syn}$ is the total possible conductance, and $r$ is the proportion of synaptic ion channels that are open. This depends on the presence of neurotransmitter in the synaptic cleft and time, and can be modeled as a reaction system, with separate rates describing the binding and dissociating of neurotransmitter to the receptor.

::: callout-note
## Modeling a synaptic current

The shape of the ionic current arising from activating a synapse is described as having an $\alpha$- or double exponential shape. Our passive neuron model has a single exponential shape: there is only one time constant, $\tau$, that describes the timing of its rise and fall in response to a current input. For a double exponential, there are two time constants, one for the rise and a different one for the fall. The time constant for the rising phase is faster than that for the falling phase, leading to a skewed response. The differential equation that captures this is:

$$ \frac{dr}{dt} = \alpha[T]r - \beta(1-r) $$

The equation states that the change in the proportion of open ion channels, $r$, increases at a rate $\alpha$ when neurotransmitter, $[T]$, is present in the synapse. When neurotransmitter is absent from the synapse, $r$ then decreases at the rate of $\beta$. (For more details, see [Destexhe, Mainen & Sejnowski 1994](https://pubmed.ncbi.nlm.nih.gov/8792231/)).
:::

We can incorporate synapses into our active model to explore they affect the membrane potential and drive action potentials.

```{python}
# create an active neuron with synapse class that inherits from the active neuron model
class SynapticNeuron(ActiveNeuron):
    def __init__(self, esyn=50, gsyn=5e-9, asyn=900, bsyn=500, tdur=3, **kwargs):
        super().__init__(**kwargs) # allows us to pass arguments for the passive properties of the neuron
        self._esyn = esyn / 1000 # mV
        self._gsyn = gsyn # S
        self._asyn = asyn 
        self._bsyn = bsyn
        self._tdur = tdur / 1000 # ms, duration of transmitter release
        self._syn_timer = 0 # timer for transmitter release
        self._r = 0 # fraction of open channels

    def reset_state(self):
        super().reset_state()
        self._syn_timer = 0 # timer for transmitter release
        self._r = 0 # fraction of open channels

    def gen_syn(self, prespk=False):
        # synaptic mechanism
        if prespk:
            self._syn_timer = self._tdur
        elif self._syn_timer > 0:
            self._syn_timer -= self._dt
        
        # update fraction of open channels
        self._r = self._r + self._dt * ((self._asyn * (self._syn_timer>0) * (1-self._r)) \
                                        - (self._bsyn * (self._r)))
        
        # add synaptic current
        self._add = self._add + self._gsyn * self._r * (self._vm - self._esyn)

    def run(self, dur=0.1, dt=0.0001, inp=0, prespk=False): # added prespk to drive synapse

        self.reset_state() # reset state
        
        # initialize arrays to store values
        t = self.get_t_vec(dur, dt) # time array
        v = np.zeros(len(t)) # voltage array
        i = np.zeros(len(t)) # current array
        self._dt = dt # set time step

        # if input is scalar, make it an array
        if isinstance(inp, (int, float)):
            inp = np.ones(len(t)) * inp

        if isinstance(prespk, (bool, int, float)):
            prespk = np.ones(len(t)) * prespk
        
        # run simulation
        for ind, _ in enumerate(t):
            self.set_input(inp[ind]) # set input current
            self.gen_syn(prespk[ind]) # generate synaptic response <-- NEW
            self.gen_ap() # generate action potential
            self.update() # update membrane potential and current
            v[ind], i[ind] = self.get_state() # store values

        return v, i
```

```{python}
# MODIFY PLOTTING CODE HERE
def plot_sim_prespk(t, v, i, inp, spk, title=''):

    # ensure neuron inputs have same length as time vector
    if isinstance(inp, (int, float)):
        inp = np.ones(len(t)) * inp
    
    if isinstance(spk, (bool, int, float)):
        spk = np.zeros(len(t))
    
    plt.subplot(2,1,1)
    plt.plot(t*1e3, v, color='b')
    plt.vlines(t[spk==1]*1e3, plt.ylim()[0], plt.ylim()[1], color='k')
    plt.yticks(color='b')
    plt.ylabel('mV', color='b')
    plt.grid()
    plt.title('Membrane potential')

    plt.subplot(2,1,2)
    plt.plot(t*1e3, i, color='r')
    plt.vlines(t[spk==1]*1e3, plt.ylim()[0], plt.ylim()[1], color='k')
    plt.yticks(color='r')
    plt.ylabel('nA', color='r')
    plt.title('Membrane current')
    plt.grid()

    plt.suptitle(title)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

# Function for generating spike trains
def spk_train(t, spk_times=0.1):
    spk_times = np.array([spk_times]) # ensures spk_times is a numpy array
    spk = np.zeros(len(t))
    spk[np.round(spk_times / sim_dt).astype(int)] = 1
    return spk
```

```{python}
# examine response to a single presynaptic spike
syn_nrn = SynapticNeuron()
sim_dur_syn = 0.2
t_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)

in_prespk = spk_train(t_pts, [0.05])

syn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)
plot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to a single presynaptic spike')
```

Activating the synapse at 50 ms leads to a rapid negative current that charges the membrane capacitance and depolarizes $V_{m}$. This is soon followed by a decay in the synaptic current, as the proportion of synaptic ion channels that were open shrinks. Within \~10 ms the synaptic current is finished, and replaced by a positive leak current, which is the membrane returning to its resting potential. The rate of this return to rest is determined by the membrane time constant.

A single synaptic input is usually insufficient to drive an action potential. Typically multiple synaptic events have to be combined in close temporal proximity. We can explore this by delivering two presynaptic spikes and varying the time between them.

```{python}
# delivery multiple presynaptic spikes with varying inter-spike intervals
sim_dur_syn = 1.1
t_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)

in_prespk = spk_train(t_pts, [0.1, 0.15, 0.4, 0.425, 0.7, 0.7125, 1.0, 1.00625])

syn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)
plot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to multiple presynaptic spikes')
plt.gcf().set_size_inches((12, 6))
plt.gcf().axes[0].axhline(syn_nrn._vthresh*1e3, color='k', linestyle='--')
```

The dashed line for the membrane potential plot is the action potential threshold. A single synaptic response is not sufficient to cross this threshold. If two are delivered, they must be close enough together, less than 25 ms between them, to trigger an action potential. Note also that making them closer does not increase the number of action potentials, only one is generated. To elicit multiple action potentials would require a sustained train of presynaptic spikes, such as below:

```{python}
# deliver a train of presynaptic spikes at 40 hz
sim_dur_syn = 0.5
t_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)

in_prespk = spk_train(t_pts, np.arange(0.05, 0.4, 1/40))

syn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)
plot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to a 40 Hz spike train')
```

Delivering a train of presynaptic spikes at 40 Hz elicits a steady stream of action potentials in the postsynaptic neuron. Not every presynaptic spike elicits an action potential, instead they have to summate. In this case, it took 3 presynaptic spikes to elicit the first action potential, and subsequently every 2 spikes drove an action potential.

## Detecting the activity of neurons

So far, we have been tracking neural activity by measuring the voltage and currents across the membrane. Experimentally, this is done using fine glass pipettes that are inserted into or make a hole in single neurons. This approach, known as *intracellular* or *whole-cell patch recording*, is not practical in humans, and exceptionally difficult in awake behaving animals. Instead, we typically use metal electrodes that do not need to be in direct contact with the neurons whose activity we want to detect. These electrodes pick up weak electrical signals emanating from nearby neurons, a technique referred to as *extracellular recording*. How does this work?

### Physics of extracellular potentials

As we discussed above, the currents flowing across the membrane are composed of ions. Each ion has a charge, and charge produces an electric field that exerts a force on other charges. The force required move a charge through an electric field can be picked up as a potential on an electrode. Since the neuronal lipid membrane is an insulator, our electrode can only pick up the movement of charges across the membrane. If positive charges flow into the neuron, that is a net negative of charge on the outside an a negative potential results. If positive charges flow out of the neuron, that is a negative positive of charge on the outside, causing a positive potential.

![Extracellular field of a cat motor neuron firing an action potential (numbers are in millivolts). From [Rall 1962](https://pmc.ncbi.nlm.nih.gov/articles/PMC1366481/?page=1).](images/Rall1962.png){width="441"}

Thus, the currents flowing through the membrane during synaptic barrages or action potentials can be detected by nearby electrodes. Fortunately for us, these transmembrane currents are so weak and so slow that we can use a simple equation to describe their influence on the voltage picked up by a nearby electrode. This is given by the equation:

$$ V = \frac{1}{4\pi\sigma}\frac{I}{d} $$

Here $\sigma$ is the conductivity of neural tissue, $I$ is a transmembrane current, and $d$ is the distance between the electrode and the current. A simplifying assumption we make when using this equation is that the the conductivity of the brain is homogeneous (sometimes referred to as being Ohmic). A few trends are clear from this equation. The influence that a transmembrane current has on an electrode will only depend on the the distance the electrode has from the current, and the current's sign and magnitude. The further a current is from the electrode, the weaker it gets, with the greatest fall off happening at short distances. If a current is flowing into the neuron (excitatory), then it will produce a negative voltage at the electrode, while a current flowing out of the neuron (inhibitory) produces a positive voltage. The magnitude of the voltage signal will be linearly proportional to the voltage, e.g. doubling the current will double the voltage.

There are multiple current sources in the brain. Every open ion channel is a potential current source and these carpet the entire neuronal membrane. To capture their collective influence on a recording electrode, we can treat each as producing their own voltage and add them together.

$$ V = \frac{1}{4\pi\sigma}\sum_{i=0}^{n}\frac{I_i}{d_e - d_i} \tag{6}$$

In this equation we sum the resulting voltages from all currents, with each current $I_i$ divided by its distance from the recording electrode, $d_e - d_i$. Stronger currents will tend to dominate over weaker ones. However, if there are a large number widely distributed of weak currents with similar waveforms then when added together they can produce a prominent potential.

### Modeling the relationship between electrode distance and recorded potential

Using our `SynapticNeuron` model, we can explore how different types of neural activity are detected by extracellular electrodes. This model will simulate a collection of neurons arranged in a circular sheet. To capture the kinds of activities observed in the brain, we will include a slow global fluctuation in membrane potential across all the neurons (`slow_noise`), noise that is unique to each neuron (`noise`), and a pulse of synaptic drive (`mean_t` and `std_t`). It will also require a method to calculate the extracellular potential, using equation 6 (`calc_extracell`).

::: callout-note
## A caveat to our extracellular recording model

The extracellular recording model we implement below is technically incorrect. Recall that Kirchoff's current law requires the total current flowing into a point in a circuit to equal the amount flowing out. For our extracellular model, we measure the current flowing across the membrane on the extracellular side, which is composed of all the passive, action potential, and synaptic currents, which must sum to 0. This means that the total current flowing across the membrane in our model is 0, which would produce no extracellular field given equation 6. So, why do we detect extracellular potentials in the real world? This is because neurons are composed of more than just their cell body. They also have dendrites and axons. This allows currents to flow not only across the membrane, but also within the neuron itself. Thus, the total current flowing across a single segment of the membrane does not have to balance, since part of that current will flow inside the neuron to another part of the neuron, and eventually flow out of the membrane in a different segment. If you want to realistically model the extracellular field, this requires your neuron model to have multiple compartments that are connected together, each modeled using the formalism we use here. In contrast, our model is a *single compartment model*. To get an extracellular field, we will just measure the leak current, which responds to all the other currents crossing the membrane.
:::

```{python}
class ExtracellRec():
    # class for extracellular recording of a sheet of neurons using the SynapticNeuron class
    # user can set the radius of the sheet, the density of neurons, the mean time and standard deviation 
    # around when a synaptic input arrives, the level of a shared slow input noise, and an individual gaussian noise
    # level for each neuron, electrode distance from the sheet
    def __init__(self, radius=2, density=1000, mean_t=0.1, std_t=0.005, slow_noise=25, \
                 noise=200, extra_cond=0.3, **kwargs):
        self._radius = radius # cm
        self._density = density # neurons per cm^2
        self._mean_t = mean_t # sec
        self._std_t = std_t # sec
        self._slow_noise = slow_noise * 1e-12 # slow shared noise standard deviation in pA
        self._noise = noise * 1e-12 # individual neuron gaussian standard deviation in pA
        self._extra_cond = extra_cond # extracellular conductivity in S/m
        self._v = [] # voltage array
        self._i = [] # current array
        self._t = [] # time array

        # calculate number of neurons
        self._n_neurons = int(np.round(np.pi * self._radius**2 * self._density))

        # calculate positions of neurons, uniformly distributed in the circular sheet
        # place each neuron by setting a random angle and radius from the center
        self._neuron_pos = np.zeros((self._n_neurons, 2))
        for ind in range(self._n_neurons):
            curr_ang = np.random.rand() * 2 * np.pi
            curr_rad = np.sqrt(np.random.rand()) * self._radius
            self._neuron_pos[ind, 0] = np.cos(curr_ang) * curr_rad
            self._neuron_pos[ind, 1] = np.sin(curr_ang) * curr_rad
        # force one neuron to be at the center, so it can be easily picked up by the electrode
        self._neuron_pos[0, :] = 0

        # create neurons
        self._neurons = []
        for ind in range(self._n_neurons):
            self._neurons.append(SynapticNeuron(**kwargs))

    # method to calculate extracellular potential based on currents from each neuron
    def calc_extracell(self, h=1):
        dists = np.sqrt(np.sum(self._neuron_pos**2,axis=1)+h**2)
        return 1/(4*np.pi*self._extra_cond) * np.sum((self._i.T * 1e-9) / dists, axis=1)


        
    # create run function that will run all neurons for a given duration
    def run(self, dur=0.2, dt=0.0001, seed=47):
        # set random seed
        np.random.seed(seed)

        # initialize arrays to store values
        t = self._neurons[0].get_t_vec(dur, dt) # time array
        num_t = len(t)
        v = np.zeros((self._n_neurons, num_t)) # voltage array
        i = np.zeros((self._n_neurons, num_t)) # current array
        

        # create synaptic inputs
        in_prespk = []
        for ind in range(self._n_neurons):
            in_prespk.append(spk_train(t, np.random.normal(self._mean_t, self._std_t, 1)))
        
        # create slow noise input
        slow_noise = np.random.normal(0, 1, num_t)
        slow_noise = sig.detrend(np.cumsum(slow_noise))
        slow_noise = (slow_noise/(np.std(slow_noise))) * self._slow_noise

        # create individual noise inputs
        indiv_noise = np.random.normal(0, self._noise, (self._n_neurons, len(t)))

        # run simulation
        for ind, curr_nrn in enumerate(self._neurons):
            inp_sig = slow_noise + indiv_noise[ind, :]
            v[ind, :], i[ind, :] = curr_nrn.run(dur=dur, dt=dt, inp=inp_sig, prespk=in_prespk[ind])

        # save simulation results
        self._t = t
        self._v = v
        self._i = i
```

Now that our simulation class is all setup, let's use it. We will create a sheet of neurons with a radius of 2 cm and 1000 neurons per cm<sup>2</sup>. Using `**kwargs`, we are also able to pass arguments to `SynapticNeuron`, to configure its electrophysiological properties. We will take advantage of this by increasing the synaptic conductance, `gsyn`, so that action potentials are more reliably driven. The timing of the presynaptic spikes is set by a normal distribution, with the mean time of their emission at 100 ms, and a standard deviation of 5 ms. Since the inputs to our model neurons are generated randomly, and we want to replicate our results, we will fix the random seed of the model to ensure that the same output is generated each time we run it.

```{python}
test = ExtracellRec(gsyn=7e-9)
test.run(seed=41)
```

That is all it took to run our simulation. The more neurons we include, either by increasing the size of our sheet with the `radius` parameter or their packing with the `density` parameter, longer it takes to run. Once the simulation is finished running, we can calculate the extracellular potential for electrodes at different distances from the center of the model. We will choose a couple key values that are relevant to the recordings we will perform. An electrode on the scalp would be \~2 cm away from the surface of the brain. Intracranial surface electrodes that rest on the brain surface are picking up the activity of neurons with 0.5 mm. Lastly, depth electrodes that are inserted into the brain will detect neurons within tens of microns.

```{python}
elec_dists = [2, 0.05, 0.001] # cm
dist_labels = ['2 cm', '0.5 mm', '10 um']

fig, ax = plt.subplots(len(elec_dists),1, figsize=(5, 12), sharey=True)
for ind, curr_dist in enumerate(elec_dists):
    ax[ind].plot(test._t*1e3, test.calc_extracell(h=curr_dist)*1e6)
    ax[ind].set_title(dist_labels[ind])
    ax[ind].set_xlabel('Time (ms)')
    ax[ind].set_ylabel('Extracellular potential (uV)')

fig.tight_layout()
```

Each of these distances reveal different features of neural activity. The most obvious difference is that the signals tend to get larger as the electrode is moved closer to the sheet of neurons. At 2 cm, slow potentials, both the slow global signal and the synaptic drive, are evident. Then, moving to 0.5 mm we start to pick up some weak local spiking, and still can reliably detect synaptic currents. Note that the weak spiking we see is concentrated at the peak of the synaptic drive, with numerous overlapping spikes filling in the negative trough made by the depolarizing synaptic current. If we push the electrode to within 10 um of a neuron, the action potential becomes the dominant part of the signal. While the synaptic current is still visible prior to the spike, it is masked by the action potential once that occurs.

And going a little further, we can push the electrode right against the cell body of the neuron. This is *juxtacellular* recording, and it has a distinctive appearance where global and synaptic potentials are towered over by the robust currents generated by the action potential.

```{python}
plt.plot(test._t*1e3, test.calc_extracell(h=0.0001)*1e6)
plt.title('1 um')
plt.xlabel('Time (ms)')
plt.ylabel('Extracellular potential (uV)')
```