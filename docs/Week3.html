<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Week3 – Decoding The Brain</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Decoding The Brain</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.qmd"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Week1.html"> 
<span class="menu-text">Week 1</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Week2.html"> 
<span class="menu-text">Week 2</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./Week3.html" aria-current="page"> 
<span class="menu-text">Week3</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="./about.qmd"> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-classification-with-logistic-regression" id="toc-introduction-to-classification-with-logistic-regression" class="nav-link active" data-scroll-target="#introduction-to-classification-with-logistic-regression">Introduction to Classification with Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#what-we-will-cover" id="toc-what-we-will-cover" class="nav-link" data-scroll-target="#what-we-will-cover">What we will cover</a></li>
  <li><a href="#quantification-of-erp-strength" id="toc-quantification-of-erp-strength" class="nav-link" data-scroll-target="#quantification-of-erp-strength">Quantification of ERP strength</a>
  <ul class="collapse">
  <li><a href="#loading-eeg-data-from-stored-numpy-arrays" id="toc-loading-eeg-data-from-stored-numpy-arrays" class="nav-link" data-scroll-target="#loading-eeg-data-from-stored-numpy-arrays">Loading EEG data from stored Numpy arrays</a></li>
  <li><a href="#extracting-non-stimulus-times" id="toc-extracting-non-stimulus-times" class="nav-link" data-scroll-target="#extracting-non-stimulus-times">Extracting non-stimulus times</a></li>
  <li><a href="#extracting-eeg-epochs" id="toc-extracting-eeg-epochs" class="nav-link" data-scroll-target="#extracting-eeg-epochs">Extracting EEG epochs</a></li>
  <li><a href="#quantifying-erp-strength" id="toc-quantifying-erp-strength" class="nav-link" data-scroll-target="#quantifying-erp-strength">Quantifying ERP strength</a></li>
  </ul></li>
  <li><a href="#a-simple-decision-model" id="toc-a-simple-decision-model" class="nav-link" data-scroll-target="#a-simple-decision-model">A simple decision model</a></li>
  <li><a href="#binary-classification" id="toc-binary-classification" class="nav-link" data-scroll-target="#binary-classification">Binary classification</a>
  <ul class="collapse">
  <li><a href="#a-quick-probability-primer" id="toc-a-quick-probability-primer" class="nav-link" data-scroll-target="#a-quick-probability-primer">A quick probability primer</a></li>
  <li><a href="#logistic-function" id="toc-logistic-function" class="nav-link" data-scroll-target="#logistic-function">Logistic function</a></li>
  </ul></li>
  <li><a href="#fitting-the-logistic-function" id="toc-fitting-the-logistic-function" class="nav-link" data-scroll-target="#fitting-the-logistic-function">Fitting the logistic function</a>
  <ul class="collapse">
  <li><a href="#scikit-learn-standards" id="toc-scikit-learn-standards" class="nav-link" data-scroll-target="#scikit-learn-standards">Scikit-learn standards</a></li>
  <li><a href="#initializing" id="toc-initializing" class="nav-link" data-scroll-target="#initializing">Initializing</a></li>
  <li><a href="#fitting" id="toc-fitting" class="nav-link" data-scroll-target="#fitting">Fitting</a></li>
  <li><a href="#evaluating" id="toc-evaluating" class="nav-link" data-scroll-target="#evaluating">Evaluating</a></li>
  <li><a href="#interpreting" id="toc-interpreting" class="nav-link" data-scroll-target="#interpreting">Interpreting</a></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a></li>
  </ul></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together">Putting it all together</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Week3</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-to-classification-with-logistic-regression" class="level1">
<h1>Introduction to Classification with Logistic Regression</h1>
<p>Last week we established that the neural response to stimulus presentation produces a stereotyped pattern of electrical activity on the scalp, referred to as an event-related potential (ERP). This was determined by taking the times when a stimulus was presented and calculating the average EEG activity around that time. Since we know when the stimuli are presented, determining how the brain responds is fairly straight forward. What is not straight forward is going back the other direction: given the neural activity can we determine whether a stimulus was presented? This is the essential problem of BCI, and the one we will solve. And solving it for this case unlocks a host of new possibilities that. The approach developed this week can be elaborated to solve wide variety of brain activity decoding problems. It opens up the possibility to decode the identity of the presented stimulus, or a subject’s intended action. <strong><em>This is as close to mind-reading as one can get.</em></strong> Let’s get to work.</p>
<section id="what-we-will-cover" class="level2">
<h2 class="anchored" data-anchor-id="what-we-will-cover">What we will cover</h2>
<p>The overarching goal of this week is to introduce simple logistic regression models, where a single feature derived from neural data (ERP strength) is used to predict a single predictor (stimulus present or absent). To start we will discuss different ways to quantify the strength of an ERP. We will also weigh the advantages and disadvantages of different approaches to extract time periods from the recording that lack stimuli. Then logistic regression will be introduced and we will cover how to apply it using the scikit-learn python package. Lastly, we will discuss some of general issues that arise in machine learning algorithms like logistic regression.</p>
<div id="a5c7e60d" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, StratifiedKFold</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sys.path.append(<span class="st">'..'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> source.loaders <span class="im">import</span> EEG, remove_baseline_drift, remove_emg, remove_ac, detect_blinks</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="quantification-of-erp-strength" class="level2">
<h2 class="anchored" data-anchor-id="quantification-of-erp-strength">Quantification of ERP strength</h2>
<p>ERPs have complex waveforms that can last a couple hundred milliseconds. With a sample rate of 500 Hz, that means we will have around 100 samples of neural data to work with when trying to determine whether a stimulus was present or not. You could imagine being given a set of waveforms, some from trials with a stimulus and others without, and trying to determine which was which. One strategy is to calculate a feature of the waveform related to the presence of the ERP, and if that feature exceeds a certain value then the ERP is deemed present on that trial.</p>
<section id="loading-eeg-data-from-stored-numpy-arrays" class="level3">
<h3 class="anchored" data-anchor-id="loading-eeg-data-from-stored-numpy-arrays">Loading EEG data from stored Numpy arrays</h3>
<p>As a starting point, we will use the data set we worked with last week. An auditory cue was given about every 8 seconds, and there was a robust ERP elicited over occipital and parietal EEG sites. We will select those epochs when we know a cue was presented, and an equal number of epochs where we know a cue was not delivered. To start, we need to load the data.</p>
<p>Last week we created an EEG class that interfaced with the EEG data, along with preprocessing functions to remove noise or detect eye blinks. We will reuse them here to quickly get us back to where we left off last week.</p>
<div id="bd5fb6cb" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create file paths</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>subj <span class="op">=</span> <span class="st">'sub-AB58'</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>evt_dir <span class="op">=</span> [<span class="st">'data'</span>, <span class="st">'eeg'</span>, <span class="st">'ds003690'</span>, subj, <span class="st">'eeg'</span>, <span class="st">'</span><span class="sc">{}</span><span class="st">_task-passive_run-1_events.tsv'</span>.<span class="bu">format</span>(subj)]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>evt_path <span class="op">=</span> os.path.join(<span class="op">*</span>evt_dir)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> [<span class="st">'data'</span>, <span class="st">'eeg'</span>, <span class="st">'ds003690'</span>, subj, <span class="st">'eeg'</span>, <span class="st">'</span><span class="sc">{}</span><span class="st">_task-passive_run-1_eeg.set'</span>.<span class="bu">format</span>(subj)]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>eeg_path <span class="op">=</span> os.path.join(<span class="op">*</span>data_dir)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>chan_dir <span class="op">=</span> [<span class="st">'data'</span>, <span class="st">'eeg'</span>, <span class="st">'ds003690'</span>, subj, <span class="st">'eeg'</span>, <span class="st">'</span><span class="sc">{}</span><span class="st">_task-passive_run-1_channels.tsv'</span>.<span class="bu">format</span>(subj)]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>chan_path <span class="op">=</span> os.path.join(<span class="op">*</span>chan_dir)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>eeg <span class="op">=</span> EEG(eeg_path, chan_path)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>events <span class="op">=</span> pd.read_csv(evt_path, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># preprocess data</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>remove_baseline_drift(eeg)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>remove_emg(eeg)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>remove_ac(eeg, ac_freq<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>blinks <span class="op">=</span> detect_blinks(eeg)[<span class="dv">0</span>] <span class="co"># indexing the call to the function allows us to select the output we want</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>cues <span class="op">=</span> events[<span class="st">'onset'</span>][events[<span class="st">'trial_type'</span>] <span class="op">==</span> <span class="st">'cue'</span>].values</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="extracting-non-stimulus-times" class="level3">
<h3 class="anchored" data-anchor-id="extracting-non-stimulus-times">Extracting non-stimulus times</h3>
<p>To compare stimulus and non-stimulus periods, we need to select times when the stimulus did not occur. At first glance, this seems like a trivial task. Given the list of stimulus times, we can just create a new list by hand that does not include any of those times. However, we might unintentionally incorporate biases in our list creation. In addition, we want to avoid periods with blinks, because we exclude those periods from our ERP construction. This means we have to avoid two sets of conditions. But worst of all, we would have to create a new list for each recording session, which is tedious. We program to avoid tedium, so lets explore ways to automate this step.</p>
<p>The approach we devise to create no cue times should satisfy a few conditions:</p>
<ol type="1">
<li>The cue should not be present</li>
<li>No eye blinks should be present</li>
<li>No systematic temporal relationship with cue delivery</li>
<li>No-cue epochs should have similar properties to cue epochs, such as:
<ul>
<li>Occur during the same period of the recording</li>
<li>Similar relative timing</li>
</ul></li>
</ol>
<p>But first, we will get the cue times that did not overlap with eye blinks.</p>
<div id="3a3e3941" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function that returns cues without blinks</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>excl_blink <span class="op">=</span> <span class="fl">0.6</span> <span class="co"># seconds, exclusion period surrounding a blink</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>rem_blinks <span class="op">=</span> <span class="kw">lambda</span> c,b: np.array([ci <span class="cf">for</span> ci <span class="kw">in</span> c <span class="cf">if</span> <span class="kw">not</span> <span class="bu">any</span>(np.<span class="bu">abs</span>(ci<span class="op">-</span>b) <span class="op">&lt;</span> excl_blink)])</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># keep those cues that are not within the blink exclusion period</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>cues_nob <span class="op">=</span> rem_blinks(cues, blinks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The same function should also be used with the non-stimulus times we will generate. We want the no-cue times we create to be as similar as possible to the times when cues were presented. If we are excluding cues that were near eye blinks, then we have to do the same for the no-cue periods. If we do not, then the cue and no-cue times will differ in whether an eye blink was present, which our algorithm could potentially pick up on to distinguish between them.</p>
<p>To ensure that our no-cue periods do not come close to the cue periods, we could set the no-cue times to be at a fixed time <em>before</em> each cue. If that time is greater than the post-window, and far enough from the previous cue time to ensure that it does not overlap with its ERP, this might work. This strategy would be questionable if the ITIs were at a fixed interval, because then there would be a systematic relationship between the cue times and no-cue times. But because they are random, that means the previous cue time will be random, plus or minus a second, with respect to the no-cue time.</p>
<div id="7ccec53b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fixed time before cue method</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># fixed time should be less than the shortest iti minus the window.</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>fixed_time <span class="op">=</span> <span class="dv">4</span> <span class="co"># half of the mean ITI</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>nocues_fixed <span class="op">=</span> cues<span class="op">-</span>fixed_time</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>nocues_fixed_nob <span class="op">=</span> rem_blinks(nocues_fixed, blinks)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_nocues(ax, nocues_orig, nocues_rem, cues):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Plot no-cue times, with cues and cut no-cue times overlaid</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">    ax : matplotlib.axes.Axes</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Axes to plot on</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">    nocues_orig : numpy.ndarray</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Original no-cue times</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">    nocues_rem : numpy.ndarray</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">        No-cue times with blinks removed</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">    cues : numpy.ndarray</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">        Cue times</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    ax.vlines(cues, colors<span class="op">=</span><span class="st">'k'</span>, ymin<span class="op">=-</span><span class="dv">1</span>, ymax<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.2</span>) <span class="co"># cue times</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    ax.eventplot(nocues_rem, colors<span class="op">=</span><span class="st">'b'</span>) <span class="co"># no-cue times</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    nocues_cut <span class="op">=</span> np.setdiff1d(nocues_orig, nocues_rem) <span class="co"># cut no-cue times</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> nocue_cut <span class="kw">in</span> nocues_cut:</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        ax.text(nocue_cut, <span class="dv">1</span>, <span class="st">'X'</span>, fontsize<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'r'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks([])</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Time (s)'</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># plot permuted no-cue times</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">2</span>))</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>plot_nocues(ax, nocues_fixed, nocues_fixed_nob, cues)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Fixed offset no-cue times'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>Text(0.5, 1.0, 'Fixed offset no-cue times')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-5-output-2.png" width="764" height="228" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We created a custom plotting function to see how our event times for the cue and no-cue periods related to each other. In it vertical lines are the times when cues were presented, blue hatch marks are when valid random no-cue times occur, and red ’X’s are the no-cue times that were eliminated due to overlapping with a cue or blink. Using times at a fixed interval before cue delivery looks good. Our no-cue periods are consistently far away from the cues and have a similar distribution as the cues across the session. We will use these times to get our no-cue epochs.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Some flawed approaches to selecting non-cued periods
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are a variety of strategies for choosing the times when no stimuli are present that should meet the criteria listed above. Here we will explore a couple that seem like good ideas, but upon closer examination exhibit flaws.</p>
<p>Since these approaches will generate no-cue times that do not have a prespecified fixed offset from the cue times, we need to exclude those times near a cue. For this, we will create a similar function to the one that removed times close to blinks (<code>rem_blinks</code>), but use a tighter exclusion period because of the tight temporal relationship between cue delivery and the ERP.</p>
<div id="ddf808a8" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create version outside cue period</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>excl_cue <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># seconds, exclusion period around cue</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>rem_cues <span class="op">=</span> <span class="kw">lambda</span> nc,c: np.array([nci <span class="cf">for</span> nci <span class="kw">in</span> nc <span class="cf">if</span> <span class="kw">not</span> <span class="bu">any</span>(np.<span class="bu">abs</span>(nci<span class="op">-</span>c) <span class="op">&lt;</span> excl_cue)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="random-strategy" class="level4">
<h4 class="anchored" data-anchor-id="random-strategy">Random strategy</h4>
<p>For this approach, we will create a list of times using a random number generator. The random times we generate cannot have their window times extend past the beginning or end of the recording, nor overlap with blink period or cue periods. We will use a fixed random seed when generating these times to ensure consistency across runs.</p>
<div id="ee3f3113" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set random seed, ensures consistent results across runs</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">47</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># random times between 0 and recording duration</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>rec_dur <span class="op">=</span> eeg.data.shape[<span class="dv">1</span>]<span class="op">/</span>eeg.srate <span class="co"># (number of samples / sample rate) gives time in seconds</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>cue_num <span class="op">=</span> <span class="bu">len</span>(cues) <span class="co"># number of cues</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>nocues_rand <span class="op">=</span> np.random.uniform(<span class="dv">1</span>, rec_dur<span class="op">-</span><span class="dv">1</span>, cue_num)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>nocues_rand_nbc <span class="op">=</span> rem_cues(rem_blinks(nocues_rand, blinks), cues)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We use the <code>random.uniform</code> random number generator in numpy to get as many times as we have cues (<code>cue_num</code>), and restricted to times from the first till the last second of the recording session. Those times that overlap with cues or blinks are then removed, leaving a list of no-cue times that we can use. A good way to establish that the criteria for our no cue periods are followed is to plot the no-cue times overlaid on top of the cue times.</p>
<div id="a6909929" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">2</span>))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plot_nocues(ax, nocues_rand, nocues_rand_nbc, cues)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Random no-cue times'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>Text(0.5, 1.0, 'Random no-cue times')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-8-output-2.png" width="763" height="228" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If we compare the cue and no-cue times, it is apparent that the fourth criterion for good no-cue times is violated. One problem is that we have several no-cue times that occur outside the time period when the cue times were delivered (notice the two at the beginning and four at the end.) The other problem is that no-cue times are distributed unevenly, unlike the cues which are evenly spread across the recording session.</p>
</section>
<section id="permutation-strategy" class="level4">
<h4 class="anchored" data-anchor-id="permutation-strategy">Permutation strategy</h4>
<p>If we want to have the no-cue times have a similar distribution across the session as the cue times, we could generate random times but ensure that they have the same spacing between cues. One way to do this is to get a list of times between cue trials (inter-trial intervals, ITIs), shuffle those intervals, and then create a new list of times by adding them together one at a time. This strategy works because the times between cues are randomly generated, varying between 7 and 9 seconds.</p>
<div id="ff894ae5" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set random seed, ensures consistent results across runs</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">45</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate ITI times</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Note, we repeat the first ITI time so that when we sum the ITIs</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># we get the correct number of trials</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>iti_times <span class="op">=</span> np.insert(np.diff(cues), <span class="dv">0</span>, cues[<span class="dv">1</span>]<span class="op">-</span>cues[<span class="dv">0</span>])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># randomly shuffle the iti_times</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>iti_times <span class="op">=</span> np.random.permutation(iti_times)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># get no-cue times by summing the iti_times.</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># cumsum adds each element of the array to the previous element, </span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># returning an array of the same size</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>nocues_perm <span class="op">=</span> np.cumsum(iti_times)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co"># remove no-cue times that overlapped with cue times and blinks</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>nocues_perm_nbc <span class="op">=</span> rem_cues(rem_blinks(nocues_perm, blinks), cues)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co"># plot permuted no-cue times</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">2</span>))</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>plot_nocues(ax, nocues_perm, nocues_perm_nbc, cues)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Permuted no-cue times'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>Text(0.5, 1.0, 'Permuted no-cue times')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-9-output-2.png" width="764" height="228" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This looks much better than the purely random strategy. The no-cue times are during the same period when cues were presented, and have similar relative timing. However, they appear to sometimes overlap quite closely for runs of several trials with the cue times.</p>
</section>
</div>
</div>
</section>
<section id="extracting-eeg-epochs" class="level3">
<h3 class="anchored" data-anchor-id="extracting-eeg-epochs">Extracting EEG epochs</h3>
<p>Last week we covered extracting EEG epochs, so we will only lightly reprise this here. This time we need one set of epochs centered on the cue times, and another on the no-cue times. We can use the <code>get_data</code> method in our <code>eeg</code> object to extract the EEG data around these times. Each will be a 3-D numpy array, where time is on axis 0, channel is axis 1, and trial on axis 2. Instead of using all channels, we will focus only on a channel were the cue ERP was strong, O1.</p>
<div id="74a012e0" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># time window edges</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>pre_win <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># time to sample before the cue</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>post_win <span class="op">=</span> <span class="fl">0.5</span> <span class="co"># time to sample after the end</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>epoch_dur <span class="op">=</span> pre_win<span class="op">+</span>post_win <span class="co"># duration of an epoch</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>sel_chan <span class="op">=</span> <span class="st">'O1'</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># get cue epochs</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>cue_starts <span class="op">=</span> cues_nob <span class="op">-</span> pre_win</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>cue_epochs, t_erp, _ <span class="op">=</span> eeg.get_data(chans<span class="op">=</span>sel_chan, start_t<span class="op">=</span>cue_starts, dur_t<span class="op">=</span>epoch_dur, scale<span class="op">=</span><span class="st">'relative'</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># get no-cue epochs</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>nocue_starts <span class="op">=</span> nocues_fixed_nob <span class="op">-</span> pre_win</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>nocue_epochs, _, _ <span class="op">=</span> eeg.get_data(chans<span class="op">=</span>sel_chan, start_t<span class="op">=</span>nocue_starts, dur_t<span class="op">=</span>epoch_dur, scale<span class="op">=</span><span class="st">'relative'</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># correct the relative time stamps to account for the pre_win period</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>t_erp <span class="op">-=</span> pre_win</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since we only loaded one channel, our channel axis (axis 1) only has a length of 1. This ‘singleton’ dimension is useless for our purposes and so to make things easier we will remove it using the numpy <code>squeeze</code> function.</p>
<div id="6cccd1a7" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The original shape of the cue_starts array is: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(cue_epochs.shape))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>cue_epochs <span class="op">=</span> np.squeeze(cue_epochs)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The squeezed shape of the cue_starts array is: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(cue_epochs.shape))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>nocue_epochs <span class="op">=</span> np.squeeze(nocue_epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The original shape of the cue_starts array is: (300, 1, 25)
The squeezed shape of the cue_starts array is: (300, 25)</code></pre>
</div>
</div>
<p>Let’s plot the ERPs for the cue and no-cue epochs to make sure the ERP is only present for the cue.</p>
<div id="4cf604cb" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the average ERP for each condition</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>cue_erp <span class="op">=</span> np.mean(cue_epochs, axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>nocue_erp <span class="op">=</span> np.mean(nocue_epochs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the ERPs</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.plot(t_erp, cue_erp, label<span class="op">=</span><span class="st">'Cue'</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>plt.plot(t_erp, nocue_erp, label<span class="op">=</span><span class="st">'No-cue'</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Time (s)'</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Voltage (uV)'</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Average ERP for channel '</span> <span class="op">+</span> sel_chan)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-12-output-1.png" width="810" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So far so good. We can see that the ERP is strong for epochs centered on the cue, and non-existent for times that avoid cue presentation.</p>
</section>
<section id="quantifying-erp-strength" class="level3">
<h3 class="anchored" data-anchor-id="quantifying-erp-strength">Quantifying ERP strength</h3>
<p>The next step is to distill the complex ERP waveform into a single number that reflects its strength. There are numerous measures you can use for this, and we will explore a few. This step is known as feature engineering, wherein we try to distill multidimensional samples to a few values that capture the aspects of the sample that are relevant to our task. In this case, each sample is a collection of 300 voltage measurements, and the aspect we want to measure is the strength of the ERP. A good feature will take on values that distinguish between the cue and no cue epochs.</p>
<section id="voltage-at-erp-peak-time" class="level4">
<h4 class="anchored" data-anchor-id="voltage-at-erp-peak-time">Voltage at ERP peak time</h4>
<p>Since we know the shape of the ERP, we know on average where it tends to peak in value. The voltage at that time should indicate whether an ERP is present. Looking at the average ERP traces immediately above, you can see that on trials with a cue the voltage will tend to be near 10 uV at the peak time, while on no-cue trials it will tend to be near 2 uV. To get the peak voltage on each trial, we first find the time when the ERP peak occurs, then sample the voltage at that time for each epoch.</p>
<div id="32d9510e" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get peak idx of cue erp</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>cue_peak_idx <span class="op">=</span> np.argmax(cue_erp)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># return the voltages at the ERP peak time</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>cue_erp_peaks <span class="op">=</span> cue_epochs[cue_peak_idx, :]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>nocue_erp_peaks <span class="op">=</span> nocue_epochs[cue_peak_idx, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To examine how well the peak voltage distinguishes between the Cue and No-cue conditions, we can plot their distribution. The less their respective distributions overlap, the better the feature is for distinguishing between the types of trials.</p>
<div id="d01aa539" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># box plot of peak values for cue and nocue conditions with event plot of points on top</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_dist_erps(cue_vals, nocue_vals, ax<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Plot distribution of cue and no-cue ERP peak values</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">    cue_vals : numpy.ndarray</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">        Array of cue ERP peak values</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    nocue_vals : numpy.ndarray</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Array of no-cue ERP peak values</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    ax : matplotlib.axes.Axes</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Axes to plot on</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># violin plot of values, with cue_vals blue and nocue_vals orange</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    cue_vp <span class="op">=</span> ax.violinplot(cue_vals, positions<span class="op">=</span>[<span class="dv">1</span>], showmedians<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pc <span class="kw">in</span> cue_vp[<span class="st">'bodies'</span>]:</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        pc.set_facecolor(<span class="st">'tab:blue'</span>)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    nocue_vp <span class="op">=</span> ax.violinplot(nocue_vals, positions<span class="op">=</span>[<span class="dv">2</span>], showmedians<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pc <span class="kw">in</span> nocue_vp[<span class="st">'bodies'</span>]:</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        pc.set_facecolor(<span class="st">'tab:orange'</span>)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot the cue_vals and nocue_vals as points on top of the violin plot</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    ax.plot(np.ones(<span class="bu">len</span>(cue_vals)), cue_vals, <span class="st">'.'</span>, color<span class="op">=</span><span class="st">'tab:blue'</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="dv">2</span> <span class="op">*</span> np.ones(<span class="bu">len</span>(nocue_vals)), nocue_vals, <span class="st">'.'</span>, color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the xticks to be at 1 and 2, with labels Cue and No Cue</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    ax.set_xticklabels([<span class="st">'Cue'</span>, <span class="st">'No Cue'</span>])</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">4</span>))</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>plot_dist_erps(cue_erp_peaks, nocue_erp_peaks, ax<span class="op">=</span>ax)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Peak Amplitude (uV)'</span>)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Peak Amplitude of Cue and No Cue Conditions'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>Text(0.5, 1.0, 'Peak Amplitude of Cue and No Cue Conditions')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-14-output-2.png" width="388" height="357" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The violin plot above shows a smoothed distribution of the data points as a shaded region. The solid horizontal lines from top to bottom are the maximum, median, and minimum values. The distributions do overlap, with the median value for the No-cue condition falling within the distribution of values for the Cue. This overlap is likely because there is substantial spontaneous activity in the EEG signal that is added to our ERP peak estimate on each trial. When we got the average ERP, this spontaneous activity that was not time-locked to cue delivery canceled out because it was equally likely to be positive or negative. We can visualize this spontaneous component by plotting the EEG signal for each epoch with the ERP subtracted out. That looks like:</p>
<div id="3c9b0ac8" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># subtract the mean ERP waveform from each epoch</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># epochs have to be transposed, x.T, because of broadcasting rules in numpy</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>cue_epochs_spon <span class="op">=</span> cue_epochs.T<span class="op">-</span>cue_erp</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>nocue_epochs_spon <span class="op">=</span> nocue_epochs.T<span class="op">-</span>nocue_erp</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot EEG without ERP, and distribution of values where ERP peak would be</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplot_mosaic([[<span class="st">'ul'</span>, <span class="st">'ul'</span>, <span class="st">'r'</span>],[<span class="st">'ll'</span>, <span class="st">'ll'</span>, <span class="st">'r'</span>]], sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'ul'</span>].plot(t_erp, cue_epochs_spon.T, color<span class="op">=</span><span class="st">'tab:blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'ul'</span>].plot(t_erp, cue_erp, color<span class="op">=</span><span class="st">'tab:blue'</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'ul'</span>].grid()</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'ll'</span>].plot(t_erp, nocue_epochs_spon.T, color<span class="op">=</span><span class="st">'tab:orange'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'ll'</span>].plot(t_erp, nocue_erp, color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'ll'</span>].grid()</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'ll'</span>].<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'Time (s)'</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>plot_dist_erps(cue_epochs_spon[:,cue_peak_idx], nocue_epochs_spon[:,cue_peak_idx], ax<span class="op">=</span>ax[<span class="st">'r'</span>])</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'r'</span>].set_yticklabels(<span class="bu">range</span>(<span class="op">-</span><span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">10</span>))</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>fig.subplots_adjust(hspace <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'ul'</span>].<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Waveforms'</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="st">'r'</span>].<span class="bu">set</span>(title<span class="op">=</span><span class="st">'EEG voltage at ERP peak'</span>)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Spontaneous EEG'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'Voltage (uV)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8892/2930109674.py:16: UserWarning:

set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>Text(0.02, 0.5, 'Voltage (uV)')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-15-output-3.png" width="633" height="477" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>After removing the ERP, a a great deal of variation still remains, with an amplitude on par with the ERP itself. In fact, the shape of the distribution of EEG voltages at the ERP peak time is the same as the one for the ERPs above. This is a simple consequence of the ERP peak values reflecting the sum of the mean ERP peak and the spontaneous EEG at that time. When we subtract the mean ERP peak, we are just shifting the distribution to zero. Is there any way we can account for the spontaneous EEG activity on a single trial?</p>
</section>
<section id="average-voltage-during-erp" class="level4">
<h4 class="anchored" data-anchor-id="average-voltage-during-erp">Average voltage during ERP</h4>
<p>Examining the spontaneous EEG waveforms, we see that the signal fluctuates between positive and negative values on the order of tens of milliseconds. Perhaps if we average the EEG signal over the 100 ms when the strongest ERP component is present, from 80 to 180 ms, the spontaneous activity will cancel itself out. The ERP, on the other hand, maintains positive values during that period, so it should persist despite the averaging. The logic here is similar to averaging the EEG epochs across trials, because the spontaneous voltage at each time point across epochs will randomly vary between positive and negative values. In this case, we are just applying that principle across time within a trial.</p>
<p>When we restrict analysis or calculation to a specific time window, that period is referred to as the <em>region of interest</em> (ROI). Here, our ROI will be from 80 to 180 ms post cue (or no-cue), and we will take the mean value of the EEG voltage during that time.</p>
<div id="af369f46" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get indices between 80 and 180 ms</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>roi_idxs <span class="op">=</span> np.where((t_erp <span class="op">&gt;=</span> <span class="fl">0.08</span>) <span class="op">&amp;</span> (t_erp <span class="op">&lt;=</span> <span class="fl">0.18</span>))[<span class="dv">0</span>]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># get the mean EEG voltage during the strongest ERP component across trials for each condition</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>cue_erp_means <span class="op">=</span> np.mean(cue_epochs[roi_idxs, :], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>nocue_erp_means <span class="op">=</span> np.mean(nocue_epochs[roi_idxs, :], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot mean voltages</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">4</span>))</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>plot_dist_erps(cue_erp_means, nocue_erp_means, ax<span class="op">=</span>ax)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Mean ERP voltage (uV)'</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Mean amplitude of Cue and No Cue Conditions'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>Text(0.5, 1.0, 'Mean amplitude of Cue and No Cue Conditions')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-16-output-2.png" width="393" height="357" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Well that did not work! Averaging over the period where the strongest ERP component was present diminished median peak ERP voltage for the Cue condition, making it less separable from the No-cue condition. This happens because the magnitude of the ERP varies across time, and we are averaging together periods where it is weaker. Indeed, if we calculate the mean of the ERP during our ROI:</p>
<div id="0d551f01" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get mean of ERP during the ROI</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>erp_mean <span class="op">=</span> np.mean(cue_erp[roi_idxs])</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean of ERP during ROI: </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(erp_mean))</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># get ERP voltage during its peak</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>erp_peak <span class="op">=</span> cue_erp[cue_peak_idx]</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ERP voltage during peak: </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(erp_peak))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean of ERP during ROI: 6.63
ERP voltage during peak: 11.21</code></pre>
</div>
</div>
<p>We have gone from an ERP peak voltage of 11.2 to 6.6 uV, a decrease by almost half! While this might tempt us to go back to the original approach of just sampling the voltage at the ERP peak, we would leave behind one advantage of the averaging approach: it reduced the variability in the distribution of ERP values. This is evident when we compare the standard deviations of the distributions between the ROI mean and peak estimation approaches.</p>
<div id="4287d289" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># variance of ERP ROI means</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>cue_erp_means_var <span class="op">=</span> np.std(cue_erp_means)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>nocue_erp_means_var <span class="op">=</span> np.std(nocue_erp_means)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># variance of ERP peaks</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>cue_erp_peaks_var <span class="op">=</span> np.std(cue_erp_peaks)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>nocue_erp_peaks_var <span class="op">=</span> np.std(nocue_erp_peaks)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print comparisons</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Cue ERP ROI </span><span class="sc">{:.2f}</span><span class="st"> vs peaks </span><span class="sc">{:.2f}</span><span class="st"> uV'</span>.<span class="bu">format</span>(cue_erp_means_var, cue_erp_peaks_var))</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'No-cue ERP ROI </span><span class="sc">{:.2f}</span><span class="st"> vs peaks </span><span class="sc">{:.2f}</span><span class="st"> uV'</span>.<span class="bu">format</span>(nocue_erp_means_var, nocue_erp_peaks_var))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cue ERP ROI 4.82 vs peaks 6.04 uV
No-cue ERP ROI 6.75 vs peaks 8.80 uV</code></pre>
</div>
</div>
<p>Taking the mean over the ROI reduced the variability in our ERP measures. This means their distributions are tighter. Assuming the means of those distributions do not change, this would decrease their overlap. Consequently, our ability to discriminate between cue and no-cue trials should improve. However, the means did change, we lost almost half of the ERP strength compared with just taking the peak.</p>
<p>Is there a way we can benefit from averaging over multiple samples to get a measure of ERP strength, while not diminishing the strength of the ERP itself?</p>
</section>
<section id="dot-product-of-erp" class="level4">
<h4 class="anchored" data-anchor-id="dot-product-of-erp">Dot product of ERP</h4>
<p>One way to average over multiple samples while not diminishing ERP strength is to weight each time point by the value of the ERP. In effect, this would measure the alignment between the voltages recorded during an epoch and the mean ERP. Here we determine the strength of the ERP by multiplying each data point by its corresponding voltage in the ERP. If we are at a sample where the ERP is strongly positive, then we want that point to strongly contribute the mean, while a point where the ERP is weak should contribute less. If the ERP is negative at a certain time point, we want to negate that sample so it will constructively add to the mean. By doing this, if the EEG contains a signal that follows the waveform of the ERP it will return a strong mean value, while an EEG signal that does not follow the ERP time course will return a weak mean.</p>
<p>A mathematical operation that can be used to do this is called the <em>dot product</em>. A dot product multiplies together each corresponding element in two vectors, <span class="math inline">x</span> and <span class="math inline">y</span>, and then adds those together, returning a scalar <span class="math inline">z</span>.</p>
<p><span class="math display"> \begin{align}
    \notag z &amp;= x \cdot y \\
    \notag  &amp;= \sum_{i=1}^{n}x_{i}y_{i}
\end{align}
</span></p>
<p>Let’s explore how to calculate a dot product and its behavior.</p>
<div id="f9a8ce05" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># how to code a dot product in base python</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dot(x, y):</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate the dot product of two vectors x and y</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co">    x : array-like</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co">        First vector</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="co">    y : array-like</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Second vector</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="co">    out : float</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Dot product of x and y</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="dv">0</span> <span class="co"># initialize output to 0</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x_i, y_i <span class="kw">in</span> <span class="bu">zip</span>(x, y): <span class="co"># loop over elements of x and y</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> x_i <span class="op">*</span> y_i <span class="co"># add the product of the elements to out</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>v1 <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>v2 <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Dot product of two aligned vectors, </span><span class="sc">{}</span><span class="st"> and </span><span class="sc">{}</span><span class="st"> = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(v1,v2,dot(v1, v2)))</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>v1 <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>v2 <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Dot product of two partly aligned vectors, </span><span class="sc">{}</span><span class="st"> and </span><span class="sc">{}</span><span class="st"> = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(v1,v2,dot(v1, v2)))</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>v1 <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>v2 <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Dot product of two misaligned vectors, </span><span class="sc">{}</span><span class="st"> and </span><span class="sc">{}</span><span class="st"> = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(v1,v2,dot(v1, v2)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dot product of two aligned vectors, [1, 2, 3] and [1, 2, 3] = 14
Dot product of two partly aligned vectors, [1, 2, 3] and [2, 2, 2] = 12
Dot product of two misaligned vectors, [1, 2, 3] and [-1, 2, -1] = 0</code></pre>
</div>
</div>
<p>Instead of rolling our own, we will use the numpy array dot function, <code>dot</code>, because it handles dot products on multidimensional arrays and runs faster.</p>
<p>To calculate the alignment between an EEG epoch and the mean ERP, we use the ERP waveform as our weighting vector. We will scale its values by its <em>euclidean norm</em>, so that the values returned by our dot product are in a comparable range to those from the peak and mean calculations we did previously. The euclidean norm is:</p>
<p><span class="math display"> ||x|| = \sqrt{\sum_{i=1}^{n}{x_{i}^{2}}} </span></p>
<p>This function is implemented in the linear algebra portion of the numpy package as <code>linalg.norm</code>.</p>
<p>Thus, the alignment between an EEG epoch and ERP is:</p>
<p><span class="math display"> Alignment = EEG \cdot \frac{ERP}{||ERP||} </span></p>
<p>Let’s give it a try.</p>
<div id="7857f6ca" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># our function that measures the alignment between the EEG epoch and ERP</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> erp_align(epochs, erp):</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate the dot product of each epoch with the ERP</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">    epochs : numpy.ndarray</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Array of EEG epochs, with epochs in the last dimension</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co">    erp : numpy.ndarray</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Array of ERP waveform</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="co">    out : numpy.ndarray</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Array of dot products, one for each epoch</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(epochs.T, erp<span class="op">/</span>np.linalg.norm(erp))</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>cue_erp_dots <span class="op">=</span> erp_align(cue_epochs, cue_erp)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>nocue_erp_dots <span class="op">=</span> erp_align(nocue_epochs, cue_erp)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="co"># box plot of peak values for cue and nocue conditions with event plot of points on top</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">4</span>))</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>plot_dist_erps(cue_erp_dots, nocue_erp_dots, ax<span class="op">=</span>ax)</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Dot product (uV)'</span>)</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Dot product of Cue and No Cue Conditions'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>Text(0.5, 1.0, 'Dot product of Cue and No Cue Conditions')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-20-output-2.png" width="358" height="357" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>These two distributions seem much more separable than the previous ones. The median values for the Cue and No-cue conditions do not overlap with the distribution of the opposite condition, and their overall distributions overlap less as well. Replotting all three together highlights the improvement we have made in tailoring the feature we extract from the EEG for detecting the Cue-evoked ERP.</p>
<div id="109f6aa8" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>,figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>plot_dist_erps(cue_erp_peaks, nocue_erp_peaks, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Peak amplitude'</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>plot_dist_erps(cue_erp_means, nocue_erp_means, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Mean amplitude'</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>plot_dist_erps(cue_erp_dots, nocue_erp_dots, ax<span class="op">=</span>ax[<span class="dv">2</span>])</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'Dot product'</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'uV'</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-21-output-1.png" width="569" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="a-simple-decision-model" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-decision-model">A simple decision model</h2>
<p>Our goal is to determine whether a trial had a cue stimulus or not. The dot product feature seems to distinguish between trial types, such that its value on a given trial indicates whether the cue was present. This is a decision process, where we assign a trial to either the Cue or No-cue category depending on the value of the dot product. A simple way to make this decision is to set a threshold value for the dot product, above which the trial is classified as a Cue trial. If the value is below the threshold, we label it a No-cue trial. This can be represented as a piecewise mathematical formula:</p>
<p><span class="math display"> f(x,t) =
\begin{cases}
    0 &amp; x\leq t \\
    1 &amp; x\gt t \\
\end{cases}
</span></p>
<p>Here <span class="math inline">x</span> is the dot product of the epoch, and <span class="math inline">t</span> is the threshold. When the function returns a 1 we refer to it as a Cue trial, while a 0 is labeled a No-cue trial. We have to choose a threshold, and to start we will use the minimum dot product of our Cue trials.</p>
<div id="750563b1" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set threshold</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>thresh <span class="op">=</span> np.<span class="bu">min</span>(cue_erp_dots)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create vectors for ERP values and class labels</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>cue_num <span class="op">=</span> cue_erp_dots.size <span class="co"># number of cue trials</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>nocue_num <span class="op">=</span> nocue_erp_dots.size <span class="co"># number of nocue trials</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>class_act <span class="op">=</span> np.repeat([<span class="va">True</span>, <span class="va">False</span>], [cue_num, nocue_num]) <span class="co"># actual class labels, 1 for cue, 0 for nocue</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> np.append(cue_erp_dots, nocue_erp_dots) <span class="co"># ERP values</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>class_min_pred <span class="op">=</span> vals <span class="op">&gt;</span> thresh <span class="co"># predicted class labels</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co"># print the true vs. predicted class labels</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Actual class labels: '</span>, class_act)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Predicted class labels: '</span>, class_min_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Actual class labels:  [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False]
Predicted class labels:  [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True False
  True False False  True False  True False False False False False False
  True  True False False False False False False  True False  True  True
 False False False False False False]</code></pre>
</div>
</div>
<p>If you look closely, you can see that some of our predicted classes do not agree with the true ones. There are different types of errors a classifier can make, and numerous ways to measure their performance. We will start with two simple measures, <em>error rate</em> and <em>accuracy</em>. Error rate is the proportion of trials that were misclassified. This can be defined mathematically as:</p>
<p><span class="math display"> Error Rate = \frac{1}{n}\sum_{i=1}^{n}{I(y_{i}\ne \hat{y_{i}})} </span></p>
<p>Here <span class="math inline">y</span> is the actual class labels and <span class="math inline">\hat{y}</span> are the predicted class labels. The function <span class="math inline">I(x,y)</span> returns a 1 when <span class="math inline">x \ne y</span>, and 0 when <span class="math inline">x=y</span>. Summing across all trials and dividing by the total number of trials, <span class="math inline">n</span>, gives the proportion of trials whose predicted class disagreed with its actual class.</p>
<p>Another helpful measure is accuracy, which is simply the proportion of trials where the predicted and actual class labels agreed. Expressed as:</p>
<p><span class="math display"> Accuracy = 1 - Error Rate </span></p>
<p>How does our simple classifier perform based on these metrics?</p>
<div id="31273f09" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># as an example, here we define error rate method with base python</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> error_rate_base(y_act, y_pred):</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    er <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(y_act)):</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> y_act[i] <span class="op">!=</span> y_pred[i]:</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>            er <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">100</span> <span class="op">*</span> er <span class="op">/</span> <span class="bu">len</span>(y_act)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co"># define error rate method with numpy</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> error_rate(y_act, y_pred):</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(y_act <span class="op">!=</span> y_pred) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co"># define accuracy method with numpy</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> accuracy(y_act, y_pred):</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">100</span><span class="op">-</span>error_rate(y_act, y_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2c5c9cc6" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate error rate for ERP peak</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 for cue trials, 0 for non-cue trials</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>er_min <span class="op">=</span> error_rate(class_act, class_min_pred)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>ac_min <span class="op">=</span> accuracy(class_act, class_min_pred)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Error rate for ERP peaks: </span><span class="sc">{:.1f}</span><span class="st">%'</span>.<span class="bu">format</span>(er_min))</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy for ERP peaks: </span><span class="sc">{:.1f}</span><span class="st">%'</span>.<span class="bu">format</span>(ac_min))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Error rate for ERP peaks: 14.8%
Accuracy for ERP peaks: 85.2%</code></pre>
</div>
</div>
<p>Not too bad. Just setting our threshold to the minimum value from the Cue trials, we can correctly classify ~85% of trials. But, how do we know we have chosen the best threshold? Moreover, can we ascertain how confident our classifier is in the choice it puts out? To address these issues, we need to formalize the classification process. One approach is to use <em>logistic regression</em>, which takes a measurement (e.g.&nbsp;ERP dot product) and returns the <em>probability</em> that it was generated by class (e.g.&nbsp;cue stimulus). For this week, we will pose the problem and learn how to use the logistic regression functions in the scikit-learn package.</p>
</section>
<section id="binary-classification" class="level2">
<h2 class="anchored" data-anchor-id="binary-classification">Binary classification</h2>
<p>A binary classifier takes a set of measurements, <span class="math inline">x</span>, as inputs and returns the probability that they were generated by a specific class, <span class="math inline">\hat{y}</span>. (This is known as the <em>discriminative view</em> of classification. We will take on the <em>generative view</em> later in the semester when we tackle Naïve Bayes classifiers.) To get from <span class="math inline">x</span> to <span class="math inline">\hat{y}</span>, we need a function that describes the <em>probability</em> of the class occurring over a range of ERP measurement values.</p>
<section id="a-quick-probability-primer" class="level3">
<h3 class="anchored" data-anchor-id="a-quick-probability-primer">A quick probability primer</h3>
<p>Probabiltilies describe how likely events are to occur. They range from 0, for events that never happen, to 1, for events that are guaranteed to happen. When quantifying probabilities we do this for a class of events, with the total probability across all events adding up to 1 (which means that at any time one of them has to occur). For instance, in the case of flipping a coin, there is a 0.5 (1/2 or 50%) chance that the coin will come up Heads, and 0.5 that it will be Tails. These are the only possibilities (this is a Platonic coin, so it has no thickness and thus cannot land on its side). A coin flip is a good example of an <em>unconditional probability</em>, which is the same regardless of the circumstances. For this, we would write:</p>
<p><span class="math display">\begin{align}
p(H)&amp;=0.5 \\
p(T)&amp;=0.5 \\
\end{align}
</span></p>
<p>which says that the probability of the coin coming up heads, <span class="math inline">p(H)</span>, is 0.5, and the probability of coming up tails, <span class="math inline">p(T)</span>, is 0.5.</p>
<p>But probabilities can also depend on the situation, such as the probability that you will buy lunch at a certain time. It is more likely that you will purchase lunch at 11:30 AM than at 10:00 AM. This is a <em>conditional probability</em>. Conditional probabilities are expressed as <span class="math inline">P(Lunch|Time)</span>, which translates as the probability of going for Lunch, (<span class="math inline">Lunch</span>), is conditional, <span class="math inline">|</span>, on the time, <span class="math inline">Time</span>. For a conditional probability we need to know the time to give the probability that we are going to lunch.</p>
<p>In the case of our ERP decoder, you can say that the probability of a trial having a cue is conditional on the strength of the ERP, p(Cue|ERP). For this, we need an equation that describes how the probability of being a Cue trial varies as a function of ERP strength.</p>
</section>
<section id="logistic-function" class="level3">
<h3 class="anchored" data-anchor-id="logistic-function">Logistic function</h3>
<p>One equation that is a useful way to express a conditional probability is the <em>logistic function</em> (also known as the sigmoid function). It has the form:</p>
<p><span class="math display"> \sigma(x) = \frac{1}{1+e^{-x}} </span></p>
<p>Let’s code it up and visualize it:</p>
<div id="48276b1a" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a logistic function</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic(x):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x)) </span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the logistic function</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>plt.plot(x, logistic(x))</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic Function'</span>)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>plt.yticks(np.arange(<span class="dv">0</span>, <span class="fl">1.1</span>, <span class="fl">0.25</span>))</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-25-output-1.png" width="597" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This function is bounded between 0 and 1, just like probabilities. However, by itself it is not useful. It also has a probability of 0.5 when <span class="math inline">x=0</span>, which is not good for us because our decision point, called the <em>location parameter</em>, for a cue trial usually had our ERP measure at a positive value.</p>
<p>To shift the location, we can modify x by subtracting the location value from it.</p>
<p><span class="math display"> \sigma(x) = \frac{1}{1+e^{-(x-loc)}}</span></p>
<p>How does this look?</p>
<div id="a8d57bb5" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create logistic functio with adjustable location</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic(x,loc<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>(x<span class="op">-</span>loc)))</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot logistic function with different locations</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>loc_list <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">5</span>,<span class="dv">6</span>,<span class="fl">2.5</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">100</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ind, loc <span class="kw">in</span> <span class="bu">enumerate</span>(loc_list):</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    plt.plot(x,logistic(x,loc),label<span class="op">=</span><span class="st">'$loc=</span><span class="sc">{}</span><span class="st">$'</span>.<span class="bu">format</span>(loc), color<span class="op">=</span>[ind<span class="op">/</span><span class="bu">len</span>(loc_list), <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>plt.yticks(np.arange(<span class="dv">0</span>,<span class="fl">1.1</span>,<span class="fl">0.25</span>))</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x$'</span>)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$\sigma(x)$'</span>)</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic function with different locations'</span>)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="dv">2</span>, borderaxespad<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-26-output-1.png" width="746" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This looks promising, we can shift the point at which the logistic function crosses 0.5. However, the range of <span class="math inline">x</span> values over which the logistic function varies, its <em>scale</em>, is narrow compared to the range of values we get for our dot product measures of the ERP. This may not seem important now, since a simple classification threshold at 0.5 probability does not care about the spread in our logistic function. But, it does matter if we want our function to characterize <span class="math inline">p(Cue|ERP)</span>, which can indicate how confident the model is in its prediction of whether a cue was present on a trial. Adjusting the scale is straightforward with the addition of a new parameter.</p>
<p><span class="math display"> \sigma(x)=\frac{1}{1+e^{-\frac{x-loc}{scale}}} </span></p>
<p>By dividing <span class="math inline">x-loc</span> by <span class="math inline">scale</span>, we can stretch or contract the logistic function with respect to the x-axis. As you increase <span class="math inline">scale</span>, the values of <span class="math inline">x</span> have to get larger to push the output closer to 0 or 1. If you decrease <span class="math inline">scale</span>, when only a small change in <span class="math inline">x</span> is needed to have the logistic function return 0 or 1. We can see this below:</p>
<div id="bf29782a" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic function with location and scale parameters</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic(x, loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(x <span class="op">-</span> loc) <span class="op">/</span> scale))</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the logistic function</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>loc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>scale_list <span class="op">=</span> np.power(<span class="fl">2.0</span>, <span class="bu">range</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>))</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ind, scale <span class="kw">in</span> <span class="bu">enumerate</span>(scale_list):</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, logistic(x, loc, scale), label<span class="op">=</span><span class="st">'$scale$ = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(scale), color<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">0</span>, ind <span class="op">/</span> <span class="bu">len</span>(scale_list)])</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>plt.yticks(np.arange(<span class="dv">0</span>, <span class="fl">1.1</span>, <span class="fl">0.25</span>))</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x$'</span>)</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$\sigma(x)$'</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic function with different scales'</span>)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="dv">2</span>, borderaxespad<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-27-output-1.png" width="755" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Great, we can change the spread of the logistic function now. With these two degrees of freedom, location and scale, we are able to create a function that captures the probability of a certain class (in our case, a cue trial) occurring. An important caveat to this function is that it is monotonic, meaning that the probability of being that class only increases (or decreases) as one increases the value of <span class="math inline">x</span>. This works for most cases, but not if our class occurs for only a restricted range of <span class="math inline">x</span> values (e.g.&nbsp;between 2 and 5). In later lectures we will cover other types of classifiers that circumvent this limitation.</p>
</section>
</section>
<section id="fitting-the-logistic-function" class="level2">
<h2 class="anchored" data-anchor-id="fitting-the-logistic-function">Fitting the logistic function</h2>
<p>Using the logistic function requires us to choose values for the location and scale parameters, which we <em>could</em> try doing by hand. That is not recommended. Instead, we will use the <code>LogisticRegression</code> class in the scikit-learn package. This class defines an object that can take our measurements and true classes, fit a logistic function to that data, and then deliver class predictions. To start using this, we will cover how scikit-learn implements its fitting functions in general, and then the specifics of the <code>LogisticRegression</code> class.</p>
<section id="scikit-learn-standards" class="level3">
<h3 class="anchored" data-anchor-id="scikit-learn-standards">Scikit-learn standards</h3>
<p>Scikit-learn is a python package started in 2007 and has grown to include a wide variety of machine learning algorithms. Most of these are implemented as <em>estimators</em>, which are classes that allow one to fit a model or function to some data and then make predictions from that model. The project has adopted a uniform standard for the creation of estimators, making it easier to incorporate new ones into the project or develop your own that will comprehensible to users already familiar with scikit-learn. For details you can check out the <a href="https://scikit-learn.org/stable/developers/develop.html">online documentation for developers</a>.</p>
<p>Each estimator has an <code>__init__</code> method that creates the estimator object. When calling this method, you can pass settings parameters that determine how the estimator will fit to the data. Generally, these settings are supposed to be independent of the data being fit to. The estimator then has a <code>fit</code> function, which accepts a data matrix <code>X</code> and predicted classes <code>y</code>. Additional parameters can be set here that affect the fitting process in ways specific to the data. Once the <code>fit</code> function has been called, you can evaluate the performance of the fit using the <code>score</code> method, or predict new classes from new data using the <code>predict</code> method.</p>
<p>We will step through these using the <code>LogisticRegression</code> class to fit our predictor of of whether a cue was present on a trial given the ERP.</p>
</section>
<section id="initializing" class="level3">
<h3 class="anchored" data-anchor-id="initializing">Initializing</h3>
<p>To use the <code>LogisticRegression</code> class, we need to create an instance of it as an object. Parameters specifying how to fit the function to your data, using set using the <code>__init__</code> method, are also accessible with a <code>set_params</code> method after you created the estimator. When no parameters are given to initialize the object, it takes on default values. These can be inspected in the <a href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html">documentation</a> or using the estimator’s <code>get_params</code> method.</p>
<div id="02021c36" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a vanila LogisticRegression object</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression()</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># examine its default fitting parameters</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>clf.get_params()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>{'C': 1.0,
 'class_weight': None,
 'dual': False,
 'fit_intercept': True,
 'intercept_scaling': 1,
 'l1_ratio': None,
 'max_iter': 100,
 'multi_class': 'deprecated',
 'n_jobs': None,
 'penalty': 'l2',
 'random_state': None,
 'solver': 'lbfgs',
 'tol': 0.0001,
 'verbose': 0,
 'warm_start': False}</code></pre>
</div>
</div>
<p>The only parameter worth mentioning at this time is <code>fit_intercept</code>, which determines if we will include a location argument in our fitting. By default this is set to True, so we don’t have to worry about explicitly setting it.</p>
</section>
<section id="fitting" class="level3">
<h3 class="anchored" data-anchor-id="fitting">Fitting</h3>
<p>Now that we have our <code>LogisticRegression</code> object, we can call its <code>fit</code> method. It accepts arrays containing your independent, <span class="math inline">x</span>, and dependent, <span class="math inline">y</span> variables and optimizes the function to best fit that data. The first parameter is <code>X</code>, which is an array of measurements. It has 2 dimensions, with each row a different <em>sample</em> (e.g.&nbsp;trial), and each column a different <em>feature</em> (e.g.&nbsp;ERP peak voltage). Its shape is (n_samples, n_features), where ‘n’ stands for ‘number of …’. You can have multiple features per sample, which allows us to use more than one aspect of the measured brain activity on a trial to decode whether a stimulus was present. For now we will just use the one, our measure of ERP strength on a given trial.</p>
<p>The next parameter is <code>y</code>, the true class labels (e.g.&nbsp;Cue or NoCue) labels. Its shape is (n_samples,), where n_samples is the same as <code>X</code>. Notice that <code>X</code> is uppercase and <code>y</code> is lowercase. This is because a common convention is that a matrix (a 2-D array) is represented by an uppercase character, while vectors (1-D array) use lowercase characters.</p>
<div id="c809d16f" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># format X</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="co"># add dimension to X, because X needs to be a 2D array</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> vals[:, np.newaxis]</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The shape of X is </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(vals.shape))</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co"># class_act is already formatted correctly</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The shape of y is </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(class_act.shape))</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>clf.fit(vals, class_act)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The shape of X is (54, 1)
The shape of y is (54,)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="28">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</a></style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LogisticRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html">?<span>Documentation for LogisticRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LogisticRegression()</pre></div> </div></div></div></div>
</div>
</div>
<p>That’s it, we fit the model! A bit anticlimactic, since all we did was update the <code>clf</code> object with a fitted model. The next thing we need to do is evaluate the performance of the fitted model.</p>
</section>
<section id="evaluating" class="level3">
<h3 class="anchored" data-anchor-id="evaluating">Evaluating</h3>
<p>There are couple ways to judge the performance of our model. When we implemented our simple threshold decision model above, we used error rate and accuracy as our measures. The <code>LogisticRegression</code> class provides a method named <code>score</code> that calculates accuracy. It accepts <code>X</code> and <code>y</code> parameters similar to the <code>fit</code> method. Note: you do not have to provide the same <code>X</code> and <code>y</code> used for <code>fit</code>, but can use new data that the model was not trained on (in fact, this is a better practice as we will discuss later). It returns a number between 0 and 1, with 1 being perfect classification, and 0 being totally incorrect classification. This single number provides a good top line indication of whether the fit was successful. Poor accuracy will usually not be at 0, but at a level consistent with random guessing (typically 1 divided by the number of options). For binary decoding with equal numbers of each class, this would be 0.5 (1/2).</p>
<div id="96c8d8a4" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the score for our model</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> clf.score(vals, class_act)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy for our model: </span><span class="sc">{:.2f}</span><span class="st">%'</span>.<span class="bu">format</span>(score<span class="op">*</span><span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy for our model: 87.04%</code></pre>
</div>
</div>
<p>The logistic regression decoder out performs our simple threshold classifier we set by eye, which was 85.2%. Not by much, but better nonetheless. Just as a sanity check, let’s see what happens if we scramble the actual class labels. In this case, the performance of the classifier should be random, since we have removed any correspondence between the ERP strength and whether the trial had a cue or not.</p>
<div id="2c75c9e5" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>clf_rand <span class="op">=</span> LogisticRegression()</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>clf_rand.fit(vals, np.random.permutation(class_act))</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>score_rand <span class="op">=</span> clf_rand.score(vals, np.random.permutation(class_act))</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Random prediction accuracy: </span><span class="sc">%.2f%%</span><span class="st">"</span> <span class="op">%</span> (score_rand <span class="op">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random prediction accuracy: 53.70%</code></pre>
</div>
</div>
<p>If we run this cell a few times we can see that the average accuracy is around 50%, which is what we would expect from random performance. However, random performance of a binary classifier does not need to be at 50%. If there are more of one of the classes over the other, then the decoder could perform better than 50%. Let’s remove a bunch of the No-cue trials and see if that can affect our chance performance.</p>
<div id="4855eb4a" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create new logistic regression classifier for unbalanced data set</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>clf_ub <span class="op">=</span> LogisticRegression()</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># remove some no-cue trials</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>vals_ub <span class="op">=</span> vals[:<span class="op">-</span><span class="dv">20</span>].copy() <span class="co"># remove last 20 trials, which are no-cue trials</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>class_act_ub <span class="op">=</span> class_act[:<span class="op">-</span><span class="dv">20</span>].copy() <span class="co"># remove last 20 trials, which are no-cue trials</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>clf_ub.fit(vals_ub, class_act_ub)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>class_pred_ub <span class="op">=</span> clf_ub.predict(vals_ub)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>score_ub <span class="op">=</span> clf_ub.score(vals_ub, class_act_ub)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy for unbalanced model: </span><span class="sc">{:.2f}</span><span class="st">%'</span>.<span class="bu">format</span>(score_ub<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="co"># randomly permute (shuffle) the class labels</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">43</span>)</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>class_rand_ub <span class="op">=</span> np.random.permutation(class_act_ub)</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>clf_ub.fit(vals_ub, class_rand_ub)</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>score_rand_ub <span class="op">=</span> clf_ub.score(vals_ub, class_rand_ub)</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy for unbalanced random model: </span><span class="sc">{:.2f}</span><span class="st">%'</span>.<span class="bu">format</span>(score_rand_ub<span class="op">*</span><span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy for unbalanced model: 94.12%
Accuracy for unbalanced random model: 73.53%</code></pre>
</div>
</div>
<p>Notice that the accuracy has gone up for our model, even for the random case! This is because the model optimized its performance simply by biasing its response to indicate Cue trials instead of No-cue trials. This works because the data set is unbalanced, with much fewer No-cue trials compared with Cue trials.</p>
<p>Is there a way to account for this? The metrics section of scikit-learn provides a range of performance score functions that measure other aspects of classifier performance or control for issues in the data set. In this case, we are dealing with unbalanced data. To address that, we can use the function <code>balanced_accuracy_score</code>. It works by calculating the accuracy for each class separately, and then taking the average across classes. In this way, if one class is overexpressed and classified correctly more often due to bias, then its contribution to the accuracy score will be downgraded, while the less well represented class will have its score enhanced. Specifically, for the binary classification we are doing here, the is calculated using the following equation:</p>
<p><span class="math display"> Balanced Score = \frac{1}{2} \left( \frac{TP}{TP+FN}+\frac{TN}{TN+FP}  \right) </span></p>
<p>Here we will introduce a few new terms that are going to come up in discussing classifiers. Each trial has an actual condition or label associated with it, and a label predicted by the model. The terminology here comes from the signal detection literature, where one is trying to determine if a particular event occurred or did not on a given trial. Our case fits into this framework nicely: was a cue present or not. When a cue is present we refer to it as a <em>Positive</em> trial, while the No-cue trial is a <em>Negative</em> trial. When the classifier correctly labels a trial, we say that trial is <em>True</em>, and incorrectly labeling a trial makes it <em>False</em>. For instance, a true positive trial would have the cue and be labeled as a cue trial by the logistic regression model. We can list all possible trial types using this phrasing.</p>
<ol type="1">
<li>TP - <strong>T</strong>rue <strong>P</strong>ositive, event occurred and was predicted as occurring * Actual Cue / Predicted Cue</li>
<li>TN - <strong>T</strong>rue <strong>N</strong>egative, event did not occur and was predicted as not occurring * Actual No-cue / Predicted No-cue</li>
<li>FP - <strong>F</strong>alse <strong>P</strong>ostive, event did not occur but was predicted as occurring * Actual No-cue / Predicted Cue</li>
<li>FN - <strong>F</strong>alse <strong>N</strong>egative, event occurred but was predicted as not occurring * Actual Cue / Predicted No-cue</li>
</ol>
<p>Looking back at the equation for balanced scores, we can break it down into something a bit more interpretable:</p>
<p><span class="math display"> \begin{align}
    \notag \frac{TP}{TP+FN} &amp;= \frac{\text{Number of correctly predicted cue labeled trials}}{\text{Number of all actual cue labeled trials}} = \text{Proportion correctly labeled cue trials} \\
    \notag \frac{TN}{TN+FP} &amp;= \frac{\text{Number of correctly predicted no-cue labeled trials}}{\text{Number of all actual no-cue labeled trials}} = \text{Proportion correctly labeled no-cue trials} \\
    \end{align}
</span></p>
<p>This means the balanced score equation is just taking the average correct proportion across both classes, regardless of how many trials belonged to either class.</p>
<p>If you are working with unbalanced data, then using the balanced accuracy score is essential for interpretable accuracy scores. When doing that, you should use the built in one provided by scikit-learn, the function <code>balanced_accuracy_score</code>. An additional advantage of the scikit-learn version is that it supports classifiers with more than two outcomes, which we will make use of later in the semester.</p>
<div id="3a53cb81" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate balanced accuracy scores using the function provided by scikit-learn</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>score_bal_ub <span class="op">=</span> balanced_accuracy_score(class_act_ub, class_pred_ub)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Balanced accuracy score for our model: </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(score_bal_ub<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>score_bal_ub <span class="op">=</span> balanced_accuracy_score(class_act_ub, class_rand_ub)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Balanced accuracy score for random model: </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(score_bal_ub<span class="op">*</span><span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Balanced accuracy score for our model: 88.89
Balanced accuracy score for random model: 54.67</code></pre>
</div>
</div>
<p>The accuracy has gone down now, with the random case close to 50%, as would be expected. Normally we will use balanced data sets so there should not be much of a difference between the score returned by the estimator object and the balanced accuracy metric.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Coding our own balanced accuracy score function
</div>
</div>
<div class="callout-body-container callout-body">
<p>To better understand the balanced accuracy score, we can try coding our own. This should make things a bit clearer.</p>
<div id="92a1eebe" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># balanced accuracy score in base python</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bal_acc_score(y_act, y_pred):</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize variables</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    tp <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> tn <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> fp <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> fn <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop through each pair of actual and predicted labels</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> act, pred <span class="kw">in</span> <span class="bu">zip</span>(y_act, y_pred):</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> act <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> pred <span class="op">==</span> <span class="dv">1</span>: <span class="co"># true positive</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>            tp <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> act <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> pred <span class="op">==</span> <span class="dv">0</span>: <span class="co"># false negative</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>            fn <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> act <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> pred <span class="op">==</span> <span class="dv">1</span>: <span class="co"># false positive</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>            fp <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> act <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> pred <span class="op">==</span> <span class="dv">0</span>: <span class="co"># true negative</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>            tn <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate proportion of correct predictions for each class</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>    prop_corr_pos <span class="op">=</span> tp <span class="op">/</span> (tp <span class="op">+</span> fn)</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>    prop_corr_neg <span class="op">=</span> tn <span class="op">/</span> (tn <span class="op">+</span> fp)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return average of the two proportions</span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (prop_corr_pos <span class="op">+</span> prop_corr_neg) <span class="op">/</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s make sure our homemade version agrees with the one in scikit-learn.</p>
<div id="cb87d713" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see how our scores have changed using the balanced accuracy score metric</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>score_bal_ub <span class="op">=</span> bal_acc_score(class_act_ub, class_pred_ub)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Balanced accuracy score for our model: </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(score_bal_ub<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>score_bal_ub <span class="op">=</span> bal_acc_score(class_act_ub, class_rand_ub)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Balanced accuracy score for random model: </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(score_bal_ub<span class="op">*</span><span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Balanced accuracy score for our model: 88.89
Balanced accuracy score for random model: 54.67</code></pre>
</div>
</div>
<p>Yay, they agree!</p>
</div>
</div>
<p>The score measures above take actual and predicted class labels, compare and combine them, and return a single number that reflects the performance of our classifier. This is good as a general summary of how well the classifier works, but it leaves out details about how the classifier achieves that performance. For instance, with our case, is our model really good at detecting cue trials, or at no-cue trials? Does performance vary depending on the type of trial?</p>
<p>A way to easily visualize the performance of a classifier in all its gritty details is with a <em>confusion matrix</em>. In a confusion matrix, each row corresponds to an actual label, and each column the predicted label. Each cell in the matrix has the number of trials with that particular pairing of actual and predicted labels. For a binary classifier, this looks like:</p>
<p><span class="math display"> Confusion \: Matrix = \begin{pmatrix}
                        TN &amp;FP \\
                        FN &amp;TP
                    \end{pmatrix}
</span></p>
<p>Along the diagonal, we have the number of trials that were correctly classified for each label type. Off-diagonal elements show the number of confused (i.e.&nbsp;mislabeled) instances, hence the name. The instances most prone to confusion helps us diagnose problems in our classifier. As a case study, let’s examine the confusion matrices for our classifiers trained on balanced and unbalanced data sets.</p>
<p>The metrics portion of the scikit-learn package provides functions for computing confusion matrices.</p>
<div id="a7d620c1" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use balanced model to predict the class for each observation</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>clf.fit(vals, class_act)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>class_pred <span class="op">=</span> clf.predict(vals)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="co"># PLOT THE CONFUSION MATRIX</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the figure</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate confusion matrix</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>confmat_full <span class="op">=</span> confusion_matrix(class_act, class_pred)</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the confusion matrix using built-in function</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>confmat_full, </span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>                       display_labels<span class="op">=</span>[<span class="st">'No-cue'</span>, <span class="st">'Cue'</span>]).plot(ax<span class="op">=</span>ax[<span class="dv">0</span>], cmap<span class="op">=</span>plt.cm.Blues)</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="co"># redo the same for the unbalanced dataset</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>confmat_ub <span class="op">=</span> confusion_matrix(class_act_ub, class_pred_ub)</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>confmat_ub, </span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>                       display_labels<span class="op">=</span>[<span class="st">'No-cue'</span>, <span class="st">'Cue'</span>]).plot(ax<span class="op">=</span>ax[<span class="dv">1</span>], cmap<span class="op">=</span>plt.cm.Blues)</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a><span class="co"># label the figure</span></span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Balanced dataset'</span>)</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Unbalanced dataset'</span>)</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Confusion matrices'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-36-output-1.png" width="939" height="475" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The confusion matrix shows that both classifiers works well. High counts along the diagonal indicate that labels are correctly predicted. For the balanced case, the misclassifications tend to be equal for Cue and No-cue trials. On the other hand, when we decreased the number of No-cue trials misclassifications only occurred for the No-cue trials. Moreover, we had fewer misclassifications for the Cue trials, which were now overrepresented in the data. Thus, the off diagonal elements tell us how the model trained on data with too many Cue trials is biased perform better on that trial type.</p>
<p>That error suggests that the decision boundary of our logistic model is biased towards lower ERP strengths for the unbalanced data. In this way, it will tend to never miss a cue trial, and classify No-cue trials as cue trials. To determine if this is the case, we might want to visualize the logistic functions fitted to our balanced and unbalanced datasets.</p>
<p>To do this, we can use the <code>predict_proba</code> method. This takes samples, not necessarily the ones we fitted on, and returns the probability of either class for each sample. A good strategy here to is take the minimum and maximum values of the features in your dataset, and then create a grid of samples that span their range. This works really well when we have less than 3 features per sample, or in our case just 1 feature. If we plot the predicted probabilities across values of <span class="math inline">x</span>, we can visualize the logistic function.</p>
<div id="c5473aea" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create range of feature values</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>min_erp_peak <span class="op">=</span> np.<span class="bu">min</span>(vals)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>max_erp_peak <span class="op">=</span> np.<span class="bu">max</span>(vals)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>erp_peak_vals <span class="op">=</span> np.linspace(min_erp_peak, max_erp_peak, <span class="dv">100</span>)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>erp_peak_vals <span class="op">=</span> erp_peak_vals[:, np.newaxis]</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="co"># predict probabilities over range of values</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>pred_probs <span class="op">=</span> clf.predict_proba(erp_peak_vals)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The shape of pred_probs is: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(pred_probs.shape))</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co"># plot predicted probability of each class</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>plt.plot(erp_peak_vals, pred_probs[:,<span class="dv">0</span>], label<span class="op">=</span><span class="st">'NoCue'</span>)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>plt.plot(erp_peak_vals, pred_probs[:,<span class="dv">1</span>], label<span class="op">=</span><span class="st">'Cue'</span>)</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'ERP Peak (uV)'</span>)</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted Probability'</span>)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Predicted Probability vs. ERP Peak'</span>)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>plt.yticks(np.arange(<span class="dv">0</span>,<span class="fl">1.1</span>,<span class="fl">0.25</span>))</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="dv">2</span>, borderaxespad<span class="op">=</span><span class="fl">0.</span>)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The shape of pred_probs is: (100, 2)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-37-output-2.png" width="715" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note that <code>predict_proba</code> returns a matrix with the same number of rows as the samples we passed in <code>erp_peak_vals</code>, and two columns, one for each class (NoCue or Cue). Since we only have two classes, the probabilities of NoCue and Cue will add to 1 for each sample. This makes one of them redundant, so for future plotting we will drop the NoCue one.</p>
<p>It is helpful to plot our classes with the data points layered on top, which helps us better visualize the performance of the fit and whether it is falling prey to some issues that will be discussed later.</p>
<div id="d4007f54" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_logistic(lf_obj, vals, class_act, ax<span class="op">=</span><span class="va">None</span>, labels<span class="op">=</span>[<span class="st">'NoCue'</span>, <span class="st">'Cue'</span>], color<span class="op">=</span><span class="st">'tab:blue'</span>):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Plot the logistic function for a given logistic regression object</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="co">    and a range of values.</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a><span class="co">    lf_obj : sklearn.linear_model.LogisticRegression</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="co">        The logistic regression object.</span></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="co">    vals : array-like</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="co">        The X values used to train the logistic model.</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a><span class="co">    classes : array-like</span></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="co">        The y values used to train the logistic model.</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co">    ax : matplotlib.axes.Axes</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a><span class="co">        The axes to plot on.</span></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>        _, ax <span class="op">=</span>  plt.subplots()</span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>    min_val <span class="op">=</span> np.<span class="bu">min</span>(vals)</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>    max_val <span class="op">=</span> np.<span class="bu">max</span>(vals)</span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>    val_grid <span class="op">=</span> np.linspace(min_val, max_val, <span class="dv">100</span>)</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a>    val_grid <span class="op">=</span> val_grid[:, np.newaxis]</span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># predict probabilities over range of values</span></span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a>    lf_obj.fit(vals, class_act) <span class="co"># refit the model, because for some reason passing it as an argument removes the fit</span></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a>    pred_probs <span class="op">=</span> lf_obj.predict_proba(val_grid)</span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>    ax.plot(val_grid, pred_probs[:,<span class="dv">1</span>], label<span class="op">=</span><span class="st">'Logistic model'</span>, color<span class="op">=</span>color)</span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>    ax.scatter(vals, class_act, c<span class="op">=</span>color, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'True data'</span>)</span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks(np.arange(<span class="dv">0</span>,<span class="fl">1.1</span>,<span class="fl">0.25</span>))</span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a>    ax.set_yticklabels([labels[<span class="dv">0</span>], <span class="st">'0.25'</span>, <span class="st">'0.5'</span>, <span class="st">'0.75'</span>, labels[<span class="dv">1</span>]])</span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a>    ax.grid()</span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">1</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a>plot_logistic(clf, vals, class_act, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a>plot_logistic(clf_ub, vals_ub, class_act_ub, ax<span class="op">=</span>ax[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Logistic regression'</span>)</span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a>fig.supxlabel(<span class="st">'ERP dot product (uV)'</span>)</span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'P(Cue|ERP)'</span>)</span>
<span id="cb59-40"><a href="#cb59-40" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Balanced data'</span>)</span>
<span id="cb59-41"><a href="#cb59-41" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Unbalanced data'</span>)</span>
<span id="cb59-42"><a href="#cb59-42" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-38-output-1.png" width="663" height="485" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The balanced logistic function seems to reflect the p(Cue|ERP) quite well. Its probability increases strongly after the bulk of the NoCue ERP peaks, and is near 1 when only Cue ERP peak values remain. The one trained on the unbalanced is shifted to the left, towards lower ERP values, biasing its classification towards the overrepresented Cue trials.</p>
<p>Moving beyond this qualitative way of describing our logistic fit, it would be better to do so quantitatively, using the location parameter we discussed before. However, here we will have to make things a little more complicated by changing how we parameterize the model. This initial pain is worth it, though, because later it will give us far more freedom in how we can use the logistic function.</p>
</section>
<section id="interpreting" class="level3">
<h3 class="anchored" data-anchor-id="interpreting">Interpreting</h3>
<p>The parameters of the logistic function tell us how it assigns a class probability to a measure. When we first introduced it, we used location and scale parameters. These are not used by the <code>LogisticRegression</code> class. Instead, it is formulated as:</p>
<p><span class="math display"> \sigma(x) = \frac{1}{1+e^{-(b+wx)}} </span></p>
<p>Here <span class="math inline">b</span> stands for the <em>bias</em> or <em>intercept</em>, which is similar, but not exactly the same as the location parameter. <span class="math inline">w</span> is the <em>slope</em> of the dependence on <span class="math inline">x</span>, and is similar, but not exactly the same as the scale parameter.</p>
<p>How do these new parameters affect the logistic function?</p>
<div id="0fda6edb" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic(x, b<span class="op">=</span><span class="dv">0</span>, w<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(w <span class="op">*</span> x <span class="op">+</span> b)))</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the logistic function as b is varied</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>b_list <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">5</span>,<span class="dv">6</span>,<span class="fl">2.5</span>)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, b <span class="kw">in</span> <span class="bu">enumerate</span>(b_list):</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, logistic(x, b<span class="op">=</span>b), label<span class="op">=</span><span class="ss">f'$b$=</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">'</span>, color<span class="op">=</span>[idx<span class="op">/</span><span class="bu">len</span>(b_list), <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co">#plot legend outside axes</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>plt.yticks(np.arange(<span class="dv">0</span>,<span class="fl">1.1</span>,<span class="fl">.25</span>))</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x$'</span>)</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$\sigma(x)$'</span>)</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic Function as $b$ is varied'</span>)</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-39-output-1.png" width="725" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looks like <span class="math inline">b</span> has exactly the same effect as location. What if we vary <span class="math inline">w</span>?</p>
<div id="4c33f995" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot logistic function as w is varied</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>w_list <span class="op">=</span> np.power(<span class="fl">2.0</span>, np.arange(<span class="op">-</span><span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, w <span class="kw">in</span> <span class="bu">enumerate</span>(w_list):</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, logistic(x, b<span class="op">=</span><span class="dv">0</span>, w<span class="op">=</span>w), label<span class="op">=</span><span class="st">'$w$=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(w), color<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">0</span>,idx<span class="op">/</span><span class="bu">len</span>(w_list)])</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>plt.yticks(np.arange(<span class="dv">0</span>, <span class="fl">1.1</span>, <span class="fl">0.25</span>))</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'center left'</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x$'</span>)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$\sigma(x)$'</span>)</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic Function as $w$ is varied'</span>)</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-40-output-1.png" width="704" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It may look the same, but look closely at the line colors. When we increased the scale parameter, the spread of the logistic function decreased. But in this case, increasing the <span class="math inline">w</span> parameter decreases the spread. And it gets worse, look what happens when we vary <span class="math inline">w</span> and <span class="math inline">b</span> is not set to 0:</p>
<div id="ba4a88be" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot logistic function as w is varied and b is not zero</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>new_b <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, w <span class="kw">in</span> <span class="bu">enumerate</span>(w_list):</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, logistic(x, b<span class="op">=</span> new_b, w<span class="op">=</span>w), label<span class="op">=</span><span class="st">'$w$=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(w), color<span class="op">=</span>[<span class="dv">0</span>,idx<span class="op">/</span><span class="bu">len</span>(w_list),<span class="dv">0</span>])</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>plt.yticks(np.arange(<span class="dv">0</span>, <span class="fl">1.1</span>, <span class="fl">0.25</span>))</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'center left'</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x$'</span>)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$\sigma(x)$'</span>)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic Function as $w$ is varied and $b$ is fixed at </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(new_b))</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-41-output-1.png" width="704" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>When <span class="math inline">b</span> is not at zero, then changing <span class="math inline">w</span> affects the location of the logistic function. You can see this if you follow where each curve crosses the 0.5 line, which is the threshold for classifying a trial as receiving a Cue. We can use some simple algebra to resolve the relationship between location, <span class="math inline">w</span>, and <span class="math inline">b</span>.</p>
<p>First, recognize that the <em>loc</em> parameter specifies when the logistic function is equal to 0.5. This happens when <span class="math inline">x = 0</span>:</p>
<p><span class="math display"> \begin{align}
        \notag 0.5&amp;=\frac{1}{1+e^{-x}} \\
        \notag 0.5&amp;=\frac{1}{1+e^{-0}} \\
        \notag 0.5&amp;=\frac{1}{1+1} \\
        \notag 0.5&amp;=\frac{1}{2}
    \end{align}
</span></p>
<p>How does <span class="math inline">b</span> and <span class="math inline">w</span> relate to the <span class="math inline">x=0</span> location? <span class="math display"> \begin{align}
        \notag 0&amp;=-(b+wx) \\
        \notag 0&amp;=-b-wx \\
        \notag b&amp;=-wx \\
        \notag -\frac{b}{w}&amp;=x
    \end{align}
</span></p>
<p>The location parameter is important to us when interpreting the logistic function because it tells us when the classifier will label an ERP as arising from a Cue. It is the <em>decision boundary</em>. To get that, we need to pull out the <span class="math inline">b</span> and <span class="math inline">w</span> parameters from the fitted <code>LogisticRegression</code> object.</p>
<div id="27e1afd6" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get b, also known as the intercept parameter</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> clf.intercept_</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The shape of intercept_ is: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(b.shape))</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The intercept of our model is </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(b[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The shape of intercept_ is: (1,)
The intercept of our model is -4.37</code></pre>
</div>
</div>
<p><code>intercept_</code> is a class variable created once we call the <code>fit</code> method. It is a numpy array with a single value. To get <span class="math inline">w</span>, access the <code>coef_</code> variable.</p>
<div id="b6557c45" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get w from coef_</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> clf.coef_</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The shape of coef_ is: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(w.shape))</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The value of coef_ is: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w[<span class="dv">0</span>,<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The shape of coef_ is: (1, 1)
The value of coef_ is: 0.13</code></pre>
</div>
</div>
<p>Now that we have <span class="math inline">b</span> and <span class="math inline">w</span>, we can precisely position the decision boundary for our fitted logistic function.</p>
<div id="d4b99b3f" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the decision boundary for balanced model</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>dec_bound <span class="op">=</span> <span class="op">-</span>b[<span class="dv">0</span>]<span class="op">/</span>w[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the decision boundary for unbalanced model</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>b_ub <span class="op">=</span> clf_ub.intercept_</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>w_ub <span class="op">=</span> clf_ub.coef_</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>dec_bound_ub <span class="op">=</span> <span class="op">-</span>b_ub[<span class="dv">0</span>]<span class="op">/</span>w_ub[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print comparison of decision boundaries</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Decision boundary for balanced model: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(dec_bound))</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Decision boundary for unbalanced model: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(dec_bound_ub))</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the logistic regression with decision boundaries marked</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">1</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>plot_logistic(clf, vals, class_act, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>plot_logistic(clf_ub, vals_ub, class_act_ub, ax<span class="op">=</span>ax[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Logistic regression'</span>)</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>fig.supxlabel(<span class="st">'ERP dot product (uV)'</span>)</span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'P(Cue|ERP)'</span>)</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Balanced data'</span>)</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Unbalanced data'</span>)</span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].axvline(dec_bound, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].axvline(dec_bound_ub, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Decision boundary for balanced model: 32.88
Decision boundary for unbalanced model: 23.89</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-44-output-2.png" width="663" height="485" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Using the logistic model’s internal parameters, we can explicitly calculate the decision boundary for classifying Cue trials. Doing this, we found that the model trained on balanced data has a boundary ~9 uV higher than the one trained on unbalanced data. This underscores the importance of considering the data ones trains the model on. Biases in the data will be reflected in the trained model, and in many cases impair predictions on new data it had not been trained on.</p>
</section>
<section id="prediction" class="level3">
<h3 class="anchored" data-anchor-id="prediction">Prediction</h3>
<p>Once the model is fitted, you want to use it to fit new samples. Since we used all our data to fit the model, predicting on the same data would be circular. The model was optimized to perform best on the data set it was trained on, not the data it will receive following training. By chance, there may be tendencies in the training data that are not expressed in newly acquired trials. Since we do not have a time machine that allows us to go into the future and acquire the data the model will eventually have to classify (which, for that matter, would eliminate the need for a model anyway), we need a way to estimate how the model will perform on data it has not been trained on.</p>
<p>This is a well-recognized problem in evaluating the performance of statistical models, with a well-established solution - <em>cross-validation</em>. Instead of training the model on all our data, we train on a subset of the data, and test on the remainder that was held out. To do this, we need to divide our data into <em>train</em> and <em>test</em> sets, recalculate the mean ERP using the training set, fit the logistic model using the training set, and test the performance on the test set.</p>
<p>Scikit-learn provides a collection of functions for subdividing data and some of their estimators have versions with built-in cross-validation. We will make use of those, but first let’s code our own just to get a sense of how they work. To start, we will code a function to split our data into train and test sets.</p>
<p>When specifying the size of the training and test datasets, we usually decide how many sets (or folds), <span class="math inline">k</span>, we want to split the data set into. The model is trained on the most data possible, k-1 sets (<span class="math inline">\frac{k-1}{k}</span> % of data), and the last set is held out for testing (<span class="math inline">\frac{1}{k}</span> % of data). Deciding on a value for <span class="math inline">k</span> requires balancing two needs. First, the model should be exposed to the most training data possible, so that it can be optimized on a representative sample. This inclines us to set <span class="math inline">k</span> to a high value, so that a larger fraction of the data goes in to training. Second, we want to test the model on a representative sample as well, so that we get a more precise estimate of its performance. This inclines us to set <span class="math inline">k</span> to a low value, which maximizes the data we test on. These conflicting demands need to be balanced based on the number of features the model is being trained on, how large the data set is, and the tendency of the model to overfit. All those issues will vary across situations, but a good rule of thumb is to set <span class="math inline">k=5</span>. This means that 80% (4/5) of your data will be used for training, while the remaining 20% (1/5) is held in reserve for testing.</p>
<p>When splitting our data, we also must decide how to do the actual splitting. The simplest approach is to split the data into sets based on their order. In that case, the first <span class="math inline">\frac{4}{5}\text{n\_samples}</span> are used for training, and the remaining <span class="math inline">\frac{1}{5}\text{n\_samples}</span> for testing. If you data is randomly ordered, so that different classes are equally represented at the beginning and end of the dataset, then this will typically work. On the other hand, if you data is like ours, where the first half of the samples are Cue trials, and the second half No-cue trials, then your train and test data sets will have different proportions of trial types and not be representative samples of the entire data set. To account for this, the samples you assign to the train or test data sets can be chosen randomly. On average, this should result in an equal proportion of samples for the different classes in the train and test sets. Let’s implement that:</p>
<div id="1b166d43" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate indices for train/test sets using k-fold random assignment</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_test_kfold(n_samples, k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># n_samples, number of samples in the dataset</span></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># k, int, number of folds</span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>    fold_size <span class="op">=</span> n_samples<span class="op">/</span>k <span class="co"># number of samples in each fold</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    test_bool <span class="op">=</span> np.arange(<span class="dv">0</span>, n_samples)<span class="op">&lt;=</span>fold_size <span class="co"># boolean array of test indices</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>    test_bool <span class="op">=</span> np.random.permutation(test_bool) <span class="co"># randomize the test indices</span></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    test_idxs <span class="op">=</span> np.where(test_bool<span class="op">==</span><span class="va">True</span>)[<span class="dv">0</span>] <span class="co"># get the indices of the test set</span></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>    train_idxs <span class="op">=</span> np.where(test_bool<span class="op">==</span><span class="va">False</span>)[<span class="dv">0</span>] <span class="co"># get the indices of the train set</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_idxs, train_idxs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our function takes just two arguments, the number of samples, <code>n_samples</code>, and the number of folds, <code>k</code>, and returns randomly chosen indices for the test (<code>test_idxs</code>) and train (<code>train_idxs</code>) sets. Because we are assigning samples to the train and test groups at random, we should get equal proportions of Cue trials (or No-cue trials) in both sets. Before going further, we should make sure that is the case.</p>
<div id="5b6f2b69" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>num_runs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>prop_cue_test <span class="op">=</span> np.zeros(num_runs)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>prop_cue_train <span class="op">=</span> np.zeros(num_runs)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>    test_idxs, train_idxs <span class="op">=</span> train_test_kfold(<span class="bu">len</span>(class_act))</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    prop_cue_test[i] <span class="op">=</span> class_act[test_idxs].mean()</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>    prop_cue_train[i] <span class="op">=</span> class_act[train_idxs].mean()</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">1</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(prop_cue_test, bins<span class="op">=</span>np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">20</span>), alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(prop_cue_train, bins<span class="op">=</span>np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">20</span>), alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Test set, mean proportion = </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(prop_cue_test.mean()))</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Train set, mean proportion = </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(prop_cue_train.mean()))</span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'Count'</span>)</span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>fig.supxlabel(<span class="st">'Proportion of cues'</span>)</span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a><span class="co"># proportion of Cue trials across all trials</span></span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Proportion of Cue trials across all trials: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(class_act.mean()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-46-output-1.png" width="663" height="480" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Proportion of Cue trials across all trials: 0.46</code></pre>
</div>
</div>
<p>On average, random assignment of trials yields equal proportions of Cue trials to test and train sets. The variance is greater, though, for the smaller test set. Sometimes the test set will be dominated by Cue or No-cue trials, so for those cases will not offer a balanced assessment of classifier performance. To avoid this, we want to <em>stratify</em> assignment of trials to the test and train sets by the trial type. To do this, we will generate train and test sets for each class of trials separately, and then combine them.</p>
<div id="bc3c873c" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate indices for train/test sets using stratified k-fold random assignment</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_test_kstrat(class_act, k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># class_act, array-like, shape (n_samples,)</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># k, int, number of folds</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gets unique classes</span></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    uniq_classes <span class="op">=</span> np.unique(class_act) </span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize empty array for train indices</span></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>    train_idxs <span class="op">=</span> np.array([]) </span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for each class, get indices of samples in class_ and add k-1/k of them to train_idxs</span></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> class_ <span class="kw">in</span> uniq_classes: <span class="co"># variable named class_ to avoid confusion with 'class' keyword</span></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get indices of samples in class_</span></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>        class_idxs <span class="op">=</span> np.where(class_act<span class="op">==</span>class_)[<span class="dv">0</span>] </span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># number of samples in class_</span></span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>        n_class <span class="op">=</span> <span class="bu">len</span>(class_idxs) </span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># check if there are enough samples in class_ for k-fold cross validation</span></span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n_class <span class="op">&lt;</span> k:</span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"k-fold cross validation requires at least k samples in each class"</span>)</span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>        n_train <span class="op">=</span> <span class="bu">int</span>(n_class<span class="op">*</span>(k<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>k) <span class="co"># number of from class_ in train set</span></span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add train indices to train_idxs</span></span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a>        train_idxs <span class="op">=</span> np.append(train_idxs, np.random.choice(class_idxs, n_train, replace<span class="op">=</span><span class="va">False</span>)) </span>
<span id="cb72-28"><a href="#cb72-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-29"><a href="#cb72-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># indices not selected for the test group are assigned to the train group</span></span>
<span id="cb72-30"><a href="#cb72-30" aria-hidden="true" tabindex="-1"></a>    test_idxs <span class="op">=</span> np.setdiff1d(np.arange(<span class="dv">0</span>, <span class="bu">len</span>(class_act)), train_idxs) </span>
<span id="cb72-31"><a href="#cb72-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-32"><a href="#cb72-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_idxs.astype(<span class="bu">int</span>), train_idxs.astype(<span class="bu">int</span>) <span class="co"># return indices as integers</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The stratified train/test splitting function assigns a random subset of samples from each class into a train group. The size of the subset is dictated by the <code>k</code> and number of samples of that class, ensuring that the proportion of samples used in the train and test set is the same for each class. This should result in the same proportions every time we call this splitting function.</p>
<div id="86b6ac0b" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the distribution of Cue trials across multiple runs of the stratified train-test split</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>num_runs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>prop_cue_test <span class="op">=</span> np.zeros(num_runs)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>prop_cue_train <span class="op">=</span> np.zeros(num_runs)</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    test_idxs, train_idxs <span class="op">=</span> train_test_kstrat(class_act)</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    prop_cue_test[i] <span class="op">=</span> class_act[test_idxs].mean()</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>    prop_cue_train[i] <span class="op">=</span> class_act[train_idxs].mean()</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">1</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(prop_cue_test, bins<span class="op">=</span>np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">20</span>), alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(prop_cue_train, bins<span class="op">=</span>np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">20</span>), alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Test set, mean proportion = </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(prop_cue_test.mean()))</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Train set, mean proportion = </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(prop_cue_train.mean()))</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'Count'</span>)</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>fig.supxlabel(<span class="st">'Proportion of cues'</span>)</span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a><span class="co"># proportion of Cue trials across all trials</span></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Proportion of Cue trials across all trials: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(class_act.mean()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-48-output-1.png" width="663" height="480" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Proportion of Cue trials across all trials: 0.46</code></pre>
</div>
</div>
<p>Statifying the assignment of samples based on their class has eliminated variability in their proportions. One drawback, though, is that the mean proportion is subtly different between the train and test sets, with 0.45 and 0.47 respectively. This bias happens because we cannot evenly split the samples from the Cue class into train and test sets, and since the same number of Cue samples are drawn each time, this difference persists across runs. By minimizing the variability in the proportions, we have introduced a bias in the proportions. There is a well-known bias-variance trade-off in statistics that speaks to this.</p>
<p>Returning to the prediction question, we want to determine how our logistic regression model performs on data it was not trained on. Since the selection of trials in the test and train sets is random, a single estimate of performance would not tell us how the decoder performs in general. To estimate the performance, we will run it a thousand times. In addition, we will evaluate the performance on both the test and train data sets.</p>
<div id="e05c5076" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit and evalulate logistic regression model to ERP data using cross-validation</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logreg_traintest(epochs, class_act, k):</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    test_idxs, train_idxs <span class="op">=</span> train_test_kstrat(class_act, k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set union of class_act and train_idxs</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>    train_cue_idxs <span class="op">=</span> np.intersect1d(np.where(class_act)[<span class="dv">0</span>], train_idxs)</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate ERP for training set</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>    cue_erp <span class="op">=</span> np.mean(epochs[:, train_cue_idxs], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate ERP alignment for each trial</span></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>    val_all <span class="op">=</span> erp_align(epochs, cue_erp)</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create train and test sets</span></span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>    val_test <span class="op">=</span> val_all[test_idxs, np.newaxis]</span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>    class_act_test <span class="op">=</span> class_act[test_idxs]</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>    val_train <span class="op">=</span> val_all[train_idxs, np.newaxis]</span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a>    class_act_train <span class="op">=</span> class_act[train_idxs]</span>
<span id="cb75-19"><a href="#cb75-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-20"><a href="#cb75-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit logistic regression model</span></span>
<span id="cb75-21"><a href="#cb75-21" aria-hidden="true" tabindex="-1"></a>    clf_sub <span class="op">=</span> LogisticRegression()</span>
<span id="cb75-22"><a href="#cb75-22" aria-hidden="true" tabindex="-1"></a>    clf_sub.fit(val_train, class_act_train)</span>
<span id="cb75-23"><a href="#cb75-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-24"><a href="#cb75-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get performance for train and test sets</span></span>
<span id="cb75-25"><a href="#cb75-25" aria-hidden="true" tabindex="-1"></a>    score_test <span class="op">=</span> clf_sub.score(val_test, class_act_test)</span>
<span id="cb75-26"><a href="#cb75-26" aria-hidden="true" tabindex="-1"></a>    score_train <span class="op">=</span> clf_sub.score(val_train, class_act_train)</span>
<span id="cb75-27"><a href="#cb75-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> score_train<span class="op">*</span><span class="dv">100</span>, score_test<span class="op">*</span><span class="dv">100</span></span>
<span id="cb75-28"><a href="#cb75-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-29"><a href="#cb75-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Run logistic regression model on ERP data</span></span>
<span id="cb75-30"><a href="#cb75-30" aria-hidden="true" tabindex="-1"></a>num_runs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb75-31"><a href="#cb75-31" aria-hidden="true" tabindex="-1"></a>all_epochs <span class="op">=</span> np.concatenate((cue_epochs, nocue_epochs), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb75-32"><a href="#cb75-32" aria-hidden="true" tabindex="-1"></a>scores_train <span class="op">=</span> np.zeros(num_runs)</span>
<span id="cb75-33"><a href="#cb75-33" aria-hidden="true" tabindex="-1"></a>scores_test <span class="op">=</span> np.zeros(num_runs)</span>
<span id="cb75-34"><a href="#cb75-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb75-35"><a href="#cb75-35" aria-hidden="true" tabindex="-1"></a>    scores_train[i], scores_test[i] <span class="op">=</span> logreg_traintest(all_epochs, class_act, <span class="dv">5</span>)</span>
<span id="cb75-36"><a href="#cb75-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-37"><a href="#cb75-37" aria-hidden="true" tabindex="-1"></a><span class="co"># plot violin plot of scores compared across train and test sets</span></span>
<span id="cb75-38"><a href="#cb75-38" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">5</span>))</span>
<span id="cb75-39"><a href="#cb75-39" aria-hidden="true" tabindex="-1"></a>ax.violinplot([scores_train, scores_test], showmedians<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb75-40"><a href="#cb75-40" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb75-41"><a href="#cb75-41" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="st">'Train'</span>, <span class="st">'Test'</span>])</span>
<span id="cb75-42"><a href="#cb75-42" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Accuracy (%)'</span>)</span>
<span id="cb75-43"><a href="#cb75-43" aria-hidden="true" tabindex="-1"></a>ax.grid(axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb75-44"><a href="#cb75-44" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Cross-validated logistic regression performance'</span>)</span>
<span id="cb75-45"><a href="#cb75-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-46"><a href="#cb75-46" aria-hidden="true" tabindex="-1"></a><span class="co"># print median performance for train and test sets</span></span>
<span id="cb75-47"><a href="#cb75-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Median train accuracy: </span><span class="sc">{:.2f}</span><span class="st"> %'</span>.<span class="bu">format</span>(np.median(scores_train)))</span>
<span id="cb75-48"><a href="#cb75-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Median test accuracy: </span><span class="sc">{:.2f}</span><span class="st"> %'</span>.<span class="bu">format</span>(np.median(scores_test)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Median train accuracy: 86.05 %
Median test accuracy: 81.82 %</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-49-output-2.png" width="400" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Performance on the train set is close to the performance achieved when we trained on the model on the entire data set. When the model was tested on the test set, performance was more variable. It is skewed to lower performance, sometimes much worse than the train set. We can compare the performance for each pair of train and test sets to determine if there was a systematic difference in performance.</p>
<div id="86188f9c" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># difference in scores by train and test sets</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>score_diff <span class="op">=</span> scores_test <span class="op">-</span> scores_train</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate median difference</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>median_diff <span class="op">=</span> np.median(scores_test <span class="op">-</span> scores_train)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Median difference: </span><span class="sc">{:0.2f}</span><span class="st"> %"</span>.<span class="bu">format</span>(median_diff))</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot distribution of score differences</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>plt.hist(score_diff,<span class="dv">20</span>, label<span class="op">=</span><span class="st">"Distribution"</span>)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Test score - train score"</span>)</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Count"</span>)</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Histogram of score differences"</span>)</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>plt.axvline(median_diff, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Median"</span>)</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Median difference: -6.45 %</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-50-output-2.png" width="593" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The median performance dropped ~7% on the test set. However, the extremes of the differences are wide, sometimes performing worse by 50% or better by 20%. It is common for performance to decrease on the test set, and the small drop we see here on average is not bad.</p>
<p>Before ending this section, we will go over how scikit-learn implements data splitting. It features a variety of splitting schemes, whose documentation can be found <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection">here</a>.</p>
<p>To segregate data into train and test sets, you use a splitter object. Splitter objects are implemented similar to estimator objects. You first initialize them with parameters that are agnostic to the dataset, and then pass your data to the object’s <code>split</code> method, which returns indices for the train and test sets. Each time you call <code>split</code>, a new subset of indices is generated. To do this, it is implemented as type of iterator object (e.g.&nbsp;python’s <code>range</code> function) known as a <em>generator</em> object. Usually iterators are python objects that have an <code>__iter__</code> method, but in the case of the splitter objects they use <code>yield</code> in their <code>split</code> method to return a new set of indices each time they are called.</p>
<div id="6345e8e7" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the KFold object</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>ksplit <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">47</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first parameter for <code>KFold</code> is <code>n_splits</code>, which specifies how many folds, k, to use. <code>shuffle</code> sets whether the indices selected for the sets should be randomly drawn from across the data set. By default it is set to False, which will select indices in the order they occur in the dataset. Lastly, when randomly shuffling the indices we can specify the random seed to use using the <code>random_state</code> parameter so that the same random subset can be selected each time the function is called.</p>
<p>Now you might be tempted to get the indices for the train and test sets by directly calling the <code>split</code> method, such as:</p>
<div id="7557bc01" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>ksplit.split(vals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>&lt;generator object _BaseKFold.split at 0x15fa68d60&gt;</code></pre>
</div>
</div>
<p>But, this does not return what we want. Instead, it delivers a <em>generator object</em> that is not the indices we want. To get the indices, we have to call the <code>split</code> method as part of a loop, which engages its behavior as an iterator.</p>
<div id="72063a9f" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print the indices for each test set</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> run_num, (train_idxs, test_idxs) <span class="kw">in</span> <span class="bu">enumerate</span>(ksplit.split(vals)):</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Test indices on run </span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(run_num<span class="op">+</span><span class="dv">1</span>, test_idxs))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test indices on run 1: [12 14 24 32 35 36 40 41 43 44 46]
Test indices on run 2: [10 11 13 18 19 21 27 29 30 33 37]
Test indices on run 3: [ 3  4  5 15 17 20 22 25 38 39 52]
Test indices on run 4: [ 0  1  2  9 26 28 31 34 42 47 49]
Test indices on run 5: [ 6  7  8 16 23 45 48 50 51 53]</code></pre>
</div>
</div>
<p>Repeated calls to the <code>split</code> method in the for loop returns a new set of indices each time. Notice that each set is distinct, no overlap between sets. This is because one often runs cross-validation multiple times and to ensure that independent data sets are used when measuring performance. This restriction means that the total number of runs for generating a cross-validation set is the number of splits, <code>k</code>. If you want just a single run to be returned, you can use the python function <code>next</code>.</p>
<div id="339b7772" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>train_idxs, test_idxs <span class="op">=</span> <span class="bu">next</span>(ksplit.split(vals))</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train indices: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(train_idxs))</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test indices: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(test_idxs))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 15 16 17 18 19 20 21 22 23 25 26
 27 28 29 30 31 33 34 37 38 39 42 45 47 48 49 50 51 52 53]
Test indices: [12 14 24 32 35 36 40 41 43 44 46]</code></pre>
</div>
</div>
<p>Normally when you call next with an iterator it will return the a different output each time it is called. This is not the case for the splitter objects, so if you want to get a new set of indices (and do not mind them overlapping with the previous set), then you have to reinitialize the splitter object each time.</p>
<p>If you want the splits to be balanced in their proportion of trial types, you can use a stratified approach to generate the train and test sets. This is done with the <code>StratifiedKFold</code> object. It is used similarly to <code>KFold</code>, except that when calling <code>split</code> you pass both the <code>X</code> and <code>y</code> parameters. <code>y</code> is used for stratifying the selection of indices by the classes. Here is how to use it:</p>
<div id="c02b8833" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the KFold object</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>kstratsplit <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">47</span>)</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print the indices for each test set</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> run_num, (train_idxs, test_idxs) <span class="kw">in</span> <span class="bu">enumerate</span>(kstratsplit.split(vals, class_act)):</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Test indices on run </span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(run_num<span class="op">+</span><span class="dv">1</span>, test_idxs))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test indices on run 1: [ 4  5  6 11 13 42 43 44 48 49 53]
Test indices on run 2: [ 0  2 21 23 24 28 30 33 36 39 50]
Test indices on run 3: [ 7  8  9 10 14 27 31 32 45 46 51]
Test indices on run 4: [12 15 17 18 19 25 26 29 34 35 52]
Test indices on run 5: [ 1  3 16 20 22 37 38 40 41 47]</code></pre>
</div>
</div>
</section>
</section>
<section id="putting-it-all-together" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together">Putting it all together</h2>
<p>Now that we have gone over how to detect an ERP, we should pull all our code together into a single class that implements each stage of the fitting pipeline. We will use the builtin scikit-learn functions to do this.</p>
<div id="e55ccd36" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ERP_Decode():</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, k<span class="op">=</span><span class="dv">5</span>, rand_seed<span class="op">=</span><span class="dv">47</span>, <span class="op">**</span>kwargs):</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._k <span class="op">=</span> k <span class="co"># number of folds for cross-validation</span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._rand_seed <span class="op">=</span> rand_seed <span class="co"># random seed for reproducibility</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._test_acc <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._train_acc <span class="op">=</span> <span class="va">None</span></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.erp <span class="op">=</span> <span class="va">None</span> <span class="co"># ERP is the average of the ERP labeled epochs</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._logreg <span class="op">=</span> LogisticRegression(<span class="op">**</span>kwargs) <span class="co"># **kwargs allows us to pass in arguments to the LogisticRegression class</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._stratkfold <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="va">self</span>._k, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="va">self</span>._rand_seed)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_rand_seed(<span class="va">self</span>, rand_seed):</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># set random seed to generate new random folds</span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._rand_seed <span class="op">=</span> rand_seed</span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._stratkfold <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="va">self</span>._k, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="va">self</span>._rand_seed)</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _erp_calc(<span class="va">self</span>, epochs):</span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate the ERP</span></span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(epochs, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _erp_align(<span class="va">self</span>, epochs, erp):</span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># align the ERP to the origin</span></span>
<span id="cb88-23"><a href="#cb88-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.dot(epochs, erp.T<span class="op">/</span>np.linalg.norm(erp))</span>
<span id="cb88-24"><a href="#cb88-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-25"><a href="#cb88-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, epochs, labels):</span>
<span id="cb88-26"><a href="#cb88-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fit the model</span></span>
<span id="cb88-27"><a href="#cb88-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-28"><a href="#cb88-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get the training and testing indices for first fold</span></span>
<span id="cb88-29"><a href="#cb88-29" aria-hidden="true" tabindex="-1"></a>        train_idxs, test_idxs <span class="op">=</span> <span class="bu">next</span>(<span class="va">self</span>._stratkfold.split(epochs, labels))</span>
<span id="cb88-30"><a href="#cb88-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-31"><a href="#cb88-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get the training and testing data for first fold</span></span>
<span id="cb88-32"><a href="#cb88-32" aria-hidden="true" tabindex="-1"></a>        y_train <span class="op">=</span> labels[train_idxs]</span>
<span id="cb88-33"><a href="#cb88-33" aria-hidden="true" tabindex="-1"></a>        X_train <span class="op">=</span> epochs[train_idxs]</span>
<span id="cb88-34"><a href="#cb88-34" aria-hidden="true" tabindex="-1"></a>        train_cue_idxs <span class="op">=</span> np.intersect1d(np.where(y_train)[<span class="dv">0</span>], train_idxs)</span>
<span id="cb88-35"><a href="#cb88-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.erp <span class="op">=</span> <span class="va">self</span>._erp_calc(X_train[train_cue_idxs])</span>
<span id="cb88-36"><a href="#cb88-36" aria-hidden="true" tabindex="-1"></a>        X_train <span class="op">=</span> <span class="va">self</span>._erp_align(X_train, <span class="va">self</span>.erp)[:,np.newaxis]</span>
<span id="cb88-37"><a href="#cb88-37" aria-hidden="true" tabindex="-1"></a>        X_test <span class="op">=</span> <span class="va">self</span>._erp_align(epochs[test_idxs], <span class="va">self</span>.erp)[:,np.newaxis]</span>
<span id="cb88-38"><a href="#cb88-38" aria-hidden="true" tabindex="-1"></a>        y_test <span class="op">=</span> labels[test_idxs]</span>
<span id="cb88-39"><a href="#cb88-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-40"><a href="#cb88-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fit the model</span></span>
<span id="cb88-41"><a href="#cb88-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._logreg.fit(X_train, y_train)</span>
<span id="cb88-42"><a href="#cb88-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._train_acc <span class="op">=</span> <span class="va">self</span>._logreg.score(X_train, y_train)<span class="op">*</span><span class="dv">100</span></span>
<span id="cb88-43"><a href="#cb88-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._test_acc <span class="op">=</span> <span class="va">self</span>._logreg.score(X_test, y_test)<span class="op">*</span><span class="dv">100</span></span>
<span id="cb88-44"><a href="#cb88-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-45"><a href="#cb88-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return the training and testing accuracies</span></span>
<span id="cb88-46"><a href="#cb88-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._train_acc, <span class="va">self</span>._test_acc</span>
<span id="cb88-47"><a href="#cb88-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-48"><a href="#cb88-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decision_boundary(<span class="va">self</span>):</span>
<span id="cb88-49"><a href="#cb88-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get the decision boundary</span></span>
<span id="cb88-50"><a href="#cb88-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span><span class="va">self</span>._logreg.intercept_[<span class="dv">0</span>]<span class="op">/</span><span class="va">self</span>._logreg.coef_[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb88-51"><a href="#cb88-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-52"><a href="#cb88-52" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> model_coef(<span class="va">self</span>):</span>
<span id="cb88-53"><a href="#cb88-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get the model coefficients</span></span>
<span id="cb88-54"><a href="#cb88-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._logreg.coef_[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb88-55"><a href="#cb88-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-56"><a href="#cb88-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> model_intercept(<span class="va">self</span>):</span>
<span id="cb88-57"><a href="#cb88-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get the model intercept</span></span>
<span id="cb88-58"><a href="#cb88-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._logreg.intercept_[<span class="dv">0</span>]</span>
<span id="cb88-59"><a href="#cb88-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-60"><a href="#cb88-60" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, epochs):</span>
<span id="cb88-61"><a href="#cb88-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predict the labels of new data using the trained model</span></span>
<span id="cb88-62"><a href="#cb88-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._logreg.predict(<span class="va">self</span>.erp_align_(epochs, <span class="va">self</span>.erp))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ec96bdce" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>train_ac <span class="op">=</span> np.zeros(<span class="dv">1000</span>)</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>test_ac <span class="op">=</span> np.zeros(<span class="dv">1000</span>)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rep <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    erp_dec <span class="op">=</span> ERP_Decode(rand_seed<span class="op">=</span>rep)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>    train_ac[rep], test_ac[rep] <span class="op">=</span> erp_dec.fit(all_epochs.T, class_act)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(erp_dec.decision_boundary())</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">5</span>))</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>ax.violinplot([train_ac, test_ac])</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="st">'Train'</span>, <span class="st">'Test'</span>])</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="dv">20</span>, <span class="dv">105</span>])</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>ax.grid(axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'ERP Decoding'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>35.90827949750561</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>Text(0.5, 1.0, 'ERP Decoding')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Week3_files/figure-html/cell-57-output-3.png" width="221" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This replicates the results we got earlier with the splitting functions we wrote ourselves. Performance on the training data typically exceeds that on the test data. Let’s save the parameters from the last model fitting. We will use them next week when we evaluate our hand-coded functions for fitting logistic regression models.</p>
<div id="9288d2d2" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> erp_dec.model_coef()</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> erp_dec.model_intercept()</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>erp <span class="op">=</span> erp_dec._erp_calc(all_epochs[:,class_act].T)</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> erp_dec._erp_align(all_epochs.T, erp)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> class_act</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="co"># had to convert X and y formats to save in JSON file</span></span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>logreg_data <span class="op">=</span> {<span class="st">'w'</span>: w, <span class="st">'b'</span>: b, <span class="st">'X'</span>: <span class="bu">list</span>(X), <span class="st">'y'</span>: <span class="bu">list</span>(y.astype(<span class="bu">float</span>))}</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(os.path.join(<span class="st">'data'</span>, <span class="st">'logregdata.json'</span>), <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>    json.dump(logreg_data, f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>References</p>
<p>G. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics, Chapter 4, https://doi.org/10.1007/978-3-031-38747-0_4</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>