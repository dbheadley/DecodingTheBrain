[
  {
    "objectID": "Week1.html",
    "href": "Week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Welcome to the course Decoding the Brain. We will cover how to process electrical signals generated by neural activity, use machine learning approaches to infer the presentation of stimuli or an intended motor action from that activity, and evaluate the performance of these inferences.\nThis course is designed so that students gain a practical understanding of how to design the algorithms used for brain-computer interfaces (BCI). By the end of the course, you should understand enough to program your own BCI algorithms for non-invasive (scalp) and invasive (electrodes implanted in the brain) measures of brain activity. These activities can be weak electrical activity recorded from the scalp (EEG), high-frequency oscillations recorded on the brain surface (ECoG), and the firing of individual neurons (units). Decoding each of these types of signals requires several steps we will cover. First, you will be shown how to process and clean them for subsequent decoding. Then, we will cover the theory behind how the decoding algorithms work and code basic versions of them from scratch in using the Python packages Numpy or PyTorch. Next, we will expand their capabilities to handle more complex patterns of brain activity or decode multiple events. Finally, we will evaluate the decoder’s performance.\nExcept for cursory discussion, we will not cover the hardware, surgical, and biocompatibility issues of recording brain activity. Those topics depend upon fundamentally different skills from the design of BCI algorithms.\nThe two principal algorithms covered in this course, logistic regression and naive Bayes, are useful outside of BCIs. Thus, the understanding you get of them here will apply to many other domains."
  },
  {
    "objectID": "Week1.html#syllabus-and-class-structure",
    "href": "Week1.html#syllabus-and-class-structure",
    "title": "Week 1",
    "section": "",
    "text": "Welcome to the course Decoding the Brain. We will cover how to process electrical signals generated by neural activity, use machine learning approaches to infer the presentation of stimuli or an intended motor action from that activity, and evaluate the performance of these inferences.\nThis course is designed so that students gain a practical understanding of how to design the algorithms used for brain-computer interfaces (BCI). By the end of the course, you should understand enough to program your own BCI algorithms for non-invasive (scalp) and invasive (electrodes implanted in the brain) measures of brain activity. These activities can be weak electrical activity recorded from the scalp (EEG), high-frequency oscillations recorded on the brain surface (ECoG), and the firing of individual neurons (units). Decoding each of these types of signals requires several steps we will cover. First, you will be shown how to process and clean them for subsequent decoding. Then, we will cover the theory behind how the decoding algorithms work and code basic versions of them from scratch in using the Python packages Numpy or PyTorch. Next, we will expand their capabilities to handle more complex patterns of brain activity or decode multiple events. Finally, we will evaluate the decoder’s performance.\nExcept for cursory discussion, we will not cover the hardware, surgical, and biocompatibility issues of recording brain activity. Those topics depend upon fundamentally different skills from the design of BCI algorithms.\nThe two principal algorithms covered in this course, logistic regression and naive Bayes, are useful outside of BCIs. Thus, the understanding you get of them here will apply to many other domains."
  },
  {
    "objectID": "Week1.html#mixing-code-theory-and-practice",
    "href": "Week1.html#mixing-code-theory-and-practice",
    "title": "Week 1",
    "section": "Mixing code, theory, and practice",
    "text": "Mixing code, theory, and practice\nLectures will introduce the theory behind what we do, practical details of how to go about doing it, and code that does the doing. The astounding thing about BCI design is how accessible the algorithms and plentiful the data sources are. Of course, a real BCI system uses specialized hardware implanted in or on a person and is run in near real-time, which is not feasible in a classroom setting.\nWe will use Python throughout this course, and publicly available data sets.\n\n# This will be the standard set of packages we import for most lectures. \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.signal as sig\nfrom IPython.display import display, Math, Latex"
  },
  {
    "objectID": "Week1.html#general-principles-of-bci-design",
    "href": "Week1.html#general-principles-of-bci-design",
    "title": "Week 1",
    "section": "General principles of BCI design",
    "text": "General principles of BCI design\nThe goal of BCI is to attribute electrical activity in a subject’s brain to stimuli they received or actions they intended to produce. Grounding this is the fundamental assumption in neuroscience that all we experience and do arises from patterns of activity in the brain. These patterns are manifested at multiple levels, from single neurons to large portions of the cortical surface. Every stimulus produces a chain of activities starting at peripheral receptors, ascending through chains of neurons that eventually reach the neocortex, where they are processed and elaborated, linked with your expectations, memories, and goals. At every level specific groups of neurons are activated reflecting the properties of the stimuli or your cognitive state. Sometimes these trigger actions - e.g. moving your arm - that depends on a chain of activities stretching back from the muscles to your spinal cord, then brain stem, then cortex. Remarkably, in a mouse it only takes the activation of just over a dozen neurons to influence perception.\nGiven that there is a physical correlate in the brain of what we perceive and do, then it is possible to ‘tap’ them. The limitations are technical such as how well can be extract neural activities and our computational resources for attributing them to the external world. Much work has been spent on these problems. This is often conceptualized in two different ways.\n.\nOne is treat it as an encoding problem, where we try to understand how events in the environment are reflected in neural activities. The other is the decoding approach, where we use neural activities to infer what is happening in the environment. BCIs make extensive use of decoding analyses.\n\n\n\nBCI schema\n\n\nA BCI involves measuring some activity in the brain and then transforming it so that it can be used to infer whether a certain action was intended or stimulus occurred. The transformation, referred to as the decoding step in the above figure, involves three steps. 1. Signal preprocesing: The raw neural activity measurements are cleaned and filtered to remove non-neural artifacts and highlight those aspects of the signal that reflect the processing we care about. 2. Feature extraction: Patterns are identified in the activities that are related to the events we wish to decode. 3. Classification: Based on the detected patterns, a decision is made about whether, or what kind, of event occurred.\nFor each type of brain signal involves different preprocessing and feature extraction steps. Generally the classification step can be used interchangeably between signal types, but we will explore two fundamentally different approaches."
  },
  {
    "objectID": "Week1.html#neuroscience-basics",
    "href": "Week1.html#neuroscience-basics",
    "title": "Week 1",
    "section": "Neuroscience basics",
    "text": "Neuroscience basics\nThe brain is an electrochemical machine that allows vertebrates to interact with a dynamic complex environment. It does this using specialized cells called neurons. They have the ability to receive chemical signals, neurotransmitters, that affect their electrical activity. This activity is transmitted to other neurons via an axon. When multiple neurons are connected together they form a network that supports the transformation of incoming activity and spontaneous generation of new activities.\n\nBrain anatomy\nThe brain is composed of numerous regions, but its cortex is generally divided into four principal areas: \n\nFrontal lobe: Reasoning and motor control\nParietal lobe: Touch and visual spatial processing\nOccipital lobe: Low level visual processing\nTemporal lobe: Audition, high level visual processing, memory\n\nKeep in mind these are gross generalizations regarding the function of these areas.\n\n\n\nBrain cross section\n\n\nThe brain is situated in the skull and is separated by a couple centimeters from the surface of the scalp. Above it lies several layers of connective tissue (pia, dura, arachnoid) and cerebrospinal fluid that act as a protective cushion. The skull itself has multiple layers, followed by fatty tissue and the skin.\n\n\nNeurons\n\n\n\nNeuron schematic\n\n\nNeurons are the principal cell type in the brain for processing information. Neurons are tiny, ~10 microns, which is 5-10 times smaller than the diameter of a human hair. They are comprised of a soma (cell body), dendrites, and an axon. The dendrites take in signals from other neurons, integrate them, and convey them to the soma. When these signals exceed a threshold, the soma can generate an action potential that is conveyed down the axon towards other neurons.\n\n\nMembranes and ions\n Like all eukaryotic cells, neurons have a lipid membrane that seals in their internal, or intracellular, fluid (also known as cytoplasm) and organelles from the extracellular environment. This membrane also acts as an electrical insulator, blocking the flow of electrical charges across it. Electrical charge in the brain is carried by ions in the intracellular and extracellular fluids (cerebrospinal fluid). Ions are atoms or molecules that are either positively or negatively charged depending on whether they have a fewer or greater number of electrons than protons. The ones that are most determinative of neural activity are Na+, K+, Cl-, and Ca2+. Between the intracellular and extracellular fluids, the concentrations of these ions are different. Na+,Cl-, and Ca2+ have higher concentrations in the extracellular fluid, while K+ ions have a higher concentration in the intracellular fluid.\n\n\n\nMembrane and ions\n\n\n\nion_props = pd.DataFrame({'Ion': ['Na', 'K', 'Cl', 'Ca'],\n                          'Out_mM': [140, 3, 140, 1.5],\n                          'In_mM': [7, 140, 7, 0.0001],\n                          'Charge': [1, 1, -1, 2]})\nion_props.set_index('Ion', inplace=True)\n\nprint(ion_props)\n\n     Out_mM     In_mM  Charge\nIon                          \nNa    140.0    7.0000       1\nK       3.0  140.0000       1\nCl    140.0    7.0000      -1\nCa      1.5    0.0001       2\n\n\nThis unequal distribution of ions across the membrane is unstable because ions tend to flow from where they are highly concentrated to where they are less concentrated. To cross the membrane, ions flow through channels, unsurprisingly called ion channels. These are proteins that span the inside and outside of the neuron with a central hole, known as a pore, through which the ions flow. The pore’s shape and charge determine its selectivity for specific ionic species. Additional factors can control the opening of the pore. Some ion channel pores are controlled by the voltage across the membrane, others by substances such as neurotransmitters or hormones that bind to them, or even other ions like Mg2+.\n\n\n\nNa channel\n\n\nSince ions move from high to low concentrations, Na+, Cl-, and Ca2+ are inclined to flow into neurons. K+, on the other hand, wants to flow out of the neuron. As these ions flow through the membrane they build up on its surface. This create a force that opposes further flow from diffusion, because ions with the same sign to their charge tend to push away from each other. The work needed to move a charge from one place to another is measured as a voltage or potential. The potential at which an ion’s electrical forces from the buildup of ions balance out the diffusion force is referred to as the reversal or equilibrium potential, E. In the case here, we are measuring potential between the inside and outside of the neuron. We can calculate the strength of the potentials that reflect these drives using the Nernst equation.\nE = \\frac{RT}{zF}ln\\frac{[Out]}{[In]}\nR is the gas constant and F is Faraday’s constant, while T is temperature in Kelvin and z is the charge of the ion. [Out] and [In] are the extracellular and intracellular concentrations of that ion, respectively.\nMost of the time when an equation is introduced, we will try to translate it into Python. A method that calculates the equilibrium potential of a given ion can be given as:\n\n# calculate Nernst equilibrium potential for an ion\ndef nernst(conco=3, conci=140, z=1, t=310):\n    \"\"\"Calculate Nernst potential for an ion.\n    \n    Parameters\n    ----------\n    conco : float\n        Concentration of ion outside cell (mM). Default is for K+.\n    conci : float\n        Concentration of ion inside cell (mM). Default is for K+.\n    z : int\n        Charge of ion.  Default is for K+.\n    t : float\n        Temperature (Kelvin). Default is body temperature.\n    \n    Returns\n    -------\n    float\n        Nernst potential for ion (mV).\n    \"\"\"\n    r = 8.314 # J/mol/K, gas constant\n    f = 96485 # C/mol, Faraday's constant\n    return (r*t)/(z*f) * np.log(conco/conci) * 1000\n\nUsing this function and the dataframe of ionic concentrations above, we can calculate the equilibrium potentials for each of the major ionic species found in the nervous system.\n\nion_props['E'] = ion_props.apply(lambda ion: nernst(ion['Out_mM'], ion['In_mM'], ion['Charge']), axis=1)\nprint(ion_props)\n\n     Out_mM     In_mM  Charge           E\nIon                                      \nNa    140.0    7.0000       1   80.023015\nK       3.0  140.0000       1 -102.656323\nCl    140.0    7.0000      -1  -80.023015\nCa      1.5    0.0001       2  128.430326\n\n\nSeveral insights are readily apparent from cursory inspection of the calculated equilibrium potentials and consideration of the Nernst equation. First, and most importantly, positive ions that have a higher concentration outside the neuron will have positive potentials, while those more concentrated inside the neuron have negative potentials. This is stands out when comparing the potential of Na+, whose potential is 80 mV and K+, whose potential is -80 mV. Second, since we are taking the ratio between [Out] and [In], the overall amount of ions in our system does not affect the potential, only their relative proportion. Put another way, you could decrease the concentration of Na+ ions inside and outside the neuron by ten times and as long as they have the same proportion, the potential will be unchanged. Indeed, Ca2+ has a much lower overall concentration inside and outside the membrane, but its reversal potential is higher than that for Na+. Third, if an ion has a negative charge, z, it flips the sign of the potential. For instance, Cl- ions have a higher concentration outside the neuron, just like Na+, but their potential is negative.\nThe potential across a neuron’s membrane is the mean of all these potentials, each weighted by how easy it is for the ion to cross the membrane, which is related to the number of open ion channels allowing that ion to flow. In a neuron at rest, mostly K+ ion channels are open, so its reversal potential dominates. Neurons in the cerebral cortex usually have a resting potential around -65 mV."
  },
  {
    "objectID": "Week1.html#electrical-model-of-the-neuronal-membrane",
    "href": "Week1.html#electrical-model-of-the-neuronal-membrane",
    "title": "Week 1",
    "section": "Electrical model of the neuronal membrane",
    "text": "Electrical model of the neuronal membrane\nThe electrical behavior of the membrane can be approximated by a relatively simple circuit. While this is not an electronics course, I will briefly describe this model and use it to construct a simulated neuron. This should allow us to explore some of the response properties of neurons and how they are reflected in the electrical activities detected with electrodes.\n\nEquilibrium potential as a battery and conductor\nThe equilibrium potential, E_{r}, behaves like a battery. The ionic current it produces flows through open ion channels, which we will refer to as conductors. As we discussed above, when a neuron is at rest it primarily allows K+ ions to flow. This passive leakage of ions across the membrane has given the conductance the name leak conductance. This can be schematized below as a battery and conductor in series that bridge the intracellular and extracellular spaces.\n\n\n\nMembrane with battery\n\n\nBy itself, this circuit will fix the voltage across the neuronal membrane (measured between the inside and outside) at the resting potential. Normally neurons do not remain at rest, but have a ongoing fluctuations in their membrane voltage due to the thousands of synaptic inputs they recieve. This means that the membrane potential, V_{m}, can be different from E_{r}. When V_{m} is different from E_{r}, there is a net flow of current across the membrane, since the electrical and diffusion forces are no longer balanced (think back to the Nernst equation). The ionic current can be described using Ohm’s law:\n V = IR \nwhere V is voltage (volts, V), I is current (amperes, A), and R is resistance (ohms, \\Omega). Rewriting the equation to solve for I gives us:  I = \\frac{V}{R}  The inverse of resistance is conductance (siemens, S), so this equation can be rewritten as:  I = gV  Here g is the ease with which our conductor allows current to flow. Now, if we include the effect of the reversal potential, E_{r} acting as a battery, then the equation can be written as:  I_{leak} = g_{leak}(V_{m}-E_{r}) \\tag{1}  Let’s consider the behavior of this equation. It shows that as we increase the ion channel’s conductance, g_{leak}, the current will increase as well. It also shows that the magnitude and direction of the current will depend on the membrane voltage, V_{m}. If V_{m} is below E_{r}, current will be negative. If it is above, then it will be positive. As V_{m} approaches E_{r}, the current gets smaller. When they are equal, no current flows because the electrical forces perfectly balance the concentration gradient.\nLet’s graph these relationships between V_{m} and I_{leak} by systematically varying E_{r} and g_{leak}.\n\n# Here are somme realistic parameters for a neuron\nmem_r = 25e3 # ohm*cm^2, taken from Egger et al 2020\ncell_radius = 30e-4 # cm, 30 microns, this is larger than the cell body itself to account for membrane from dendrites\ncell_area = 4*np.pi*cell_radius**2 # cm^2\ng_leak = (1/mem_r) * cell_area # S/cm^2\ne_rest = -0.065 # V, 65 mV\n\n# code for solving for leak current\ndef ionic_current(v=0, g=g_leak, e=e_rest):\n    \"\"\"Calculate ionic current.\n    \n    Parameters\n    ----------\n    v : float\n        Membrane potential (V). Default is 0.\n    g : float\n        Conductance (S). Default is leak conductance.\n    e : float\n        Equilibrium potential (V). Default is resting potential.\n    \n    Returns\n    -------\n    float\n        Ionic current (A).\n    \"\"\"\n    return g * (v - e)\n\n\n# Examine how varying the conductance affects the I-V curve\nv = np.linspace(-0.1, 0.1, 100)\ng_factors = [0.1, 0.5, 1, 2, 5]\nfor ind, curr_factor in enumerate(g_factors):\n    curr_g = g_leak*curr_factor\n    plt.plot(v*1e3, ionic_current(v, g=curr_g)*1e12, \n             label=round(curr_g*1e12), color=[ind/len(g_factors), 0,0])\n    plt.text(v[-1]*1e3+2, ionic_current(v[-1], g=curr_g)*1e12,\n                str(round(curr_g*1e12))+' pS', color=[ind/len(g_factors), 0,0])\nplt.xlim(-100, 135)\nplt.xlabel('Membrane potential (mV)')\nplt.ylabel('Ionic current (pA)')\nplt.grid()\nplt.title('Effect of conductance on ionic current')\n\nText(0.5, 1.0, 'Effect of conductance on ionic current')\n\n\n\n\n\n\n\n\n\nIt is evident that increasing the conductance dramatically increases the ionic current. Now let’s examine the effect of equilibrium potential.\n\n# Examine how varying the equilibrium potential affects the I-V curve\nv = np.linspace(-0.1, 0.1, 100)\ne_values = [-0.070, -0.035, 0, 0.035, 0.070]\nfor ind, curr_e in enumerate(e_values):\n    plt.plot(v*1e3, ionic_current(v, e=curr_e)*1e12, \n             label=curr_e*1e3, color=[0,0,ind/len(e_values)])\n    plt.text(v[-1]*1e3+2, ionic_current(v[-1], e=curr_e)*1e12,\n                str(round(curr_e*1e3))+' mV', color=[0,0,ind/len(e_values)])\nplt.xlim([-100, 135])\nplt.xlabel('Membrane potential (mV)')\nplt.ylabel('Ionic current (pA)')\nplt.grid()\nplt.title('Effect of reversal potential on ionic current')\n\nText(0.5, 1.0, 'Effect of reversal potential on ionic current')\n\n\n\n\n\n\n\n\n\nAs we shift the value of the equilibrium potential, the voltage at which the ionic current shifts from negative to positive shifts in tandem.\nBy itself, this circuit does not produce any interesting temporal dynamics or integrative abilities. To begin to capture those, we need to incorporate another detail about the electrical properties of the neuronal membrane.\n\n\nMembrane as a capacitor\nThe neuron’s membrane is conceived of as a capacitor (see this paper for more info). A capacitor is composed of an insulator sandwiched between two conductors, in our case the lipid membrane acts as the insulator since charge cannot cross it, and the extracellular and intracellular ionic solutions are the conductors. Since each charge puts out an electric field attracts the opposite charge and repels the same charge, and this field decays with distance from the charge, the thinner the insulator is the stronger charges on either side of it can influence each other. If positive charges build up on one side of the capacitor, then they will draw negative charges to build up on the opposite side. The ability of a capacitor to store charge is quantified by its capacitance:\n C = \\frac{\\epsilon\\epsilon_{0}A}{d} \nwhere \\epsilon is the dielectric constant of the insulation material, \\epsilon_{0} is the polarizability of free space, A is the surface area of the capacitor, and d is the thickness of the insulator. Capacitance is measured in the unit Farads (F). This equation tells us that to increase capacitance one should enlarge the surface area of the capacitor, allowing more space to accommodate charge, and shrink the thickness of the insulator, making it easier for charges on either side to interact.\nWhat value does the capacitance take for neurons? Since all membranes are composed of a lipid bilayer, they do not differ in their material composition so their \\epsilon stays the same, and \\epsilon_{0} is a physical constant that does not change. The thickness of the neuronal membrane, d, our insulator, is also consistent across neurons, with a value of ~9 nm (for comparison, that is about eleven thousand times thinner than the thickness of printer paper). But, since neurons can vary in size, A varies greatly across neurons. So, we often use ‘specific capacitance’, which is the ratio between capacitance and area. The specific capacitance for neurons is generally around 1 𝜇F/cm2. If we approximate a neuron as a sphere, then its capacitance can be calculated using its radius to calculate the area of the sphere (A=4 \\pi r^2), and multiplying that by the 1 𝜇F/cm2. For instance, a neuron with a radius of 10 um has a capacitance of 12.5 pF.\n\n\nPassive electrical model of the membrane\n\n\n\nmembrane as capacitor\n\n\nExpanding our electrical schematic we add the membrane capacitor in parallel with the resting potential battery and conductor. This is the passive electrical model of the membrane. Many classic phenomena of neural integration arise from the passive electrical behavior of its membrane.\n\n\n\nmembrane rest\n\n\nThe battery charges up the capacitor, forcing it to adopt a potential equal to the resting potential. Once it has reached this stable state, what if we were to artificially inject a current into the neuron? To calculate this, we can use a rule from electrical theory called Kirchoff’s current law (see Kirchoff’s current law). It states that the current flowing into a node of an electrical circuit must equal the current flowing out of it. We already have an equation (Ohm’s law) that describes the current produced by our ionic leak current, now we need to know how to calculate the current produced by the capacitor.\nThe relationship between current and voltage for a capacitor is described by the equation:\n I = C\\frac{dV}{dt} \\tag{2}\nWhat this means is that the current produced across the capacitor is proportional to the change in voltage.\n\n\n\nmembrane charging with resistance\n\n\nThe passive electrical circuit model is also known as an RC circuit, because it contains a resistor and capacitor. What is its behavior? When it is first put together, there is no voltage across the capacitor, so the battery will charge it up. However, the resistor places a limit on the current, meaning the capacitor will not fully charge instantly. To determine the trajectory the voltage will take, we can model this with an equation by combining equations 1 and 2 using Kirchoff’s current law. For the circuit above, this yields the equation:  \\begin{align}\n    \\notag\n    0 &= I_{rest} + I_{C} \\\\ \\notag\n    0 &=g_{m}(V_{m}-E_{rest}) + C_{m}\\frac{dV_{m}}{dt} \\\\ \\notag\n    -C\\frac{dV_{m}}{dt}&=g_{m}(V_{m}-E_{rest}) \\\\ \\notag\n    \\frac{dV_{m}}{dt}&=-\\frac{g_{m}}{C}(V_{m}-E_{rest})\n    \\end{align}\n\nThis is a differential equation that can be solved to give voltage as a function of time:  V_{m}(t) = E_{rest}(1-e^{\\frac{-t}{\\frac{g_{m}}{C_{m}}}}) \nRewriting g_{m}, as its inverse, referred to as membrane resistance, R_{m}, we get the equation:  V_{m}(t) = E_{rest}(1-e^{\\frac{-t}{R_{m}C_{m}}}) \\tag{3}\nThe product of R_{m} and C_{m} sets how fast the membrane charges. Larger it is, the slower the membrane capacitor will charge, and the smaller it is, the faster it charges. Given realistic values of R_{m} and C_{m}, what would be a reasonable time constant to expect from a neuron?\n\n# Calculate membrane resistance and capacitance\nmem_cap = 1e-6 # F/cm^2\ncell_c = mem_cap * cell_area # F\ncell_r = mem_r / cell_area # ohm\n\ncell_tau = cell_r * cell_c # sec\n\nprint('Membrane capacitance: {:.0f} pF'.format(cell_c*1e12))\nprint('Membrane resistance: {:.0f} MOhm'.format(cell_r*1e-6))\nprint('Membrane time constant: {:.0f} ms'.format(cell_tau*1e3))\n\nMembrane capacitance: 113 pF\nMembrane resistance: 221 MOhm\nMembrane time constant: 25 ms\n\n\n\n# Plot examples of V changing over time for different RC time constants\ntau_factors = [0.1, 0.5, 1, 2, 5]\nt = np.linspace(0, 0.1, 100)\nfor ind, curr_factor in enumerate(tau_factors):\n    curr_tau = cell_tau*curr_factor\n    plt.plot(t*1e3, e_rest*(1-np.exp(-t/curr_tau))*1e3, \n             label=curr_tau*1e3, color=[0,0,ind/len(tau_factors)])\n    plt.text(t[-1]*1e3+2, e_rest*(1-np.exp(-t[-1]/curr_tau))*1e3,\n                str(round(curr_tau*1e3,2))+' ms', color=[0,0,ind/len(tau_factors)])\n    \n    \nplt.xlim([-5, 125])\nplt.xlabel('Time (ms)')\nplt.ylabel('Membrane potential (mV)')\nplt.grid()\nplt.title('Effect of RC time constant on membrane potential charging')\n\nText(0.5, 1.0, 'Effect of RC time constant on membrane potential charging')\n\n\n\n\n\n\n\n\n\nWhat this shows is that longer membrane time constants, \\tau, slow the rate of change in the membrane potential.\n\n\nCreating a passive neuron model\nEquation 3 showed us the relationship between time and membrane potential at the moment the membrane capacitance was connected with the leak current. This is, of course, not realistic. Instead, neurons start at the resting potential and recieve occasional inputs that inject currents into the neuron. To capture this, we can use the following equation:\n \\begin{align}\n    \\notag\n    0 &= I_{rest} + I_{C} + I_{in}\\\\ \\notag\n    0 &=g_{m}(V_{m}-E_{rest}) + C_{m}\\frac{dV_{m}}{dt} + I_{in}\\\\ \\notag\n    -C_{m}\\frac{dV_{m}}{dt}&=g_{m}(V_{m}-E_{rest}) + I_{in}\\\\ \\notag\n    \\frac{dV_{m}}{dt}&=-\\frac{1}{C_{m}}({g_{m}}(V_{m}-E_{rest}) + I_{in})  \\tag{4}\n    \\end{align}\n\nSince we know how the membrane voltage will change, \\frac{dV_{m}}{dt}, based on its present value, V_{m}, and the current injected, I_{in}, we can simulate its response. To do this, we will create a class, PassiveNeuron. This class will encapsulate the data and methods needed to simulate the passive properties of a neuron’s membrane.\n\nclass PassiveNeuron:\n    def __init__(self, v_rest=-65, c_m=1, r_m=25, radius=30):\n        self._vrest = v_rest/1000 # mV\n        self._cm = c_m * 1e-6 # uF/cm^2\n        self._rm = r_m * 1e3 # kOhm*cm^2\n        self._radius = radius * 1e-4 # cm\n\n        self._area = 4 * np.pi * self._radius**2 # cm^2\n        self._c = self._cm * self._area # F  \n        self._gleak = 1 / (self._rm / self._area) # S\n\n        self._vm = self._vrest\n        self._im = 0\n        self._add = 0 # holds additional current to deliver to the neuron\n        self._dt = 0.0001 # sec\n\n    def get_tau(self):\n        return self._cm * self._rm\n    \n    def set_input(self, inp=0):\n        self._add = inp\n    \n    def reset_state(self):\n        self._vm = self._vrest\n        self._im = 0\n        self._add = 0 # holds additional current to deliver to the neuron\n    \n    def get_state(self):\n        # return membrane potential in mV, membrane current in nA, and spike status\n        return self._vm*1e3, self._im*1e9\n    \n    def get_t_vec(self, dur = 0.1, dt = 0.0001):\n        return np.arange(0, dur, dt)\n    \n    def update(self):\n        # solve for transmembrane currents\n        self._im = (self._gleak * (self._vm - self._vrest)) + self._add\n        \n        # update membrane potential\n        self._vm = self._vm + -(self._im / self._c) * self._dt\n    \n    def run(self, dur=0.1, dt=0.0001, inp=0):\n\n        self.reset_state() # reset state\n        \n        # initialize arrays to store values\n        t = self.get_t_vec(dur, dt) # time array\n        v = np.zeros(len(t)) # voltage array\n        i = np.zeros(len(t)) # current array\n        self._dt = dt # set time step\n\n        # if input is scalar, make it an array\n        if isinstance(inp, (int, float)):\n            inp = np.ones(len(t)) * inp\n        \n        # run simulation\n        for ind, _ in enumerate(t):\n            self.set_input(inp[ind]) # set input current\n            self.update() # update membrane potential and current\n            v[ind], i[ind] = self.get_state() # store values\n\n        return v, i\n\n\n# Create a neuron with default parameters\npas_nrn = PassiveNeuron()\nsim_dur = 0.2 # sec\nsim_dt = 0.0001 # sec\nt_pts = pas_nrn.get_t_vec(dur=sim_dur, dt=sim_dt)\n\n# Create a step current input\ndef step_current(t, start=0.025, end=0.125, amp=-1e-10):\n    step = np.zeros(len(t))\n    step[np.where((t&gt;=start)&(t&lt;end))[0]] = amp\n    return step\n\nin_step = step_current(t_pts)\npas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)\n\ndef plot_sim(t, v, i, inp, title=''):\n    plt.subplot(3,1,1)\n    plt.plot(t*1e3, v, color='b')\n    plt.yticks(color='b')\n    plt.ylabel('mV', color='b')\n    plt.grid()\n    plt.title('Membrane potential')\n\n    plt.subplot(3,1,2)\n    plt.plot(t*1e3, i, color='r')\n    plt.yticks(color='r')\n    plt.ylabel('nA', color='r')\n    plt.title('Membrane current')\n    plt.grid()\n\n    plt.subplot(3,1,3)\n    plt.plot(t*1e3, inp*1e9, color='k')\n    plt.xlabel('Time (ms)')\n    plt.yticks(color='k')\n    plt.ylabel('nA', color='k')\n    plt.title('Input current')\n    plt.grid()\n    plt.suptitle(title)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\nplot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron')\n\n\n\n\n\n\n\n\nWhen we inject a current step into the neuron, the membrane begins to depolarize from its resting potential towards a new voltage. This is accompanied by an strong current crossing the membrane, which reflects the injected current charging up the membrane capacitance. As the membrane capacitance charges up, this current decays because it is offset by the leak current, which flows in the opposite direction of the injected current. The leak current is driven by the difference between V_{m} and E_{r}. For the leak current to balance out the injected current, the membrane potential must reach:\n V_{m} = \\frac{I_{in}}{g_{leak}}+E_{r}\nThus, a increasing g_{leak} (or lowering membrane resistance, R_{m}), lowers the change in membrane potential to an injected current. We can see this here:\n\npas_nrn_hig = PassiveNeuron(v_rest=-65, c_m=1, r_m=5, radius=30) # we lowered the membrane resistance, r_m, which increased the membrane conductance, g_m\npas_v_hig, pas_i_hig = pas_nrn_hig.run(dur=sim_dur, dt=sim_dt, inp=in_step)\nplot_sim(t_pts, pas_v_hig, pas_i_hig, in_step, title='Passive neuron with higher $g_{leak}$')\n\n# Get values of g_leak for each model\nprint('g_leak for regular model: {:.0f} nS'.format(pas_nrn._gleak*1e9))\nprint('g_leak for high g model: {:.0f} nS'.format(pas_nrn_hig._gleak*1e9))\n\ng_leak for regular model: 5 nS\ng_leak for high g model: 23 nS\n\n\n\n\n\n\n\n\n\nIncreasing g_{leak} lowered the change in membrane potential from around 12 mV to 3 mV. Notice also that V_{m} changes much faster now, because the membrane time constant is shorter.\nIncreasing the amount of current we inject also increases the change in V_{m}.\n\nin_step = step_current(t_pts, amp=-2e-10) # increase input current by 2x\npas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)\nplot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron with $2xI_{in}$')\n\n\n\n\n\n\n\n\nSo far we have been giving negative current injections, which pushes the membrane towards positive values. If V_{m} moves towards positive values, we say it is depolarizing. By contrast, if it is pushed towards negative values, it is hyperpolarizing. To hyperpolarize, we inject a positive current.\n\n# hyperpolarize the membrane potential\nin_step = step_current(t_pts, amp=1e-10)\npas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)\nplot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron with being hyperpolarized')\n\n\n\n\n\n\n\n\nAll the activity we have seen so far is slow changes in membrane potential. These are not conveyed to downstream neurons. Instead, neurons have to fire action potentials to influence their neighbors.\n\n\nAction potentials\nAction potentials are large positive excursions of the membrane potential. They last less than 2 ms and propagate from the cell body down the axon, eventually triggering release of neurotransmitter at the synapse. They are the principal output of neurons and, for the most part, the only information about the state of a neuron that is conveyed to the rest of the network.\n\n\n\nmembrane with action potential\n\n\nTo generate an action potential, a neuron is first depolarized to a sufficient level that starts to open voltage-gated Na+ ion channels. Recalling that Na+ has a positive reversal potential, their opening will depolarize the membrane voltage. This depolarization recruits more voltage-gated Na+ ion channels, reinforcing the depolarization and driving the membrane potential towards the Na+ reversal potential (~40 mV). Then, these channels begin to inactivate, closing and bringing the membrane back towards its resting potential. This fall back towards rest is accelerated by voltage-dependent K+ channels, which open during the depolarization of the action potential and drive the membrane towards the K+ reversal potential (~-70 mV). During this period, the neuron enters a refractory period. The first phase of this is absolute, where the neuron cannot fire an action potential due to the inactivation of Na+ channels. This is followed by a relative period, where the an action potential can be generated again, but the K+ current counteracts this, effectively increasing the amount of current required to cross the voltage threshold.\nThe voltage-gated Na+ and K+ currents are similar in principal to the leak current discussed earlier. They are a battery, the equilibrium potential of Na+ or K+, in series with a conductor. The difference is that the conductor depends on V_{m} and time. We can expand our electrical model of the passive neuron to incorporate these voltage-gated currents, thus allowing us to model action potentials.\nThe equation that captures this is:\n \\frac{dV_{m}}{dt}=-\\frac{1}{C}(g_{m}(V_{m}-E_{rest}) + g_{Na_{V,t}}(V_{m}-E_{Na}) + g_{K_{V,t}}(V_{m}-E_{K}) + I_{in}) \\tag{5} \nHere g_{Na_{V,t}} is the voltage and time dependent conductance for Na+, and similar for g_{K_{V,t}}. The driving force for those currents depends on how far V_{m} is from the respective equilibrium potentials of those ions. In essence, increasing g_{Na_{V,t}} or g_{K_{V,t}} pulls V_{m} towards the corresponding equilibrium potential.\n\n\n\n\n\n\nA note about coding and neuronal firing\n\n\n\nAction potentials are often described as all-or-none events. Either they occur or they do not, and when they do, they tend to always have the same strength and duration. Since they are stereotyped, it is thought that they convey information by their timing. Several different terms are often used to describe the codes offered by action potentials. A time code is when the exact time of an action potential indicates the occurrence of some event (whether it be environmental or behavioral). This is often counterposed to a rate code, where the informational signal is how many action potentials occurred in some short amount of time, with a higher rate indicating a ‘stronger’ signal. Alternatively, a population code uses the set of neurons firing action potentials at any given moment to encode information. Different ensembles of activated neurons signal different events.\n\n\n\n\nCreating a simplified active neuron model\nWhen we include voltage-gated ion channels in a model neuron, it goes from being a passive model to an active model. We can build upon our existing PassiveNeuron class to create an ActiveNeuron class that generates an approximation of action potentials. To do this, we will add the voltage-dependent Na+ and K+ currents. This requires specifying the equilibrium potential for those ions, a V_{m} threshold that initiates the action potential, and designing voltage- and time-dependent conductances to gate the active currents.\n\n# create active neuron class that inheriting from the passive neuron model\nclass ActiveNeuron(PassiveNeuron):\n    def __init__(self, v_thresh=-50, ena=50, ek=-90, gna=8e-8, gk=4e-8, **kwargs):\n        super().__init__(**kwargs) # allows us to pass arguments for the passive properties of the neuron\n        self._vthresh = v_thresh / 1000 # mV\n        self._ena = ena / 1000 # mV\n        self._gna = gna # S\n        self._ek = ek / 1000 # mV\n        self._gk = gk # S\n        self._spk_timer = 0\n        self._spk = False\n    \n    def reset_state(self):\n        super().reset_state()\n        self._spk_timer = 0\n        self._spk = False\n\n    def gen_ap(self):\n        # action potential mechanism\n        if (self._vm &gt; self._vthresh) & (self._spk_timer &lt;= 0):\n            self._spk_timer = 0.004 # start countdown timer for duration of action potential\n            self._spk = True\n        elif self._spk_timer &gt; 0.003: # open up sodium conductance for first 1 ms\n            self._add = self._add + self._gna * (self._vm - self._ena)\n            self._spk = False\n            self._spk_timer -= self._dt\n        elif self._spk_timer &gt; 0: # open up potassium conductance for next 3 ms\n            self._add = self._add + self._gk * (self._vm - self._ek)\n            self._spk = False\n            self._spk_timer -= self._dt\n\n    def run(self, dur=0.1, dt=0.0001, inp=0):\n\n        self.reset_state() # reset state\n        \n        # initialize arrays to store values\n        t = self.get_t_vec(dur, dt) # time array\n        v = np.zeros(len(t)) # voltage array\n        i = np.zeros(len(t)) # current array\n        self._dt = dt # set time step\n\n        # if input is scalar, make it an array\n        if isinstance(inp, (int, float)):\n            inp = np.ones(len(t)) * inp\n        \n        # run simulation\n        for ind, _ in enumerate(t):\n            self.set_input(inp[ind]) # set input current\n            self.gen_ap() # generate action potential &lt;-- NEW\n            self.update() # update membrane potential and current\n            v[ind], i[ind] = self.get_state() # store values\n\n        return v, i\n\n\n# Create an active neuron and deliver a subthreshold and over threshold current\nact_nrn = ActiveNeuron()\nsim_dur_act = 0.5\nt_pts = act_nrn.get_t_vec(dur=sim_dur_act, dt=sim_dt)\nin_step_act = step_current(t_pts, start=0.1, end=0.2, amp=-5e-11) + \\\n                step_current(t_pts, start=0.3, end=0.4, amp=-1e-10)\n\nact_v, act_i = act_nrn.run(dur=sim_dur_act, dt=sim_dt, inp=in_step_act)\nplot_sim(t_pts, act_v, act_i, in_step_act, title='Active neuron with subthreshold and overthreshold current')\n\n\n\n\n\n\n\n\nFor this model we set the action potential threshold to -50 mV, and if we depolarize below that level we get the passive subthreshold depolarization that we saw in the previous model. If the current step is increased, then the membrane depolarizes passively until the threshold is reached, at which point an action potential is triggered. The action potential voltage approaches the equilibrium potential for Na+, and then quickly reverses towards the equilibrium potential of K+. The transmembrane currents associated with the action potential are substantially greater than those from the subthreshold depolarization. This is due to the very high conductance of the voltage-gated channels (Na+ channel: 80 nS vs. g_{leak}: 5 nS).\nMore than one action potential is emitted during the 100 ms over-threshold current step. The time between action potentials depends on two factors. First is the refractory period, with a stronger or longer refractory period delaying the time to the next action potential. The other is the strength of the injected current, with a stronger current able to overcome the refractory effect and elicit another action potential sooner. We can explore the effect of current strength by delivering a series of current pulses to our model with increasing amplitudes.\n\n# Examine how firing rate varies with curent injected\nact_nrn = ActiveNeuron()\nsim_dur_act = 1.1\nt_pts = act_nrn.get_t_vec(dur=sim_dur_act, dt=sim_dt)\n\n# create a series of step currents with increasing amplitudes\nin_step_act = step_current(t_pts, start=0.1, end=0.2, amp=-5e-11) + \\\n                step_current(t_pts, start=0.3, end=0.4, amp=-8e-11) + \\\n                step_current(t_pts, start=0.5, end=0.6, amp=-1.6e-10) + \\\n                step_current(t_pts, start=0.7, end=0.8, amp=-2.4e-10) + \\\n                step_current(t_pts, start=0.9, end=1.0, amp=-3.2e-10)\n\nact_v, act_i = act_nrn.run(dur=sim_dur_act, dt=sim_dt, inp=in_step_act)\nplot_sim(t_pts, act_v, act_i, in_step_act, title='Reponse to increasing current steps')\n\n\n\n\n\n\n\n\nIncreasing the current step amplitude increased the number of action potentials emitted, and shortened the time between them. Thus, the rate of action potential generation is proportional to the excitatory drive the neuron receives.\nHowever, it is important to remember that neurons are not normally driven by artificial current steps. Instead, they have synapses that are driven by the action potentials from other neurons.\n\n\nSynapses\nSince neurons form networks that share electrical signals, there must be a means for these signals to be passed from one neuron to another. This exchange occurs at synapses, where the axon from the presynaptic neuron forms a terminal on the dendrite of the postsynaptic neuron. Between them is a narrow space called the synaptic cleft, where neurotransmitters released by the presynaptic terminal crosses to binds to ion channels on the postsynaptic neuron. When the neurotransmitter binds it opens an ion channel, which allows a current composed of the ions that channel is permeable to to flow. Channels that are found at excitatory synapses, which drive the membrane potential towards positive values (depolarize), tend to be permeable positive ions, Na+, K+, and Ca2+. Inhibitory synapses rely on ion channels selective for the negative Cl- ion and can counteract the excitatory depolarization.\nCortical neurons are normally bombarded by an ongoing stream of excitatory and inhibitory synaptic activity. Usually these are balanced, so that the membrane potential shows only small changes in its level. However, if the excitatory synaptic drive overwhelms the inhibition it can push the membrane potential towards positive values and potentially trigger an action potential.\nThe ion channels that support synaptic transmission can be incorporated in our circuit model as just another branch, with an equilibrium potential reflecting their ionic permeability and a conductance that depends on number of open ion channels.\n\n\n\nMembrane with synapse\n\n\nAs for the leak and action potential related currents, the ionic current arising from synaptic transmission has the form:  I_{syn} = g_{syn}r(V_{m}-E_{syn})\nHere g_{Syn} is the total possible conductance, and r is the proportion of synaptic ion channels that are open. This depends on the presence of neurotransmitter in the synaptic cleft and time, and can be modeled as a reaction system, with separate rates describing the binding and dissociating of neurotransmitter to the receptor.\n\n\n\n\n\n\nModeling a synaptic current\n\n\n\nThe shape of the ionic current arising from activating a synapse is described as having an \\alpha- or double exponential shape. Our passive neuron model has a single exponential shape: there is only one time constant, \\tau, that describes the timing of its rise and fall in response to a current input. For a double exponential, there are two time constants, one for the rise and a different one for the fall. The time constant for the rising phase is faster than that for the falling phase, leading to a skewed response. The differential equation that captures this is:\n \\frac{dr}{dt} = \\alpha[T]r - \\beta(1-r) \nThe equation states that the change in the proportion of open ion channels, r, increases at a rate \\alpha when neurotransmitter, [T], is present in the synapse. When neurotransmitter is absent from the synapse, r then decreases at the rate of \\beta. (For more details, see Destexhe, Mainen & Sejnowski 1994).\n\n\nWe can incorporate synapses into our active model to explore they affect the membrane potential and drive action potentials.\n\n# create an active neuron with synapse class that inherits from the active neuron model\nclass SynapticNeuron(ActiveNeuron):\n    def __init__(self, esyn=50, gsyn=5e-9, asyn=900, bsyn=500, tdur=3, **kwargs):\n        super().__init__(**kwargs) # allows us to pass arguments for the passive properties of the neuron\n        self._esyn = esyn / 1000 # mV\n        self._gsyn = gsyn # S\n        self._asyn = asyn \n        self._bsyn = bsyn\n        self._tdur = tdur / 1000 # ms, duration of transmitter release\n        self._syn_timer = 0 # timer for transmitter release\n        self._r = 0 # fraction of open channels\n\n    def reset_state(self):\n        super().reset_state()\n        self._syn_timer = 0 # timer for transmitter release\n        self._r = 0 # fraction of open channels\n\n    def gen_syn(self, prespk=False):\n        # synaptic mechanism\n        if prespk:\n            self._syn_timer = self._tdur\n        elif self._syn_timer &gt; 0:\n            self._syn_timer -= self._dt\n        \n        # update fraction of open channels\n        self._r = self._r + self._dt * ((self._asyn * (self._syn_timer&gt;0) * (1-self._r)) \\\n                                        - (self._bsyn * (self._r)))\n        \n        # add synaptic current\n        self._add = self._add + self._gsyn * self._r * (self._vm - self._esyn)\n\n    def run(self, dur=0.1, dt=0.0001, inp=0, prespk=False): # added prespk to drive synapse\n\n        self.reset_state() # reset state\n        \n        # initialize arrays to store values\n        t = self.get_t_vec(dur, dt) # time array\n        v = np.zeros(len(t)) # voltage array\n        i = np.zeros(len(t)) # current array\n        self._dt = dt # set time step\n\n        # if input is scalar, make it an array\n        if isinstance(inp, (int, float)):\n            inp = np.ones(len(t)) * inp\n\n        if isinstance(prespk, (bool, int, float)):\n            prespk = np.ones(len(t)) * prespk\n        \n        # run simulation\n        for ind, _ in enumerate(t):\n            self.set_input(inp[ind]) # set input current\n            self.gen_syn(prespk[ind]) # generate synaptic response &lt;-- NEW\n            self.gen_ap() # generate action potential\n            self.update() # update membrane potential and current\n            v[ind], i[ind] = self.get_state() # store values\n\n        return v, i\n\n\n# MODIFY PLOTTING CODE HERE\ndef plot_sim_prespk(t, v, i, inp, spk, title=''):\n\n    # ensure neuron inputs have same length as time vector\n    if isinstance(inp, (int, float)):\n        inp = np.ones(len(t)) * inp\n    \n    if isinstance(spk, (bool, int, float)):\n        spk = np.zeros(len(t))\n    \n    plt.subplot(2,1,1)\n    plt.plot(t*1e3, v, color='b')\n    plt.vlines(t[spk==1]*1e3, plt.ylim()[0], plt.ylim()[1], color='k')\n    plt.yticks(color='b')\n    plt.ylabel('mV', color='b')\n    plt.grid()\n    plt.title('Membrane potential')\n\n    plt.subplot(2,1,2)\n    plt.plot(t*1e3, i, color='r')\n    plt.vlines(t[spk==1]*1e3, plt.ylim()[0], plt.ylim()[1], color='k')\n    plt.yticks(color='r')\n    plt.ylabel('nA', color='r')\n    plt.title('Membrane current')\n    plt.grid()\n\n    plt.suptitle(title)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n# Function for generating spike trains\ndef spk_train(t, spk_times=0.1):\n    spk_times = np.array([spk_times]) # ensures spk_times is a numpy array\n    spk = np.zeros(len(t))\n    spk[np.round(spk_times / sim_dt).astype(int)] = 1\n    return spk\n\n\n# examine response to a single presynaptic spike\nsyn_nrn = SynapticNeuron()\nsim_dur_syn = 0.2\nt_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)\n\nin_prespk = spk_train(t_pts, [0.05])\n\nsyn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)\nplot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to a single presynaptic spike')\n\n\n\n\n\n\n\n\nActivating the synapse at 50 ms leads to a rapid negative current that charges the membrane capacitance and depolarizes V_{m}. This is soon followed by a decay in the synaptic current, as the proportion of synaptic ion channels that were open shrinks. Within ~10 ms the synaptic current is finished, and replaced by a positive leak current, which is the membrane returning to its resting potential. The rate of this return to rest is determined by the membrane time constant.\nA single synaptic input is usually insufficient to drive an action potential. Typically multiple synaptic events have to be combined in close temporal proximity. We can explore this by delivering two presynaptic spikes and varying the time between them.\n\n# delivery multiple presynaptic spikes with varying inter-spike intervals\nsim_dur_syn = 1.1\nt_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)\n\nin_prespk = spk_train(t_pts, [0.1, 0.15, 0.4, 0.425, 0.7, 0.7125, 1.0, 1.00625])\n\nsyn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)\nplot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to multiple presynaptic spikes')\nplt.gcf().set_size_inches((12, 6))\nplt.gcf().axes[0].axhline(syn_nrn._vthresh*1e3, color='k', linestyle='--')\n\n\n\n\n\n\n\n\nThe dashed line for the membrane potential plot is the action potential threshold. A single synaptic response is not sufficient to cross this threshold. If two are delivered, they must be close enough together, less than 25 ms between them, to trigger an action potential. Note also that making them closer does not increase the number of action potentials, only one is generated. To elicit multiple action potentials would require a sustained train of presynaptic spikes, such as below:\n\n# deliver a train of presynaptic spikes at 40 hz\nsim_dur_syn = 0.5\nt_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)\n\nin_prespk = spk_train(t_pts, np.arange(0.05, 0.4, 1/40))\n\nsyn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)\nplot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to a 40 Hz spike train')\n\n\n\n\n\n\n\n\nDelivering a train of presynaptic spikes at 40 Hz elicits a steady stream of action potentials in the postsynaptic neuron. Not every presynaptic spike elicits an action potential, instead they have to summate. In this case, it took 3 presynaptic spikes to elicit the first action potential, and subsequently every 2 spikes drove an action potential."
  },
  {
    "objectID": "Week1.html#detecting-the-activity-of-neurons",
    "href": "Week1.html#detecting-the-activity-of-neurons",
    "title": "Week 1",
    "section": "Detecting the activity of neurons",
    "text": "Detecting the activity of neurons\nSo far, we have been tracking neural activity by measuring the voltage and currents across the membrane. Experimentally, this is done using fine glass pipettes that are inserted into or make a hole in single neurons. This approach, known as intracellular or whole-cell patch recording, is not practical in humans, and exceptionally difficult in awake behaving animals. Instead, we typically use metal electrodes that do not need to be in direct contact with the neurons whose activity we want to detect. These electrodes pick up weak electrical signals emanating from nearby neurons, a technique referred to as extracellular recording. How does this work?\n\nPhysics of extracellular potentials\nAs we discussed above, the currents flowing across the membrane are composed of ions. Each ion has a charge, and charge produces an electric field that exerts a force on other charges. The force required move a charge through an electric field can be picked up as a potential on an electrode. Since the neuronal lipid membrane is an insulator, our electrode can only pick up the movement of charges across the membrane. If positive charges flow into the neuron, that is a net negative of charge on the outside an a negative potential results. If positive charges flow out of the neuron, that is a negative positive of charge on the outside, causing a positive potential.\n\n\n\nExtracellular field of a cat motor neuron firing an action potential (numbers are in millivolts). From Rall 1962.\n\n\nThus, the currents flowing through the membrane during synaptic barrages or action potentials can be detected by nearby electrodes. Fortunately for us, these transmembrane currents are so weak and so slow that we can use a simple equation to describe their influence on the voltage picked up by a nearby electrode. This is given by the equation:\n V = \\frac{1}{4\\pi\\sigma}\\frac{I}{d} \nHere \\sigma is the conductivity of neural tissue, I is a transmembrane current, and d is the distance between the electrode and the current. A simplifying assumption we make when using this equation is that the the conductivity of the brain is homogeneous (sometimes referred to as being Ohmic). A few trends are clear from this equation. The influence that a transmembrane current has on an electrode will only depend on the the distance the electrode has from the current, and the current’s sign and magnitude. The further a current is from the electrode, the weaker it gets, with the greatest fall off happening at short distances. If a current is flowing into the neuron (excitatory), then it will produce a negative voltage at the electrode, while a current flowing out of the neuron (inhibitory) produces a positive voltage. The magnitude of the voltage signal will be linearly proportional to the voltage, e.g. doubling the current will double the voltage.\nThere are multiple current sources in the brain. Every open ion channel is a potential current source and these carpet the entire neuronal membrane. To capture their collective influence on a recording electrode, we can treat each as producing their own voltage and add them together.\n V = \\frac{1}{4\\pi\\sigma}\\sum_{i=0}^{n}\\frac{I_i}{d_e - d_i} \\tag{6}\nIn this equation we sum the resulting voltages from all currents, with each current I_i divided by its distance from the recording electrode, d_e - d_i. Stronger currents will tend to dominate over weaker ones. However, if there are a large number widely distributed of weak currents with similar waveforms then when added together they can produce a prominent potential.\n\n\nModeling the relationship between electrode distance and recorded potential\nUsing our SynapticNeuron model, we can explore how different types of neural activity are detected by extracellular electrodes. This model will simulate a collection of neurons arranged in a circular sheet. To capture the kinds of activities observed in the brain, we will include a slow global fluctuation in membrane potential across all the neurons (slow_noise), noise that is unique to each neuron (noise), and a pulse of synaptic drive (mean_t and std_t). It will also require a method to calculate the extracellular potential, using equation 6 (calc_extracell).\n\n\n\n\n\n\nA caveat to our extracellular recording model\n\n\n\nThe extracellular recording model we implement below is technically incorrect. Recall that Kirchoff’s current law requires the total current flowing into a point in a circuit to equal the amount flowing out. For our extracellular model, we measure the current flowing across the membrane on the extracellular side, which is composed of all the passive, action potential, and synaptic currents, which must sum to 0. This means that the total current flowing across the membrane in our model is 0, which would produce no extracellular field given equation 6. So, why do we detect extracellular potentials in the real world? This is because neurons are composed of more than just their cell body. They also have dendrites and axons. This allows currents to flow not only across the membrane, but also within the neuron itself. Thus, the total current flowing across a single segment of the membrane does not have to balance, since part of that current will flow inside the neuron to another part of the neuron, and eventually flow out of the membrane in a different segment. If you want to realistically model the extracellular field, this requires your neuron model to have multiple compartments that are connected together, each modeled using the formalism we use here. In contrast, our model is a single compartment model. To get an extracellular field, we will just measure the leak current, which responds to all the other currents crossing the membrane.\n\n\n\nclass ExtracellRec():\n    # class for extracellular recording of a sheet of neurons using the SynapticNeuron class\n    # user can set the radius of the sheet, the density of neurons, the mean time and standard deviation \n    # around when a synaptic input arrives, the level of a shared slow input noise, and an individual gaussian noise\n    # level for each neuron, electrode distance from the sheet\n    def __init__(self, radius=2, density=1000, mean_t=0.1, std_t=0.005, slow_noise=25, \\\n                 noise=200, extra_cond=0.3, **kwargs):\n        self._radius = radius # cm\n        self._density = density # neurons per cm^2\n        self._mean_t = mean_t # sec\n        self._std_t = std_t # sec\n        self._slow_noise = slow_noise * 1e-12 # slow shared noise standard deviation in pA\n        self._noise = noise * 1e-12 # individual neuron gaussian standard deviation in pA\n        self._extra_cond = extra_cond # extracellular conductivity in S/m\n        self._v = [] # voltage array\n        self._i = [] # current array\n        self._t = [] # time array\n\n        # calculate number of neurons\n        self._n_neurons = int(np.round(np.pi * self._radius**2 * self._density))\n\n        # calculate positions of neurons, uniformly distributed in the circular sheet\n        # place each neuron by setting a random angle and radius from the center\n        self._neuron_pos = np.zeros((self._n_neurons, 2))\n        for ind in range(self._n_neurons):\n            curr_ang = np.random.rand() * 2 * np.pi\n            curr_rad = np.sqrt(np.random.rand()) * self._radius\n            self._neuron_pos[ind, 0] = np.cos(curr_ang) * curr_rad\n            self._neuron_pos[ind, 1] = np.sin(curr_ang) * curr_rad\n        # force one neuron to be at the center, so it can be easily picked up by the electrode\n        self._neuron_pos[0, :] = 0\n\n        # create neurons\n        self._neurons = []\n        for ind in range(self._n_neurons):\n            self._neurons.append(SynapticNeuron(**kwargs))\n\n    # method to calculate extracellular potential based on currents from each neuron\n    def calc_extracell(self, h=1):\n        dists = np.sqrt(np.sum(self._neuron_pos**2,axis=1)+h**2)\n        return 1/(4*np.pi*self._extra_cond) * np.sum((self._i.T * 1e-9) / dists, axis=1)\n\n\n        \n    # create run function that will run all neurons for a given duration\n    def run(self, dur=0.2, dt=0.0001, seed=47):\n        # set random seed\n        np.random.seed(seed)\n\n        # initialize arrays to store values\n        t = self._neurons[0].get_t_vec(dur, dt) # time array\n        num_t = len(t)\n        v = np.zeros((self._n_neurons, num_t)) # voltage array\n        i = np.zeros((self._n_neurons, num_t)) # current array\n        \n\n        # create synaptic inputs\n        in_prespk = []\n        for ind in range(self._n_neurons):\n            in_prespk.append(spk_train(t, np.random.normal(self._mean_t, self._std_t, 1)))\n        \n        # create slow noise input\n        slow_noise = np.random.normal(0, 1, num_t)\n        slow_noise = sig.detrend(np.cumsum(slow_noise))\n        slow_noise = (slow_noise/(np.std(slow_noise))) * self._slow_noise\n\n        # create individual noise inputs\n        indiv_noise = np.random.normal(0, self._noise, (self._n_neurons, len(t)))\n\n        # run simulation\n        for ind, curr_nrn in enumerate(self._neurons):\n            inp_sig = slow_noise + indiv_noise[ind, :]\n            v[ind, :], i[ind, :] = curr_nrn.run(dur=dur, dt=dt, inp=inp_sig, prespk=in_prespk[ind])\n\n        # save simulation results\n        self._t = t\n        self._v = v\n        self._i = i\n\nNow that our simulation class is all setup, let’s use it. We will create a sheet of neurons with a radius of 2 cm and 1000 neurons per cm2. Using **kwargs, we are also able to pass arguments to SynapticNeuron, to configure its electrophysiological properties. We will take advantage of this by increasing the synaptic conductance, gsyn, so that action potentials are more reliably driven. The timing of the presynaptic spikes is set by a normal distribution, with the mean time of their emission at 100 ms, and a standard deviation of 5 ms. Since the inputs to our model neurons are generated randomly, and we want to replicate our results, we will fix the random seed of the model to ensure that the same output is generated each time we run it.\n\ntest = ExtracellRec(gsyn=7e-9)\ntest.run(seed=41)\n\nThat is all it took to run our simulation. The more neurons we include, either by increasing the size of our sheet with the radius parameter or their packing with the density parameter, longer it takes to run. Once the simulation is finished running, we can calculate the extracellular potential for electrodes at different distances from the center of the model. We will choose a couple key values that are relevant to the recordings we will perform. An electrode on the scalp would be ~2 cm away from the surface of the brain. Intracranial surface electrodes that rest on the brain surface are picking up the activity of neurons with 0.5 mm. Lastly, depth electrodes that are inserted into the brain will detect neurons within tens of microns.\n\nelec_dists = [2, 0.05, 0.001] # cm\ndist_labels = ['2 cm', '0.5 mm', '10 um']\n\nfig, ax = plt.subplots(len(elec_dists),1, figsize=(5, 12), sharey=True)\nfor ind, curr_dist in enumerate(elec_dists):\n    ax[ind].plot(test._t*1e3, test.calc_extracell(h=curr_dist)*1e6)\n    ax[ind].set_title(dist_labels[ind])\n    ax[ind].set_xlabel('Time (ms)')\n    ax[ind].set_ylabel('Extracellular potential (uV)')\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\nEach of these distances reveal different features of neural activity. The most obvious difference is that the signals tend to get larger as the electrode is moved closer to the sheet of neurons. At 2 cm, slow potentials, both the slow global signal and the synaptic drive, are evident. Then, moving to 0.5 mm we start to pick up some weak local spiking, and still can reliably detect synaptic currents. Note that the weak spiking we see is concentrated at the peak of the synaptic drive, with numerous overlapping spikes filling in the negative trough made by the depolarizing synaptic current. If we push the electrode to within 10 um of a neuron, the action potential becomes the dominant part of the signal. While the synaptic current is still visible prior to the spike, it is masked by the action potential once that occurs.\nAnd going a little further, we can push the electrode right against the cell body of the neuron. This is juxtacellular recording, and it has a distinctive appearance where global and synaptic potentials are towered over by the robust currents generated by the action potential.\n\nplt.plot(test._t*1e3, test.calc_extracell(h=0.0001)*1e6)\nplt.title('1 um')\nplt.xlabel('Time (ms)')\nplt.ylabel('Extracellular potential (uV)')\n\nText(0, 0.5, 'Extracellular potential (uV)')"
  }
]