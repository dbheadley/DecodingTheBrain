[
  {
    "objectID": "Week2.html",
    "href": "Week2.html",
    "title": "Week 2",
    "section": "",
    "text": "The most accessible way to measure brain activity is using electrodes placed on the scalp, referred to as electroencephalography (EEG). These are weak signals, on the order of 10s of microvolts (uV), which is ~100,000 times weaker than the voltage of a AAA battery. To measure these we use specialized electrodes and amplifiers. We will only briefly discuss these since this course is focused on the signal processing and data analysis end of BCI."
  },
  {
    "objectID": "Week2.html#what-we-will-cover",
    "href": "Week2.html#what-we-will-cover",
    "title": "Week 2",
    "section": "What we will cover",
    "text": "What we will cover\n\nHow EEG signals are acquired\nLoading them with Python\nPreprocessing and cleaning them\nExtracting event-related potentials\n\n\nimport os\nimport pickle\nimport os.path as op\nimport pandas as pd\nimport numpy as np\nimport scipy.io as sio\nimport scipy.signal as ss\nimport scipy.stats as st\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "Week2.html#acquisition-of-eeg",
    "href": "Week2.html#acquisition-of-eeg",
    "title": "Week 2",
    "section": "Acquisition of EEG",
    "text": "Acquisition of EEG\nEEG was one of the first widely accessible ways to track the activity of awake behaving subjects. It was developed in the 1920s by Hans Berger. He found that by inserting metal electrodes into the scalp, he was able to detect extremely weak electrical signals. While initially dismissed by the scientific community, his work was subsequently validated by the prominent neurophysiologist E.D. Adrien. Given the ease of obtaining the EEG signal, and that it did not require opening up the skull, it rapidly became a widespread tool for the study of brain activity. Today, it is commonly used in medicine to help diagnose medical conditions, such as epilepsy, characterize sleep disturbances using polysomnography, and in research labs to further our understanding of brain function. It is also a technique for studying brain activity that is readily accessible to the amateur, with many companies selling do-it-yourself EEG recording kits (e.g. OpenBCI). For a thorough manual on acquiring and interpreting EEG signals check out Niedermeyer’s Electroencephalography book.\n\n\n\nEEG electrodes\n\n\n\nEEG signal\nAs we reviewed in Week 1, the transmembrane currents of neurons produce voltages that can be picked up by electrodes. As those electrodes are moved farther away from the neurons, this voltage falls off. When the electrode reaches the surface of the scalp, the voltage from any individual neuron is so weak that it cannot be distinguished from another. Instead, it is the shared transmembrane currents across all the neurons within several centimeters of the electrode that make up the EEG signal. At the end of last week we saw that the very large currents arising from action potentials independently firing in single neurons were not visible once we were 2 cm away from them. However the weaker, but shared, synaptic currents were evident. It is estimated that the total area of cortex required to be coactivated sufficiently to generate a detectable scalp potential is around 6 cm2 (Ebersole 1997). Consequently, the EEG reflects widespread activation of cortical tissue.\n\n\n\nEEG signal\n\n\nOur simplified model ignored a few details that apply to real brains and that influence the EEG. First, neurons are not ‘point-sources’, i.e. they are not a single current source but have dendrites that currents also flow through. This create a dipole that influences the distribution of transmembrane currents and the voltages recorded at the scalp.\n\n\n\nneuron dipole\n\n\nSecond, the cortex is not a flat sheet of neurons but instead has a convoluted surface, with neurons oriented at different angles with respect to the scalp. This angling affects both the magnitude and the sign of voltage picked up on the scalp, making it exceptionally difficult to reconstruct exactly what neural sources generated any recorded potential.\n\n\n\ncortical convolutions\n\n\nLastly, we assumed that the conductivity of the tissue between the neuron and the scalp electrode was Ohmic (homogeneous), but this is not the case. A neural potential passes through brain tissue, the cerebrospinal fluid, dura mater that encapsulates the brain, bone of the skull, and the skin. These tissues have different conductivities, further complicating the reconstruction of the underlying neural generators.\n\n\nElectrode placement\nThe electrodes used in EEG research are small metal discs that are pressed into a conductive gel resting on the scalp. This gel allows electrical activity on the scalp to easily conduct to the electrode. Often the skin under the gel has to be abraded a bit to remove the top layer of dead skin cells and oil that have low conductivity.\nWhile it is not possible to exactly reconstruct the EEG sources, by placing multiple electrodes across the scalp one can observe some coarse localization of brain activity (see here for a nice discussion). For instance, presenting an auditory stimulus elicits a potential over the portion of the scalp overlying the temporal lobe, where auditory cortex is, while visual stimuli elicit potentials over occipital cortex, where visual cortex is. So, there seems to be some loose relationship between a signal in the EEG and the source of that signal. This highlights the need to place multiple electrodes over the scalp to get a complete picture of the brain activity.\nIn 1958 a standard was adopted for placement and naming of EEG electrodes with respect to features on the skull. It has been updated periodically over the decades (Guidelines for Standard Electrode Position Nomenclature), with the most recent incarnation known as the 10-20 system. The 10 refers to the percent distance between adjacent electrodes along the front-back axis of the skull (also referred to as rostro-caudal or anterior-posterior). 20 refers to the percent distance between adjacent electrodes along the left-right axis of the skull (also referred to as medial-lateral). Electrodes are named with a combination of letters and numbers. Each starts with letters corresponding to the region of the brain they lie immediately over:\n\n\n\nBrain region\nLettered indicator\n\n\n\n\nPrefrontal\nFp\n\n\nFrontal\nF\n\n\nCentral\nC\n\n\nTemporal\nT\n\n\nParietal\nP\n\n\nOccipital\nO\n\n\n\nEach is followed by a number or the letter ‘z’. ‘z’ is used for sites along the midline, even numbers on the right hemisphere, and odd numbers on the left hemisphere. The numbers start at 1 and increase as electrode sites are moved away from the midline, i.e. laterally. This layout looks like:\n\n\n\nEEG placement\n\n\nRecently, the number of recording sites has increased because of advances the miniaturization of recording systems. The recordings we will draw from for these lectures use the 10-20 extended system, that features more recording sites. In addition to the names used above, also included are sites that are equally close to two areas and will have their letters joined, e.g. FC (frontal-central) and PO (parietal-occipital). Its layout is:\n\n\n\nEEG placement of 10-10\n\n\nBesides the electrodes that are explicitly targeted to record brain activity, additional electrodes are used for referencing and detection of contaminating noise and artifacts.\n\n\nReferencing\nSince voltage is a measure of the work needed to move a charge from one place to another, it is always measured between two points. The point whose signal we seek to measure is referred to as the active site. It is measured with respect to another point, the ground. Ideally, the ground would be at a point in space that is infinitely far away from the electrode and with no charges near it to contaminate the potential we are attempting to measure at the EEG site. This is not practical (nor would it work due to limitations in electronic hardware). Instead, researchers often place the ground immediately behind the ear (referred to as the mastoid). If we only measured our signal at the active site then we face a new problem: there are numerous sources of environmental electrical noise that envelope the entire head, such as emissions from the AC power in the walls. To eliminate that, one also records from a reference electrode, whose signal is subtracted from the active. Any signal that is shared between the active and reference sites is removed (which includes signal at the ground site, since both the active and reference are measured relative to the same ground electrode). The degree of removal is referred to as the common-mode rejection ratio and depends on the quality of the amplifiers and conductivity between the electrodes and scalp. A good common-mode rejection ratio is 70 decibels (dB), which means that the shared signal is reduced by a factor of 10,000,000 (dB = 10log_{10}(x))!\n\n\nLoading and inspecting the EEG\nWhat does the EEG signal look like? Let’s load and visualize some EEG data.\nThe datasets we will work with are structured in a standardized way, which should make it easy to adapt our code between them. This data is coming from openneuro.org, a public repository of neural activity recordings from humans across a range of measures (e.g. EEG, MEG, fMRI). We will focus on EEG datasets.\nEach dataset is organized as a collection of nested directories. In the top most directory we have a README file that gives details about how the EEG was recorded and the tasks that subjects performed during the recording. For the one we will work with over the next few weeks, they recorded responses to auditory and visual stimuli under under a variety of conditions.\nEach subject will have their own subdirectory, which contains directories for different aspects of the experiment. There should be one labeled ‘eeg’, where the eeg data is. Sometimes you also see a ‘beh’ folder, which has the behavior of the subject. Within the eeg folder we will focus on the *.set files. These are produced by the EEGLAB toolbox in MATLAB and contain both the EEG data and additional data about electrode names, referencing, etc.\nTo load the set file, we use the loadmat method in the Scipy IO package.\n\n# load the a .set file\nsubj = 'sub-AB58'\ndata_dir = ['.', 'data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_eeg.set'.format(subj)]\neeg_file = op.join(*data_dir) # use * to unpack the list\neeg = sio.loadmat(eeg_file, appendmat=False)\n\nA few notes about the file paths and names. In the course directory we include a ‘data’ directory that contains folders to the datasets we want to work with. This allows us to use a relative file path, where we don’t have to specify the entire list of directories that lead up the the one we are presently in (known as the absolute file path). This improves portability of the code, because someone else could put this on their computer and not have to change the file path. Another thing we do is specify the directories as a list of strings, and use the join function in the python os package to paste these together into a file name. This is because depending on whether you are running the code on a Windows, Mac, or Linux computer, the directory separator will be different (‘\\’ for windows, ‘/’ for Max and Linux).\nIn the course directory we have a folder ‘data’ , which contains all the data files for this course.\nAnother thing to notice is that the file names are descriptive. For the .set file above, the subject name (‘sub-AB58’), task (‘task-passive’), and experimental run (‘run-1’), and data type (‘eeg’) are given. These are separated by underscores. This standard formulation makes it easier to quickly look at the files in a directory and determine which ones are relevant to us and write code that automatically accesses files from specific subjects or tasks.\nOnce the eeg data is loaded in, we should inspect it to determine its data type and contents.\n\n# get data type\ntype(eeg)\n\ndict\n\n\nThe eeg data returned by the loadmat function is a dictionary type. Just to refresh, a dictionary is a data type in python containing a collection of keys, where each key is associated with a value.\n\n# get keys\neeg.keys()\n\ndict_keys(['__header__', '__version__', '__globals__', 'setname', 'filename', 'filepath', 'subject', 'group', 'condition', 'session', 'comments', 'nbchan', 'trials', 'pnts', 'srate', 'xmin', 'xmax', 'times', 'data', 'icaact', 'icawinv', 'icasphere', 'icaweights', 'icachansind', 'chanlocs', 'urchanlocs', 'chaninfo', 'ref', 'event', 'urevent', 'eventdescription', 'epoch', 'epochdescription', 'reject', 'stats', 'specdata', 'specicaact', 'splinefile', 'icasplinefile', 'dipfit', 'history', 'saved', 'etc', 'datfile', 'run'])\n\n\nIf you go through the keys, some of these are self-explanatory and give information about the experiment. These are not always filled in, so don’t depend on them. Instead, we will focus on:\n\n‘srate’ - the sample rate in Hz. This is how many voltage measurements were made each second per EEG site\n‘data’ - the EEG data\n‘chaninfo’ - the names of the eeg data channels. We can use these to determine where each EEG signal came from.\n‘ref’ - where the reference channel is\n\nLet’s see what their data types are and their values. We will create a simple function, var_inspector, to display the data type and values of each variable.\n\ndef var_inspector(var):\n    print('Type: {}'.format(type(var)))\n    print(var)\n\n# srate\nsrate = eeg['srate']\nvar_inspector(srate)\n\nType: &lt;class 'numpy.ndarray'&gt;\n[[500]]\n\n\nsrate is a 2-D numpy array with a single value. To pull out just the numeric value, we index into the numpy array:\n\nsrate = srate[0,0]\n\nNext we will get the chaninfo data.\n\nchan_info = eeg['chaninfo']\nvar_inspector(chan_info)\n\nType: &lt;class 'numpy.ndarray'&gt;\n[[(array(['D:\\\\ProjectAgingNeuromodulation\\\\AuditoryResearch\\\\EEGLAB_analysis\\\\chanlocs_corrected\\\\AB58_locs_corrected.DAT'],\n        dtype='&lt;U106'), array(['Fp1 69 -2.854012 12.903291 -1.520481 ',\n         'Fpz 69 0.583218 13.494058 -1.592608  ',\n         'Fp2 69 3.935400 12.477923 -2.102277  ',\n         'AF3 69 -3.023524 12.428141 1.361401  ',\n         'AF4 69 4.304501 11.727693 0.427403   ',\n         'F7 69 -6.639328 8.170529 -1.526560   ',\n         'F5 69 -5.876012 9.542894 0.800148    ',\n         'F3 69 -4.229360 10.265764 2.418731   ',\n         'F1 69 -1.882614 11.018632 4.587177   ',\n         'Fz 69 0.596303 10.217070 4.293318    ',\n         'F2 69 3.996278 10.694478 3.944555    ',\n         'F4 69 5.631967 9.848669 2.084015     ',\n         'F6 69 6.969560 8.879675 -0.060155    ',\n         'F8 69 7.878357 7.523402 -2.148623    ',\n         'FT7 69 -7.918441 5.400486 -0.685157  ',\n         'FC5 69 -6.848348 6.444450 1.872787   ',\n         'FC3 69 -5.007377 6.820720 4.378383   ',\n         'FC1 69 -2.332303 7.275606 6.020996   ',\n         'FCz 69 0.892164 7.275017 6.676030    ',\n         'FC2 69 3.868832 7.218509 5.929260    ',\n         'FC4 69 6.393416 6.765655 4.161855    ',\n         'FC6 69 8.110071 5.963853 1.523854    ',\n         'FT8 69 8.880818 4.564967 -1.282408   ',\n         'T7 69 -8.341170 2.798480 0.226452    ',\n         'C5 69 -7.438693 3.373324 3.054235    ',\n         'C3 69 -5.931186 3.709710 6.488027    ',\n         'C1 69 -2.569554 3.746717 7.996653    ',\n         'Cz 69 1.032521 3.911274 8.476038     ',\n         'C2 69 4.350667 3.569207 7.594067     ',\n         'C4 69 6.801077 3.610167 5.548478     ',\n         'C6 69 9.023433 2.739705 2.800857     ',\n         'T8 69 8.986069 1.829011 -0.226452    ',\n         'M1 69 -7.644597 -0.549069 -8.007587  ',\n         'TP7 69 -8.254624 -0.373276 1.011124  ',\n         'CP5 69 -7.370408 0.278624 4.028963   ',\n         'CP3 69 -5.316664 0.049608 6.245472   ',\n         'CP1 69 -2.702063 0.250874 8.319883   ',\n         'CPz 69 0.627199 -0.010582 8.411424   ',\n         'CP2 69 4.188373 -0.366793 7.784681   ',\n         'CP4 69 6.730657 -0.589212 6.652807   ',\n         'CP6 69 8.320402 -0.371316 3.731920   ',\n         'TP8 69 8.799763 -0.721895 0.744581   ',\n         'M2 69 7.864402 -0.341263 -8.226876   ',\n         'P7 69 -7.147690 -2.857133 1.425296   ',\n         'P5 69 -5.896095 -3.084760 3.659961   ',\n         'P3 69 -3.990392 -3.311368 5.491853   ',\n         'P1 69 -1.790996 -3.225753 6.718182   ',\n         'Pz 69 0.559499 -2.994738 7.218447    ',\n         'P2 69 2.982983 -3.442504 6.627176    ',\n         'P4 69 5.367558 -3.552835 5.125538    ',\n         'P6 69 6.927846 -3.798552 3.392559    ',\n         'P8 69 7.864575 -3.658688 1.023157    ',\n         'PO7 69 -5.260816 -5.210424 1.680544  ',\n         'PO5 69 -3.805657 -5.398441 3.520525  ',\n         'PO3 69 -1.695287 -5.760008 4.798353  ',\n         'POz 69 0.620948 -5.819869 5.205846   ',\n         'PO4 69 2.894593 -5.904253 4.664386   ',\n         'PO6 69 4.777563 -6.086181 3.030508   ',\n         'PO8 69 6.204419 -5.931612 1.100191   ',\n         'CB1 69 -2.746747 -7.905792 0.181570  ',\n         'O1 69 -2.994389 -7.100698 1.760379   ',\n         'Oz 69 0.396758 -7.928689 2.063318    ',\n         'O2 69 3.565017 -7.643452 1.469474    ',\n         'CB2 69 3.304482 -8.550863 -0.418131  ',\n         'VEO 69 -4.985376 9.402884 -3.997588  ',\n         'HEO 69 -6.256028 5.891592 -6.051630  ',\n         'EKG 69 -6.121837 -3.762735 -10.496158',\n         'EMG 69 -5.970550 -4.868990 -11.655138'], dtype='&lt;U37'), array([], shape=(0, 0), dtype=uint8), array([], shape=(0, 0), dtype=uint8), array(['+X'], dtype='&lt;U2'), array([], shape=(0, 0), dtype=uint8), array([], shape=(0, 0), dtype=uint8), array([], shape=(0, 0), dtype=uint8))]]\n\n\nIt is a bit more complicated. This is a numpy array of numpy arrays of strings.\n\n# get the channel names\nchan_info = chan_info[0,0][1]\nvar_inspector(chan_info)\n\nType: &lt;class 'numpy.ndarray'&gt;\n['Fp1 69 -2.854012 12.903291 -1.520481 '\n 'Fpz 69 0.583218 13.494058 -1.592608  '\n 'Fp2 69 3.935400 12.477923 -2.102277  '\n 'AF3 69 -3.023524 12.428141 1.361401  '\n 'AF4 69 4.304501 11.727693 0.427403   '\n 'F7 69 -6.639328 8.170529 -1.526560   '\n 'F5 69 -5.876012 9.542894 0.800148    '\n 'F3 69 -4.229360 10.265764 2.418731   '\n 'F1 69 -1.882614 11.018632 4.587177   '\n 'Fz 69 0.596303 10.217070 4.293318    '\n 'F2 69 3.996278 10.694478 3.944555    '\n 'F4 69 5.631967 9.848669 2.084015     '\n 'F6 69 6.969560 8.879675 -0.060155    '\n 'F8 69 7.878357 7.523402 -2.148623    '\n 'FT7 69 -7.918441 5.400486 -0.685157  '\n 'FC5 69 -6.848348 6.444450 1.872787   '\n 'FC3 69 -5.007377 6.820720 4.378383   '\n 'FC1 69 -2.332303 7.275606 6.020996   '\n 'FCz 69 0.892164 7.275017 6.676030    '\n 'FC2 69 3.868832 7.218509 5.929260    '\n 'FC4 69 6.393416 6.765655 4.161855    '\n 'FC6 69 8.110071 5.963853 1.523854    '\n 'FT8 69 8.880818 4.564967 -1.282408   '\n 'T7 69 -8.341170 2.798480 0.226452    '\n 'C5 69 -7.438693 3.373324 3.054235    '\n 'C3 69 -5.931186 3.709710 6.488027    '\n 'C1 69 -2.569554 3.746717 7.996653    '\n 'Cz 69 1.032521 3.911274 8.476038     '\n 'C2 69 4.350667 3.569207 7.594067     '\n 'C4 69 6.801077 3.610167 5.548478     '\n 'C6 69 9.023433 2.739705 2.800857     '\n 'T8 69 8.986069 1.829011 -0.226452    '\n 'M1 69 -7.644597 -0.549069 -8.007587  '\n 'TP7 69 -8.254624 -0.373276 1.011124  '\n 'CP5 69 -7.370408 0.278624 4.028963   '\n 'CP3 69 -5.316664 0.049608 6.245472   '\n 'CP1 69 -2.702063 0.250874 8.319883   '\n 'CPz 69 0.627199 -0.010582 8.411424   '\n 'CP2 69 4.188373 -0.366793 7.784681   '\n 'CP4 69 6.730657 -0.589212 6.652807   '\n 'CP6 69 8.320402 -0.371316 3.731920   '\n 'TP8 69 8.799763 -0.721895 0.744581   '\n 'M2 69 7.864402 -0.341263 -8.226876   '\n 'P7 69 -7.147690 -2.857133 1.425296   '\n 'P5 69 -5.896095 -3.084760 3.659961   '\n 'P3 69 -3.990392 -3.311368 5.491853   '\n 'P1 69 -1.790996 -3.225753 6.718182   '\n 'Pz 69 0.559499 -2.994738 7.218447    '\n 'P2 69 2.982983 -3.442504 6.627176    '\n 'P4 69 5.367558 -3.552835 5.125538    '\n 'P6 69 6.927846 -3.798552 3.392559    '\n 'P8 69 7.864575 -3.658688 1.023157    '\n 'PO7 69 -5.260816 -5.210424 1.680544  '\n 'PO5 69 -3.805657 -5.398441 3.520525  '\n 'PO3 69 -1.695287 -5.760008 4.798353  '\n 'POz 69 0.620948 -5.819869 5.205846   '\n 'PO4 69 2.894593 -5.904253 4.664386   '\n 'PO6 69 4.777563 -6.086181 3.030508   '\n 'PO8 69 6.204419 -5.931612 1.100191   '\n 'CB1 69 -2.746747 -7.905792 0.181570  '\n 'O1 69 -2.994389 -7.100698 1.760379   '\n 'Oz 69 0.396758 -7.928689 2.063318    '\n 'O2 69 3.565017 -7.643452 1.469474    '\n 'CB2 69 3.304482 -8.550863 -0.418131  '\n 'VEO 69 -4.985376 9.402884 -3.997588  '\n 'HEO 69 -6.256028 5.891592 -6.051630  '\n 'EKG 69 -6.121837 -3.762735 -10.496158'\n 'EMG 69 -5.970550 -4.868990 -11.655138']\n\n\nEach channel is given as a string, with its name followed by a string of numbers, which represent the 3D position of each electrode on the scalp (X, Y, Z). We can break these out using the built in split method associated with the python string object (by the way, to see all the methods available for a string, you can pass the string variable to the dir method).\n\n# use split the strings in chan_names by white space\nchan_names = [name.split()[0] for name in chan_info]\nvar_inspector(chan_names)\nprint('Channel number: {}'.format(len(chan_names)))\n\nType: &lt;class 'list'&gt;\n['Fp1', 'Fpz', 'Fp2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'M1', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'M2', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POz', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'Oz', 'O2', 'CB2', 'VEO', 'HEO', 'EKG', 'EMG']\nChannel number: 68\n\n\nThese names should be familiar from our discussion of electrode placement. What about the locations of the channels?\n\n# get the channel positions on the scalp\nchan_locs = [name.split()[2:] for name in chan_info]\n# convert chan_locs to numeric data type in a numpy array\nchan_locs = np.array(chan_locs, dtype=float)\nvar_inspector(chan_locs)\n\nType: &lt;class 'numpy.ndarray'&gt;\n[[-2.8540120e+00  1.2903291e+01 -1.5204810e+00]\n [ 5.8321800e-01  1.3494058e+01 -1.5926080e+00]\n [ 3.9354000e+00  1.2477923e+01 -2.1022770e+00]\n [-3.0235240e+00  1.2428141e+01  1.3614010e+00]\n [ 4.3045010e+00  1.1727693e+01  4.2740300e-01]\n [-6.6393280e+00  8.1705290e+00 -1.5265600e+00]\n [-5.8760120e+00  9.5428940e+00  8.0014800e-01]\n [-4.2293600e+00  1.0265764e+01  2.4187310e+00]\n [-1.8826140e+00  1.1018632e+01  4.5871770e+00]\n [ 5.9630300e-01  1.0217070e+01  4.2933180e+00]\n [ 3.9962780e+00  1.0694478e+01  3.9445550e+00]\n [ 5.6319670e+00  9.8486690e+00  2.0840150e+00]\n [ 6.9695600e+00  8.8796750e+00 -6.0155000e-02]\n [ 7.8783570e+00  7.5234020e+00 -2.1486230e+00]\n [-7.9184410e+00  5.4004860e+00 -6.8515700e-01]\n [-6.8483480e+00  6.4444500e+00  1.8727870e+00]\n [-5.0073770e+00  6.8207200e+00  4.3783830e+00]\n [-2.3323030e+00  7.2756060e+00  6.0209960e+00]\n [ 8.9216400e-01  7.2750170e+00  6.6760300e+00]\n [ 3.8688320e+00  7.2185090e+00  5.9292600e+00]\n [ 6.3934160e+00  6.7656550e+00  4.1618550e+00]\n [ 8.1100710e+00  5.9638530e+00  1.5238540e+00]\n [ 8.8808180e+00  4.5649670e+00 -1.2824080e+00]\n [-8.3411700e+00  2.7984800e+00  2.2645200e-01]\n [-7.4386930e+00  3.3733240e+00  3.0542350e+00]\n [-5.9311860e+00  3.7097100e+00  6.4880270e+00]\n [-2.5695540e+00  3.7467170e+00  7.9966530e+00]\n [ 1.0325210e+00  3.9112740e+00  8.4760380e+00]\n [ 4.3506670e+00  3.5692070e+00  7.5940670e+00]\n [ 6.8010770e+00  3.6101670e+00  5.5484780e+00]\n [ 9.0234330e+00  2.7397050e+00  2.8008570e+00]\n [ 8.9860690e+00  1.8290110e+00 -2.2645200e-01]\n [-7.6445970e+00 -5.4906900e-01 -8.0075870e+00]\n [-8.2546240e+00 -3.7327600e-01  1.0111240e+00]\n [-7.3704080e+00  2.7862400e-01  4.0289630e+00]\n [-5.3166640e+00  4.9608000e-02  6.2454720e+00]\n [-2.7020630e+00  2.5087400e-01  8.3198830e+00]\n [ 6.2719900e-01 -1.0582000e-02  8.4114240e+00]\n [ 4.1883730e+00 -3.6679300e-01  7.7846810e+00]\n [ 6.7306570e+00 -5.8921200e-01  6.6528070e+00]\n [ 8.3204020e+00 -3.7131600e-01  3.7319200e+00]\n [ 8.7997630e+00 -7.2189500e-01  7.4458100e-01]\n [ 7.8644020e+00 -3.4126300e-01 -8.2268760e+00]\n [-7.1476900e+00 -2.8571330e+00  1.4252960e+00]\n [-5.8960950e+00 -3.0847600e+00  3.6599610e+00]\n [-3.9903920e+00 -3.3113680e+00  5.4918530e+00]\n [-1.7909960e+00 -3.2257530e+00  6.7181820e+00]\n [ 5.5949900e-01 -2.9947380e+00  7.2184470e+00]\n [ 2.9829830e+00 -3.4425040e+00  6.6271760e+00]\n [ 5.3675580e+00 -3.5528350e+00  5.1255380e+00]\n [ 6.9278460e+00 -3.7985520e+00  3.3925590e+00]\n [ 7.8645750e+00 -3.6586880e+00  1.0231570e+00]\n [-5.2608160e+00 -5.2104240e+00  1.6805440e+00]\n [-3.8056570e+00 -5.3984410e+00  3.5205250e+00]\n [-1.6952870e+00 -5.7600080e+00  4.7983530e+00]\n [ 6.2094800e-01 -5.8198690e+00  5.2058460e+00]\n [ 2.8945930e+00 -5.9042530e+00  4.6643860e+00]\n [ 4.7775630e+00 -6.0861810e+00  3.0305080e+00]\n [ 6.2044190e+00 -5.9316120e+00  1.1001910e+00]\n [-2.7467470e+00 -7.9057920e+00  1.8157000e-01]\n [-2.9943890e+00 -7.1006980e+00  1.7603790e+00]\n [ 3.9675800e-01 -7.9286890e+00  2.0633180e+00]\n [ 3.5650170e+00 -7.6434520e+00  1.4694740e+00]\n [ 3.3044820e+00 -8.5508630e+00 -4.1813100e-01]\n [-4.9853760e+00  9.4028840e+00 -3.9975880e+00]\n [-6.2560280e+00  5.8915920e+00 -6.0516300e+00]\n [-6.1218370e+00 -3.7627350e+00 -1.0496158e+01]\n [-5.9705500e+00 -4.8689900e+00 -1.1655138e+01]]\n\n\nSince these were strings, we had to convert them to a numeric data type, especially if we want to eventually use them for plotting.\n\ndef plot_scalp(chan_locs, chan_names, subj):\n    fig, ax = plt.subplots()\n    ax.scatter(chan_locs[:,0], chan_locs[:,1])\n    for ind, name in enumerate(chan_names):\n        ax.text(chan_locs[ind,0], chan_locs[ind,1], name)\n    ax.set_xlabel('Medial-lateral')\n    ax.set_ylabel('Anterior-posterior')\n    ax.set_title('Channel locations for {}'.format(subj))\n    ax.set_aspect('equal')\n\nplot_scalp(chan_locs, chan_names, subj)\n\n\n\n\n\n\n\n\nAnother useful piece of info in the eeg data is ‘ref’, which tells us the position of the reference electrode.\n\nref_elec = eeg['ref']\nvar_inspector(ref_elec)\n\nType: &lt;class 'numpy.ndarray'&gt;\n['between_Cz_and_CPz']\n\n\nThis is another numpy array with a string inside. Let’s extract the channels the reference was between, and then use their coordinates to plot where the reference was.\n\n# split the reference string at underscores and return the channel names\nnear_ref_names = [ref for ref in ref_elec[0].split('_') if ref in chan_names]\n\nprint('Reference channel was near: {}'.format(near_ref_names))\n\n\n# get the coordinates of the reference electrodes\nnear_ref_coords = [chan_locs[chan_names.index(ref)] for ref in near_ref_names]\n# print near ref coords with only 2 decimal places\nprint('Coordinates of electrodes near reference: {}'.format(np.round(near_ref_coords,2)))   \n\n# get the average of the coordinates\nref_coord = np.mean(near_ref_coords, axis=0)\nprint('Coordinates of reference: {}'.format(np.round(ref_coord,2)))\n\n# plot the reference electrode\nplot_scalp(chan_locs, chan_names, subj)\nplt.scatter(ref_coord[0], ref_coord[1], c='r', s=100)\nplt.text(ref_coord[0], ref_coord[1], 'REF')\n\nReference channel was near: ['Cz', 'CPz']\nCoordinates of electrodes near reference: [[ 1.03  3.91  8.48]\n [ 0.63 -0.01  8.41]]\nCoordinates of reference: [0.83 1.95 8.44]\n\n\nText(0.82986, 1.9503460000000001, 'REF')\n\n\n\n\n\n\n\n\n\nWith all this out of the way, let’s inspect the EEG data itself.\n\neeg_data = eeg['data']\nvar_inspector(eeg_data)\n\nType: &lt;class 'numpy.ndarray'&gt;\n[[-17487.854   -17494.352   -17504.037   ... -17945.916   -17952.68\n  -17949.879  ]\n [-22107.213   -22104.95    -22108.615   ... -22745.281   -22717.625\n  -22711.635  ]\n [ -5888.045    -5887.7466   -5885.6606  ...  -5802.661    -5804.211\n   -5797.6245 ]\n ...\n [ -1053.0055   -1051.8431   -1050.8597  ...   -422.0903    -421.13663\n    -418.2458 ]\n [     0.           0.           0.      ...      0.           0.\n       0.     ]\n [     0.           0.           0.      ...      0.           0.\n       0.     ]]\n\n\nAnother numpy array, but this one contains a lot of numeric data. We can get the number of channels and time points that were recorded.\n\nnchan = eeg_data.shape[0]\nnsamp = eeg_data.shape[1]\nprint('Number of samples: {}'.format(nsamp))\nprint('Duration of recording: {} seconds'.format(nsamp/srate)) # dividing number of samples by sampling rate gives the duration of the recording\nprint('Number of channels: {}'.format(nchan))\n\nNumber of samples: 132480\nDuration of recording: 264.96 seconds\nNumber of channels: 66\n\n\nUh oh, the number of channels, 66, does not match the number given in chaninfo, 68. Perhaps we can find the correct values in the file ‘*_channels.tsv’.\n\n# load the a *_channels.tsv file\nsubj = 'sub-AB58'\nchan_dir = ['.', 'data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_channels.tsv'.format(subj)]\nchan_file = op.join(*chan_dir) # use * to unpack the list\nchannels = pd.read_csv(chan_file, sep='\\t')\nprint(channels)\n\n            name   type units\n0            AF3    EEG    uV\n1            AF4    EEG    uV\n2             F7    EEG    uV\n3             F5    EEG    uV\n4             F3    EEG    uV\n..           ...    ...   ...\n61           VEO   VEOG    uV\n62           HEO   HEOG    uV\n63           EKG    ECG    uV\n64  R-Dia-X-(mm)  PUPIL    mm\n65  R-Dia-Y-(mm)  PUPIL    mm\n\n[66 rows x 3 columns]\n\n\nNotice how the number of channels listed in the .tsv file is the same as those in our EEG data. This is probably the correct file for figuring out where each channel was recorded.\nThe .tsv file type stands for tab separated values. It is a table of data stored in a text file, where each row is a different line, and each column is separated by a tab. Often the first line of text gives the names for each column, but this is not necessary. It looks like this if you open in a text editor:\n\n\n\nTSV file viewed in a text editor\n\n\nIt is difficult to compare the list of recording sites in chaninfo and those listed in the channels .tsv file by eye, but we can write some code to solve that problem.\n\n# pull the channel names from the channels tsv file\nchan_names_tsv = channels['name'].tolist()\n\n# identify the channels in chan_names that are not in chan_names_tsv\nmissing_chan = [chan for chan in chan_names if chan not in chan_names_tsv]\nprint('The missing channels are: {}'.format(', '.join(missing_chan)))\n\nThe missing channels are: Fp1, Fpz, Fp2, EMG\n\n\nWe should set chan_names to the list in the channels .tsv file and match those with the electrode locations from the eeg .set file.\n\nchan_info = pd.DataFrame({'name': chan_names, \n                             'ML': chan_locs[:,0], \n                             'AP': chan_locs[:,1], \n                             'DV': chan_locs[:,2]})\nchannels = pd.merge(channels, chan_info, how='left', on='name')\nchannels\n\n\n\n\n\n\n\n\nname\ntype\nunits\nML\nAP\nDV\n\n\n\n\n0\nAF3\nEEG\nuV\n-3.023524\n12.428141\n1.361401\n\n\n1\nAF4\nEEG\nuV\n4.304501\n11.727693\n0.427403\n\n\n2\nF7\nEEG\nuV\n-6.639328\n8.170529\n-1.526560\n\n\n3\nF5\nEEG\nuV\n-5.876012\n9.542894\n0.800148\n\n\n4\nF3\nEEG\nuV\n-4.229360\n10.265764\n2.418731\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n61\nVEO\nVEOG\nuV\n-4.985376\n9.402884\n-3.997588\n\n\n62\nHEO\nHEOG\nuV\n-6.256028\n5.891592\n-6.051630\n\n\n63\nEKG\nECG\nuV\n-6.121837\n-3.762735\n-10.496158\n\n\n64\nR-Dia-X-(mm)\nPUPIL\nmm\nNaN\nNaN\nNaN\n\n\n65\nR-Dia-Y-(mm)\nPUPIL\nmm\nNaN\nNaN\nNaN\n\n\n\n\n66 rows × 6 columns\n\n\n\nNote that there are a few extra channels with names that are not part of the EEG site naming scheme given above. ‘VEO’ and ‘HEO’ are electrodes placed near the left eye and can detect eye movements (discussed below). ‘EKG’ is from an electrode place near the heart and gives the electrocardiogram, which captures the electrical activity associated with heart beats. Lastly, ‘R-Dia-X-(mm)’ and ‘R-Dia-Y-(mm)’ are measures of pupil diameter made by a video camera trained on the eye.\nAnd now we can replot the EEG electrodes, with the missing channels removed.\n\nchan_names = channels['name'].values\nchan_locs = channels[['ML', 'AP', 'DV']].values\nplot_scalp(chan_locs, chan_names, subj)\nplt.scatter(ref_coord[0], ref_coord[1], c='r', s=100)\nplt.text(ref_coord[0], ref_coord[1], 'REF')\n\nText(0.82986, 1.9503460000000001, 'REF')\n\n\nposx and posy should be finite values\nposx and posy should be finite values\nposx and posy should be finite values\nposx and posy should be finite values\nposx and posy should be finite values\nposx and posy should be finite values\n\n\n\n\n\n\n\n\n\nThe next thing to inspect is the EEG signal itself. The EEG signal is an array of voltages, EEG, sampled at each channel, ch and time point, t. A single sample can be written as EEG_{ch,t}. If we want the voltage across all channels at a specific time that would be the vector EEG_{t}. The entire time series of voltages from a single channel is expressed as EEG_{ch}. The term time series is important to keep in mind. Virtually all the data we will work with is time series data, meaning that it is the values generated by some process that is sampled at regular intervals. In the case of our present EEG signal, this would be the uV potentials recorded from an electrode at the sample rate (srate) of 500 Hz.\nTo begin exploring this data, lets plot the voltage detected on channel O1 (an electrode over the occiptal lobe on the left hemisphere) between 50 and 52 seconds. We will express this as:  EEG_{O1}[50\\leq t \\le 52] \nFirst, we identify the index for the row containing data from channel O1.\n\n# select channel to plot\nsel_chan = 'O1'\nchan_ind = channels.index[channels['name']==sel_chan]\n\nUsing the channels dataframe we generated above, we find the row where the ‘name’ column is equal to O1, and then return the index of that row. The indices in the channels dataframe range from 0 to 65, with each corresponding to a row in eeg_data.\nNext we need to find the time points that were sampled between 50 and 52 seconds into the recording.\n\n# set the times we wish to plot\nstart_t = 50\nend_t = 52\nstart_idx = (start_t*srate).astype(int)\nend_idx = (end_t*srate).astype(int)\n\nSince our recording starts at time point 0 seconds and was recorded with srate number of samples per second, we can find the indices of the start and stop times by multiplying those times in seconds by the srate (start_t*srate and end_t*srate). Since these will be used to index into an array, we ensure that they have integer values by forcing them to an integer datatype using the numpy astype function.\n\n# extract the eeg data we wish to plot\neeg_epoch = eeg_data[chan_ind, start_idx:end_idx].squeeze() # squeeze removes the extra dimension\n\nAn ‘epoch’ is a period of time, and when we pull out a contiguous set of values from a time series will refer to that as an epoch. When plotting that data, we need to pass both the eeg signal and their corresponding time points to the plotting function. These can either be absolute or relative. For absolute, the time values are taken with respect to the entire recording, so in our case they would start at 50 sec and end just before 52 sec. For relative time, the times at taken with respect to a specific event (usually the beginning of our epoch, but not always). In that case, they would start at 0 sec and end just before 2 sec. Calculating either is straightforward:\n\n# time on an absolute scale\nt_abs_epoch = start_t + np.arange(0, eeg_epoch.size)/srate\n\n# time on a relative scale\nt_rel_epoch = np.arange(0, eeg_epoch.size)/srate\n\nNow let’s plot on both time scales, just for show.\n\nfig, ax = plt.subplots(2,1,figsize=(10,4))\nax[0].plot(t_abs_epoch, eeg_epoch) # we transpose to make the data the right shape for plotting\nax[0].set_xlabel('Absolute time (s)')\nax[0].set_ylabel('Voltage (uV)')\nax[1].plot(t_rel_epoch, eeg_epoch)\nax[1].set_xlabel('Relative time (s)')\nax[1].set_ylabel('Voltage (uV)')\nfig.suptitle('Channel {} for {}'.format(sel_chan, subj))\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThis is the electrical activity recorded on the scalp over the left occipital cortex, an area of the brain involved in visual processing. The signal has a somewhat noisy appearance, though it is unclear what is really ‘noise.’\nBefore we move on, let’s create a class for loading the EEG data from a file and giving us access to it. This will encompass all the operations we performed above, making it easier to load data from other subjects with the same format.\n\nclass EEG:\n    def __init__(self, eeg_file, chan_file):\n        \"\"\"\n        A class for loading and plotting EEG data\n\n        Parameters\n        ----------\n        eeg_file : str, path to the .set file\n        chan_file : str, path to the _channels.tsv file\n        \"\"\"\n\n        # load the eeg data\n        self._eeg = sio.loadmat(eeg_file, appendmat=False)\n        data = self._eeg['data']\n        self.srate = self._eeg['srate'][0,0]\n        self.data = data\n        self.nchan = self.data.shape[0]\n        self.nsamp = self.data.shape[1]\n        self.dur = self.nsamp/self.srate\n\n        # load the channel info and integrate with locations\n        chan_info = self._eeg['chaninfo'][0,0][1]\n        chan_names = [name.split()[0] for name in chan_info]\n        chan_locs = np.array([name.split()[2:] for name in chan_info], dtype=float)\n        chan_info = pd.DataFrame({'name': chan_names, \n                                    'ml': chan_locs[:,0], \n                                    'ap': chan_locs[:,1], \n                                    'dv': chan_locs[:,2]})\n        chans = pd.read_csv(chan_file, sep='\\t')\n        chans = pd.merge(chans, chan_info, how='left', on='name')\n        chans.index.name = 'idx'\n        self.chans = chans\n\n        # get reference electrode position\n        ref_elec = self._eeg['ref'][0]\n        near_ref_names = [ref for ref in ref_elec.split('_') if ref in chans['name'].tolist()]\n        near_ref_chans = chans[chans['name'].isin(near_ref_names)]\n        near_ref_coords = near_ref_chans[['ml', 'ap', 'dv']].to_numpy()\n        self.ref_coord = np.mean(near_ref_coords, axis=0)\n\n    def get_data(self, chans=None, start_t=0, dur_t=None, scale='absolute'):\n        \"\"\"\n        Extract EEG data from the EEG object\n\n        Parameters\n        ----------\n        chans : list of str, the channels to extract\n        start_t : numeric array, the start times in seconds\n        dur_t : float, the duration in seconds\n        scale : str, 'absolute' or 'relative'\n\n        Returns\n        -------\n        data_epochs : 3d array, the eeg data\n        tpts : 1d array, the time vector\n        chans : list of str, the channels extracted\n        \"\"\"\n\n        # ensure proper formatting of inputs\n        if not chans:\n            chans = self.chans['name']\n        elif chans == 'eeg' or chans == ['eeg']: # only extract eeg channels\n            chans = self.chans[self.chans['type']=='EEG']['name'].values\n        elif type(chans) == str:\n            chans = [chans]\n        \n        if not dur_t:\n            dur_t = self.dur-start_t\n        start_t = np.array(start_t)\n        start_t = start_t.ravel()\n        epoch_num = start_t.size\n\n        # convert times to indices\n        start_idxs = (start_t*self.srate).astype(int)\n        dur_idx = (dur_t*self.srate).astype(int)\n        end_idxs = start_idxs + dur_idx\n\n        # get the channel indices\n        chan_idxs = [np.where(self.chans['name']==sel_ch)[0][0] for sel_ch in chans]\n\n        # extract the eeg data, one epoch at a time\n        data_epochs = np.zeros((dur_idx, len(chan_idxs), epoch_num)) # this also ensures changes to the data don't affect the original\n        for i in range(epoch_num):\n            data_epochs[:,:,i] = self.data[chan_idxs, start_idxs[i]:end_idxs[i]].T\n\n        # get the time vector\n        if scale == 'absolute':\n            tpts = start_t + np.arange(0, dur_idx)/self.srate\n        elif scale == 'relative':\n            tpts = np.arange(0, dur_idx)/self.srate\n        \n        return data_epochs, tpts, chans\n\n    def plot_scalp(self, ax=None, colors='b'):\n        \"\"\"\n        Plot the channel locations on the scalp\n        \"\"\"\n\n        # ensure proper formatting of inputs\n        if not ax:\n            fig, ax = plt.subplots()\n        \n        chans = self.chans[self.chans['type']=='EEG']\n\n        # plot the channel locations\n        ax.scatter(chans['ml'], chans['ap'], c=colors)\n        for ind, name in enumerate(chans['name']):\n            ax.text(chans['ml'][ind], chans['ap'][ind], name)\n        \n        # plot the reference electrode\n        ax.scatter(self.ref_coord[0], self.ref_coord[1], c='k', s=100)\n        \n        ax.set_xlabel('Medial-lateral')\n        ax.set_ylabel('Anterior-posterior')\n        ax.set_title('Channel locations for {}'.format(subj))\n        ax.set_aspect('equal')\n\n        return ax\n\nNow let’s give this class a try. It should shorten all the code we did above.\n\n# get file paths\nsubj = 'sub-AB58'\nchan_dir = ['.', 'data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_channels.tsv'.format(subj)]\nchan_file = op.join(*chan_dir) # use * to unpack the list\ndata_dir = ['.', 'data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_eeg.set'.format(subj)]\neeg_file = op.join(*data_dir) # use * to unpack the list\n\n# create an EEG object\neeg = EEG(eeg_file, chan_file)\n\n# plot the scalp\neeg_chan_num = np.sum(eeg.chans['type']=='EEG')\nscalp_ax = eeg.plot_scalp(colors=range(eeg_chan_num))\nplt.colorbar(scalp_ax.collections[0], ax=scalp_ax, label='Channel number')\n\n# get the 2 channels of eeg data at 2 time points\neeg_epochs, tpts, chans = eeg.get_data(chans=['O1', 'O2'], start_t=[50, 100], dur_t=2, scale='relative')\n\n# plot the eeg data\nfig, ax = plt.subplots(2,1,figsize=(10,4))\nax[0].plot(tpts, eeg_epochs[:,0,:])\nax[0].set_xlabel('Relative time (s)')\nax[0].set_ylabel('Voltage (uV)')\nax[0].set_title('Channel {} from {}'.format(chans[0], subj))\nax[1].plot(tpts, eeg_epochs[:,1,:])\nax[1].set_xlabel('Relative time (s)')\nax[1].set_ylabel('Voltage (uV)')\nax[1].set_title('Channel {} from {}'.format(chans[1], subj))\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime series statistics\nNow that we can easily load in EEG data, let’s explore some of the statistics we use to summarize it. Since the EEG signal is a series of voltages sampled at evenly spaced time intervals, we will make use of the statistical measures used for time series data more generally. To start, we will reload the EEG epoch we started with using our new EEG data class.\n\neeg_data, eeg_t, eeg_chan = eeg.get_data(chans=['O1'], start_t=50, dur_t=2, scale='absolute')\neeg_data = eeg_data.ravel() # ravel flattens the array\n\n\nMean\nTime series data is often considered to be the repeated sampling of an ongoing process. The values given off by that process will have a distribution, and if that distribution is normal then the two measures that characterize that distribution are its mean and variance.\n Mean = E[X] = \\frac{1}{n}\\sum_{t=1}^{n}X_{t} \nTo translate, the mean can be written as a function E[X], which stands for the Expected value of X. The expected value is equal to the sum (\\sum) from the first (t=1) to the last (n) value of each individual (t) time point of X, expressed as X_{t}, divided by the number of time points (n). The corresponding explicit code for this is:\n\n# A method that explicitly expresses how to calculate the mean in base python\ndef mean(x):\n    sum = 0\n    n = len(x)\n    for t in range(0,n):\n        sum += x[t]\n\n    return sum / n\n\nAssuming our data has a normal distribution, the mean tells us which value is most likely to be expressed at any given moment. In the case of the EEG epoch we have above, this value is:\n\neeg_data_mean = mean(eeg_data)\nprint('Mean of the EEG epoch: {:.2f} uV'.format(eeg_data_mean))\n\nMean of the EEG epoch: -14701.82 uV\n\n\n\n\nVariance\nBesides them mean, we also want a measure of the variability in the voltage around the mean. A measure frequently used is the variance. It is calculated as:  Variance = Var[X] = E[(X-E[X])^2] = \\frac{1}{n}\\sum_{t=1}^{n}(X_{t}-E[X])^2 \nThe variance is the mean of the squared difference between the values in our time series and the mean value. We can cast this explicitly in code also:\n\n# A method that explicitly expresses how to calculate variance in base python\ndef variance(x):\n    sum = 0\n    n = len(x)\n    e_x = mean(x)\n    for t in range(0,n):\n        sum += (x[t] - e_x)**2\n\n    return sum / n\n\neeg_data_var = variance(eeg_data)\nprint('Variance of the EEG epoch: {:.2f} uV^2'.format(eeg_data_var))\n\nVariance of the EEG epoch: 71.67 uV^2\n\n\n\n\nStandard deviation\nNotice that the variance is in units of uV2. Since we analyze EEG in units of uV, it is difficult for us to directly interpret the variance. Instead, we can take its square root, giving us a measure the of the variability in the EEG signal that is in units of uV. The square root of the variance is the familiar standard deviation. If we measure the standard deviation of our EEG epoch, we get:\n\n# A method that explicitly expresses how to calculate standard deviation in base python\ndef sd(x):\n    return variance(x)**(1/2)\n\neeg_data_sd = sd(eeg_data)\nprint('Standard deviation of the EEG epoch: {:.2f} uV'.format(eeg_data_sd))\n\nStandard deviation of the EEG epoch: 8.47 uV\n\n\n\n\nThe normal distribution\nIf a signal is normally distributed, then all we need to approximate its distribution is its expected value and variance. The normal distribution is captured by the curve:  p(X_{i}) = \\frac{1}{\\sqrt{Var[X]2\\pi}}exp\\left[{-\\frac{(X_{i}-E[X])^{2}}{2Var[X]}}\\right]\nTranslating this into code we get:\n\n# A method that explicitly expresses how to calculate the probability of a \n# data point being observed given a normal distribution\ndef normal(x, x_i):\n    e_x = mean(x)\n    v_x = variance(x)\n    scale_factor = (1/((2*np.pi*v_x)**(1/2))) # sets the height of the curve\n    shape_factor = np.exp(-((x_i - e_x)**2)/(2*v_x)) # sets the width of the curve\n    return scale_factor * shape_factor\n\n\nsamp_vals = np.linspace(np.min(eeg_data), np.max(eeg_data), 50)\nnorm_vals = [normal(eeg_data, samp) for samp in samp_vals]\n\nfig, ax = plt.subplots()\nax.plot(samp_vals, norm_vals, c='r')\nax.set_xlabel('Voltage (uV)')\nax.set_ylabel('Probability density')\nax.set_title('Predicted distribution of voltages for {} from {}'.format(sel_chan, subj))\nfig.tight_layout()\n\n\n\n\n\n\n\n\nHow does this predicted distribution compare with the actual distribution of values? To answer this question, we need to plot the probability of a particular value occurring in our EEG epoch. We could try plotting the values as hatch marks at the bottom of the graph, and see if their density is highest near the center of the normal curve.\n\n# plot the normal distribution, and short vertical lines on the x-axis where each data point from eeg_epoch is\nfig, ax = plt.subplots()\nax.plot(samp_vals, norm_vals, c='r')\nax.set_xlabel('Voltage (uV)')\nax.set_ylabel('Probability density')\nax.set_title('Predicted distribution of voltages for {} from {}'.format(sel_chan, subj))\nax.vlines(eeg_data, 0, 0.005, color='k')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nIn this graph, each voltage sample is represented as a single vertical line (which we will refer to as hatch marks), with its position on the voltage axis determined by its value. It appears that they are clustered near the center of the normal distribution. Indeed, the density of hatch marks there is so high that they fuse together into a single black mass. It would be better if we could estimate the density of hatch marks along the voltage axis. To do this, we will calculate a histogram, which divides the voltage axis into a series of bins and measures the number of data points in each bin. These counts are then scaled by the number of data points, n, and the length of the bin to give us probability density. Here is how we can code this:\n\n# A method that explicitly expresses how to calculate a probability density histogram\ndef histogram(x, bin_num):\n    n = len(x)\n    x_min = min(x)\n    x_max = max(x)\n    bin_width = (x_max - x_min)/bin_num\n    bin_edges = x_min + bin_width * range(bin_num+1)\n    bin_centers = np.zeros(bin_num)\n    bin_heights = np.zeros(bin_num)\n    scale_factor = 1/(n*bin_width)\n    for i in range(0,bin_num):\n        bin_count = len(x[(x &gt;= bin_edges[i]) & (x &lt; bin_edges[i+1])])\n        bin_heights[i] = (scale_factor * bin_count)\n        bin_centers[i] = (bin_edges[i] + bin_width/2)\n    return bin_centers, bin_heights\n\n\n# calculate the empirical distribution of the EEG data\nbin_centers, bin_heights = histogram(eeg_data, 20)\n\n# plot the empirical distribution\nbin_width = bin_centers[1]-bin_centers[0]\nfig, ax = plt.subplots()\nax.bar(bin_centers, bin_heights, width=bin_width, align='center')\nax.plot(samp_vals, norm_vals, c='r')\nax.vlines(eeg_data, 0, 0.005, color='k')\nax.set_xlabel('Voltage (uV)')\nax.set_ylabel('Probability density')\nax.set_title('Histogram of voltages for {} from {}'.format(sel_chan, subj))\nfig.tight_layout()\n\n\n\n\n\n\n\n\nNow we have a much better sense of the distribution of the EEG values. We can see that they generally follow the contours of the normal distribution, but not perfectly. There is an overabundance of low values, and fewer values around the mean. Looking back at the plot of our EEG snippet, it appears that the signal wanders, with periods of low values and high values. This might explain the deviation from a normal distribution when we have only sampled for a short length of time, 2 seconds.\nAdjacent samples in the time series tend to have similar values. This is unlike most other data you may have worked with, such as tabular data, where each data point is sampled independently from the others. In the case of time series data, adjacent samples will be necessarily related to each other since they were obtained near the same time. Is there a way to quantify this?\n\n\nAutocovariance\nWhat we want is a measure of the similarity between samples in our time series as a function of how apart they are in time. If adjacent samples tend to both be above or below the mean, we want our measure to be high, and we want to it to be near zero when they are unrelated. The autocovariance function behaves this way.\n ACov[\\tau] = E[(X_{t}-E[X])(X_{t+\\tau}-E[X])]\nPut in words, we create a new time series, (X_{t}-E[X])(X_{t+\\tau}-E[X]), by multiplying a mean subtracted version of our time series by itself at some fixed time lag (\\tau), and then measure the mean value of this new time series. We do this for a range of lags. In code, this looks like:\n\n# A method that explicitly expresses how to calculate the autocovariance of a signal\ndef autocovariance(x, lag):\n    n = len(x)\n    e_x = mean(x)\n    sum = 0\n    for t in range(0,n-lag):\n        sum += (x[t] - e_x)*(x[t+lag] - e_x)\n    return sum / (n-lag)\n\n\n# Calculate autocovariance for lags 0 to 20\nlags = range(0,50)\nautocov = [autocovariance(eeg_data, lag) for lag in lags]\n\n# Since our sample rate is 500 Hz, we can convert the lags to time in seconds\nlags_sec = np.array(lags)/srate\n\n# Plot autocovariance as stem plot\nfig, ax = plt.subplots()\nax.stem(lags_sec, autocov)\nax.set_xlabel('Lag (sec)')\nax.set_ylabel('Autocovariance (uV$^2$)')\nax.set_title('Autocovariance of voltages for {} from {}'.format(sel_chan, subj))\nfig.tight_layout()\n\n\n\n\n\n\n\n\nA few things stand out here. First, when the lag is 0 sec, the value of the autocovariance is the same as the variance we estimated earlier (71.7 uV2). This is because for the zero lag case, the equation for autocovariance and variance are exactly the same. Second, as we increase the lag the autocovariance decreases, indicating that samples farther apart in time are less similar to each other. Lastly, you can see some periodic ripples riding on the autocovariance at 20, 40, 60, and 80 ms. It is no coincidence that these are evenly spaces 20 ms apart. They are due to contamination by power line noise, which is at a frequency of 50 Hz in Europe, where this data was obtained. We will discuss this further in the next section.\n\n\n\n\n\n\nUsing built-in Numpy functions\n\n\n\nNote that the Numpy package has methods for calculating the mean (np.mean), variance (np.var), and standard deviation (np.std). These offer far greater control over how these operations are carried out with multidimensional arrays, and so you should use them. As an example, let’s rewrite the autocovariance function using Numpy methods, since Numpy does not have one built-in.\n\n# A method that calculates the autocovariance of a signal using numpy methods\ndef autocovariance(x, lag):\n    n = len(x)\n    e_x = np.mean(x)\n    return np.sum((x[0:n-lag] - e_x)*(x[lag:n] - e_x)) / (n-lag)\n\n\n\n\n\n\nArtifacts and noise\nEnvironmental electrical noise that is homogeneous across the head will be eliminated by the proper referencing described above. However, there are other sources of electrical interference that are not homogeneous across this head. This is especially true for those arising from the head itself. Recall from our discussion of extracellular potentials that voltage falls off with distance. This means that electrodes close to a location on the head producing non-neural electrical signals will record very different signals.\nThese sources are sometimes referred to as noise, and other times as artifacts. Common sources are skin potentials, eye movements and blinks (electrooculogram), muscle movements (electromyogram), and power line interference. Thus, we can conceive of the EEG as a sum of neural and non-neural sources:\n EEG = Neural + EOG + ACPower + EMG + Scalp \nLet’s consider each of these non-neural sources, and how to eliminate them.\n\nSkin potentials\nThe skin is a conductive medium with its own voltage potential between its inside and outside. This voltage changes with the skin’s conductivity. Often this occurs slowly over time by the opening and closing of sweat glands and the hydration of the skin by the conductive gel underneath the EEG electrode. It can also change suddenly when the electrode is moved.\n\n\n\nskin anatomy\n\n\nTo see how much the skin potential can drift, take a look at a single EEG recording channel for the entire recording duration.\n\n# load entire O1 channel EEG recording\neeg_data_full, eeg_t_full, eeg_chan_full = eeg.get_data(chans=['O1'])\neeg_data_full = eeg_data_full.ravel()\n\n# plot full eeg recording\nfig, ax = plt.subplots()\nax.plot(eeg_t_full, eeg_data_full)\nax.set_xlabel('Time (s)')\nax.set_ylabel('Voltage (uV)')\nax.set_title('Channel {} from {}'.format(eeg_chan_full[0], subj))\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe level of the EEG varies greatly over the 4 minutes of the recording. It’s offset starts around -14,650 uV, and drifts down to -14,850 uV, spanning a range of 200 uV. This far exceeds the standard deviation of the signal, which we calculated over a short segment above at ~8.5 uV.\nThis is only on a single electrode. How heterogeneous are these offsets and drifts across electrodes?\n\n# load all eeg data\neeg_data_full, eeg_t_full, eeg_chan_full = eeg.get_data(chans='eeg')\n\n# get mean voltage from each channel\neeg_mean = np.mean(eeg_data_full, axis=0).ravel()\n\n# plot the mean voltage for each channel on the scalp\nfig, ax = plt.subplots()\nax.hist(eeg_mean)\nax.set_xlabel('Mean voltage (uV)')\nax.set_ylabel('Channel count')\nax.set_title('Mean voltage for each channel from {}'.format(subj))\nfig.tight_layout()\n\nprint('Minimum mean voltage: {:.2f} uV'.format(np.min(eeg_mean)))\nprint('Maximum mean voltage: {:.2f} uV'.format(np.max(eeg_mean)))\n\nMinimum mean voltage: -22535.51 uV\nMaximum mean voltage: 8960.47 uV\n\n\n\n\n\n\n\n\n\nThere is a wide range of mean voltages across the channels, from -22.5 mV to 8.9 mV. How do they change with time? We saw that site O1 drifted towards more negative voltages, but was that also the case for the other channels? Let’s look at a few channels spread across the scalp.\n\ndrift_chans = ['F1', 'C1', 'P1', 'O1']\n\n# get the eeg data for the drift channels\neeg_data_drift, eeg_t_drift, eeg_chan_drift = eeg.get_data(chans=drift_chans)\n\n# plot the eeg data for the drift channels\nfig, ax = plt.subplots(4,1,figsize=(5,7))\nfor i in range(4):\n    ax[i].plot(eeg_t_drift, eeg_data_drift[:,i,:])\n    ax[i].set_title('Channel {} from {}'.format(eeg_chan_drift[i], subj))\nfig.supxlabel('Time (s)')\nfig.supylabel('Voltage (uV)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nNot all channels show the same slow drift in their potential. F1 drifted up, while C1, P1, and O1 drift down. C1 drifts down as a straight line, while P1 and O1 take wobbly paths. We want to eliminate these offsets. One way we could do this is to get a running average of the voltage and then subtract it from the original. This would eliminate the large offset and its slow drift, leaving only the fast fluctuations.\nTo calculate a running average, we will calculate the mean value of the time series over a fixed interval. We do this for each time point in the series, giving us a new time series with the same length as the original, but whose values are the local average. Mathematically, we can represent this as:\n Y_{t} = \\frac{1}{2w+1}\\sum_{\\tau=-w}^{w}{X_{t+\\tau}} \\tag{1}\nThe size of the window we use is specified by w. The larger w we choose, the more data points we average together and moving average will reflect slower trends in the time series. At the limits, if w is equal to 0, then we get the exact same time series back. This is useless for our purposes, because if we subtract it from the original time series we are left with nothing. On the other hand, as we increase w the values in our moving average time series approach its mean value. This would capture the offset, but it would miss the trend. Thus, we want to set w, which we will now refer to as our window size, to a value that is small enough to capture the drift in our time series, but not so small that it just recapitulates the time series and eliminates the signals we care about.\nTo explore these issues, we can code a moving average function with a parameter that controls window size.\n\n# A method that calculates a moving average\ndef moving_average(x, w):\n    n = len(x)\n    y = np.zeros(n)\n    for t in range(0,n):\n        if t &lt; w: # if we are at the beginning of the signal\n            y[t] = np.mean(x[0:t+w])\n        elif t &gt; n-w: # if we are at the end of the signal\n            y[t] = np.mean(x[t-w:n])\n        else: # if we are in the middle of the signal, same as equation 1\n            y[t] = np.mean(x[t-w:t+w])\n    return y\n\nBefore we apply this function to our data, a quick comment about how this code handles the beginning and end of the signal. Our window spans time points from -w to w, so if we are at the start of the signal then the samples from -w that would have gone into our average are not available, and likewise for the end of the signal with points on the w end. The approach taken here was to just take the mean of the points we did have. Another approach is to pad the edges of the signal, place a number w times at the beginning and end. If the number is 0, it is called zero padding. A problem with zero padding is that the mean will be distorted by those zeros early and late in the signal. Those distortions are referred to as edge effects. Alternative padding strategies can mitigate edge effects. For instance, instead of padding with zero you can use the value for the first or last sample, and pad with that. Or, you can reflect the signal at the edges.\n\n# Try out the moving average function with different windows\nw = [1, 125, 2000, 40000, 132000]\nfig, ax = plt.subplots(2,5,figsize=(12,5), sharey='row')\nfor i in range(5):\n    eeg_ma = moving_average(eeg_data_full[:,0], w[i])\n    eeg_resid = eeg_data_full[:,0].ravel() - eeg_ma\n    ax[0,i].plot(eeg_t_full, eeg_data_full[:,0],color='k')\n    ax[0,i].plot(eeg_t_full, eeg_ma, color='r')\n    ax[0,i].set_title('{} sec'.format(2*w[i]/eeg.srate))\n    ax[1,i].plot(eeg_t_full, eeg_resid)\nfig.supxlabel('Time (s)')\nfig.supylabel('Voltage (uV)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe top row shows the original signal in black, with the moving average overlaid on top in red. The bottom row of graphs with blue lines is the difference between the original EEG signal and the moving average, referred to as the residual. The most obvious change, which holds for all the window sizes, is that the voltage offset is removed. Our residual signals are now centered on 0 uV. Looking at the red and black lines, it is apparent that as we lengthen the duration of the window, the moving average line becomes smoother. For short durations, it follows the EEG signal closely. At our shortest window, it follows the signal so closely that the resulting residual is much smaller compared with the longer windows. At the other end of the spectrum, our longest window size fails to remove slow trends in the EEG, since you can see that its residual still has slow drift. The 0.5 second and 8 second windows seems to have equivalent effects. Seeing the difference between them will require looking at the data on a shorter time scale.\n\nw = [125, 2000]\nt_start_idx = 50*eeg.srate\nt_end_idx = 55*eeg.srate\nt_idxs = range(t_start_idx, t_end_idx)\nfig, ax = plt.subplots(2,2,figsize=(12,5), sharey='row')\nfor i in range(2):\n    eeg_ma = moving_average(eeg_data_full[:,0], w[i])\n    eeg_resid = eeg_data_full[:,0].ravel() - eeg_ma\n    ax[0,i].plot(eeg_t_full[t_idxs], eeg_data_full[t_idxs,0],color='k')\n    ax[0,i].plot(eeg_t_full[t_idxs], eeg_ma[t_idxs], color='r')\n    ax[0,i].set_title('{} sec'.format(2*w[i]/eeg.srate))\n    ax[1,i].plot(eeg_t_full[t_idxs], eeg_resid[t_idxs])\nfig.supxlabel('Time (s)')\nfig.supylabel('Voltage (uV)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe half-second window seems to follow the EEG a bit too closely, while the 8 second window is more even. Given all this, the 8 second window looks the best.\nWe can now create a function that takes the EEG object and returns a version of it with the baseline subtracted.\n\n# a function to remove baseline drift\ndef remove_baseline_drift(eeg, w=8):\n    # eeg: an EEG object\n    # w: window size in seconds\n    # returns: an EEG object with baseline drift removed\n\n    # convert window size to number of samples\n    w = int((w/2) * eeg.srate)\n\n    # create convolution kernel\n    kernel = np.ones((1, 2*w+1)) / (2*w+1)\n\n    # determine which channels are EEG\n    eeg_chans = eeg.chans['type'] == 'EEG'\n\n    # convolve kernel with EEG data using scipy.signal.convolve\n    baseline = ss.convolve(eeg.data[eeg_chans,:], kernel, mode='same')\n\n    # subtract baseline from EEG data\n    eeg.data[eeg_chans,:] = eeg.data[eeg_chans,:] - baseline\n\nThis function makes use of the convolve method found in the Scipy Signal package. The convolution operation is similar to the the moving average operation we created above. For the moving average, you can imagine that we are sliding a list of numbers (often called a kernel) that is as long as our window, and each has the value of 1/(2w+1), across the EEG time series. At each time point in the time series, we multiply the values in that time window with the corresponding values in the kernel, and add them together. This gives us the average surround each time point. With convolution, instead of having all the values in the kernel have the same value, they can take on different values. We will make use of this later.\n\n\n\nconvolution\n\n\nLet’s see how our baseline removal function performs.\n\n# run slow drift removal\nremove_baseline_drift(eeg)\n\nexample_chans = ['F1', 'C1', 'P1', 'O1']\n\n# get the eeg data for the drift channels\neeg_data_drift, eeg_t_drift, eeg_chan_drift = eeg.get_data(chans=example_chans)\n\n# plot the eeg data for the drift channels\nfig, ax = plt.subplots(4,1,figsize=(5,7))\nfor i in range(4):\n    ax[i].plot(eeg_t_drift, eeg_data_drift[:,i,:])\n    ax[i].set_title('Channel {} from {}'.format(eeg_chan_drift[i], subj))\nfig.supxlabel('Time (s)')\nfig.supylabel('Voltage (uV)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nUh oh, it appears that we have edge effects. At the beginning and end of the time series there are distortions. This is because the scipy convolve function zero-pads the time series. To mitigate this, we can change the padding. Instead of using zeros, we will just take the signal value at the edges.\n\n# a function to remove baseline drift\ndef remove_baseline_drift(eeg, w=8):\n    # eeg: an EEG object\n    # w: window size in seconds\n    # returns: an EEG object with baseline drift removed\n\n    # convert window size to number of samples\n    w = int((w/2) * eeg.srate)\n\n    # create convolution kernel\n    kernel = np.ones((1, 2*w+1)) / (2*w+1)\n\n    # determine which channels are EEG\n    eeg_chans = eeg.chans['type'] == 'EEG'\n\n    # pad data with edge values &lt;-- HERE IS A CHANGE\n    data_pad = np.pad(eeg.data[eeg_chans,:], ((0,0), (w,w)), 'edge')\n\n    # convolve kernel with EEG data using scipy.signal.convolve\n    # (mode='valid' to keep output the same size as the input after padding) &lt;-- HERE IS A CHANGE\n    baseline = ss.convolve(data_pad, kernel, mode='valid')\n\n    # subtract baseline from EEG data\n    eeg.data[eeg_chans,:] = eeg.data[eeg_chans,:] - baseline\n\nTo check if our fix worked, we will load a fresh version of the eeg object and try baseline removal again.\n\n# create an EEG object\neeg = EEG(eeg_file, chan_file)\n\n# remove baseline drift\nremove_baseline_drift(eeg)\n\n# plot EEG with baseline drift removed\ndrift_chans = ['F1', 'C1', 'P1', 'O1']\n\n# get the eeg data for the drift channels\neeg_data_drift, eeg_t_drift, eeg_chan_drift = eeg.get_data(chans=example_chans)\n\n# plot the eeg data for the drift channels\nfig, ax = plt.subplots(4,1,figsize=(5,7))\nfor i in range(4):\n    ax[i].plot(eeg_t_drift, eeg_data_drift[:,i,:])\n    ax[i].set_title('Channel {} from {}'.format(eeg_chan_drift[i], subj))\nfig.supxlabel('Time (s)')\nfig.supylabel('Voltage (uV)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nMuch better! We can now cross one source of noise off our list:  EEG = Neural + EOG + ACPower + EMG \\cancel{\\ +\\ Scalp} \n\n\nMuscle artifacts\nYour muscles are electrochemical devices that put out their own potentials, called electromyographic activity, abbreviated EMG. When a muscle is depolarized, it can produce a strong Na+ and Ca2+ action potential. Muscle contraction is driven by a barrage of these fast potentials, which can be picked up on electrodes many centimeters away. EMG will be most associated with mouth, forehead, and neck movements. This activity will be very fast relative to the slower activities detected in EEG. You can see EMG in the EEG signals shown above around 200 sec into the recording. Here is a closeup:\n\n# plot the eeg data centered on a burst of EMG\nfig, ax = plt.subplots(4,1,figsize=(5,7))\nfor i in range(4):\n    ax[i].plot(eeg_t_drift, eeg_data_drift[:,i,:])\n    ax[i].set_title('Channel {} from {}'.format(eeg_chan_drift[i], subj))\n    ax[i].set_xlim([200, 205])\nfig.supxlabel('Time (s)')\nfig.supylabel('Voltage (uV)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nTo remove EMG, you can use a filter. We already covered a type of filter, the moving averages we calculated were effectively filters that kept the slow baseline shift. The same concept can be applied to remove EMG, which is at a higher frequency than the neural sources of the EEG that we care about. To provide grater control over the frequencies we want to remove, we will now turn to the filter functions provided in the Scipy Signals package.\n\n\n\nfilter diagram\n\n\nA filter is defined by its type, cutoff frequency, and order.\n\n\n\nfilter types\n\n\nThe two major types we will concern ourselves with are low pass and high pass. Low pass filters allow slow changes in the signal to be passed through, like the moving average we used above to extract the slow drift signal. High pass filters do the opposite, they block the slow portion of the signal and instead allow the faster components through.\n\n\n\nfilter properties\n\n\nWhen we talk about slow and fast parts of the signal, we formally define those by their frequency. We will discuss these in far greater detail in later lectures, but for now you can think of frequency as representing the time scale of the signal. Formally, frequency is measured in events per second, Hertz. If a signal fluctuates 10 times per second, then we say it has a frequency of 10 Hz. The cutoff frequency is the frequency that below which signals are allowed pass for a low pass filter, or which signals are above which are allowed to pass for high pass filters. EMG power tends to be present above 60 Hz. The range of frequencies that are relatively unattenuated by the filter are referred to as the pass band.\nFilters are generally not perfect, they do not block all signals on one side of the cutoff frequency, and allow all signals on the other side to pass unimpeded. To sharpen the cutoff transition, you can increase the order of the filter. However, this can introduce distortions into the signal, which usually inclines us to keep the order low.\nLet’s create a low pass filter using the Scipy Signals package to remove the EMG component from our EEG signal.\n\n# create a low pass filter\nb, a = ss.butter(4, 60, 'low', fs=eeg.srate) # create the filter coefficients\n\nThe function butter takes parameters that specify the properties of the filter, and returns the filter’s coefficients, b and a. The b term is analogous to the kernel we used in when calculating the moving average, while the a term is a kernel applied to the previous values of the filter’s output. If you generate a filter just with the b values (you can use scipy.signal.firwin), it is referred to as a finite impulse response (FIR) filter. If your filter has b and a kernels, then it is an infinite impulse response (IIR) filter. IIR filters have dramatically improved performance over FIR. But how do we evaluate the performance of a filter? An easy way to visualize this is to pass signals with different frequencies through the filter, and measure how much they are attenuated. Signals in the filter’s pass band should have little or no attenuation, while those outside it should be strongly suppressed.\nThe Scipy Signals package provides a function, freqz for doing this. Here is how to use it:\n\n# compute the frequency response of the filter\nw, h = ss.freqz(b, a, fs=eeg.srate)\nfreq_resp = 20*np.log10(abs(h)) # convert to dB\n\n# plot the frequency response in Hz\nplt.plot(w, freq_resp)\nplt.title('Butterworth filter frequency response')\nplt.xlabel('Frequency [Hz]')\nplt.ylabel('Amplitude [dB]')\nplt.xlim(0, 100)\nplt.ylim(-25, 5)\nplt.margins(0, 0.1)\nplt.grid(which='both', axis='both')\nplt.axvline(60, color='green') # cutoff frequency\n\n\n\n\n\n\n\n\nThe cutoff frequency is marked with a green vertical line. To see how the filter works, lets pass a type of periodic signal called a sine wave through it. As we increase the frequency of the sine wave above the cutoff, we should see a dramatic decrease in its amplitude.\n\nsin_freqs = [20, 45, 60, 80]\nt_vals = np.arange(0, 0.1, 0.002)\n\n# plot original sine wave and its filtered version\nfig, ax = plt.subplots(1, 4, figsize=(10, 4))\nfor i, freq in enumerate(sin_freqs):\n    sin_orig = np.sin(2*np.pi*freq*t_vals)\n    sin_filt = ss.lfilter(b, a, sin_orig) # filter the signal\n    ax[i].plot(t_vals, sin_orig)\n    ax[i].plot(t_vals, sin_filt)\n    ax[i].set_title('freq = {}'.format(freq))\n    ax[i].set_xlabel('Time (s)')\n    ax[i].set_ylabel('Amplitude')\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe blue lines are the original sine waves, while the orange is the filtered version. As we increase the frequency of the sine wave, the filter attenuates it more. At the cutoff frequency, 60 Hz, it is attenuated to approximately 70% of its original amplitude, which is known as the -3dB cutoff. You can also see that the filtered signal is shifted. This is known as phase distortion and varies as a function of the frequency. Using the output from the freqz function, we can visualize how this distortion varies systematically with frequency.\n\n# plot the phase distortion of the butterworth filter\nplt.plot(w, np.angle(h)) # h is an array of complex numbers, where the phase-shift is the angle of the complex number\nplt.title('Butterworth filter phase response')\nplt.xlabel('Frequency [Hz]')\nplt.ylabel('Phase [radians]')\nplt.xlim(0, 100)\nplt.margins(0, 0.1)\nplt.axvline(60, color='green') # cutoff frequency\n\n\n\n\n\n\n\n\nEdge effects are also present. At the start of the signal the shape of the filtered version is different from its shape later on, which appears to stabilize. This is due to the zero-padding done by the function lfilter, similar to the issue we ran into with our moving average calculation. Also noteworthy is the lack of an edge effect at the end of the signal. This is because the filter kernels are slid across the signal from left to right, and the kernels are not symmetric. Unlike the moving average kernel, which was applied to data points before and after each sample in a time series, b and a are only applied to the samples before. This is also why we have a phase distortion, because the filter is applied in one direction, from past values to the present. When doing a BCI that has data streaming in real-time, this is the only option (you cannot know what brain activity will look like in the future). However, an offline BCI does not have this limitation. To slide the filter both direction, forwards, and then backwards, we can use scipy.signals.filtfilt.\n\n# plot original sine wave and its filtered version\nfig, ax = plt.subplots(1, 4, figsize=(10, 4))\nfor i, freq in enumerate(sin_freqs):\n    sin_orig = np.sin(2*np.pi*freq*t_vals)\n    sin_filt = ss.filtfilt(b, a, sin_orig) # filter the signal forwards, and then backwards\n    ax[i].plot(t_vals, sin_orig)\n    ax[i].plot(t_vals, sin_filt)\n    ax[i].set_title('freq = {}'.format(freq))\n    ax[i].set_xlabel('Time (s)')\n    ax[i].set_ylabel('Amplitude')\nplt.tight_layout()\n\n\n\n\n\n\n\n\nUsing filtfilt improved things! It virtually eliminated the phase distortion. Frequency selectivity is also better now, with frequencies outside the pass band showing greater reduction in amplitude. This is because the filter is applied twice, effectively increasing its order. On the other hand, there is still an edge effect, but it is most prominent for frequencies outside the pass band.\nNow that we are familiar with filters, we can apply them to removing EMG activity from the EEG signal. Let’s create a function similar to our remove_baseline_drift to do that.\n\n# function to remove EMG artifacts from EEG data\ndef remove_emg(eeg, cut_freq=60):\n    # eeg: EEG data\n    # cut_freq: cutoff frequency\n    \n    # create a bandpass filter\n    b, a = ss.butter(4, cut_freq, 'low', fs=eeg.srate)\n    \n    # determine which channels are EEG\n    eeg_chans = eeg.chans['type'] == 'EEG'\n\n    # apply the filter to the data\n    eeg.data[eeg_chans,:] = ss.filtfilt(b, a, eeg.data[eeg_chans,:], axis=1)\n\n# remove the emg from the data\nremove_emg(eeg)\n\n# pull out the sample channels again, centered on the time period \n# with the EMG artifact\neeg_data_emg, eeg_t_emg, eeg_chan_emg = eeg.get_data(chans=example_chans, start_t=200, dur_t=5)\n\n# overlay the EMG filtered over the original visualize the section of \n# data that previously had an EMG artifact\nfig, ax = plt.subplots(4,1,figsize=(5,7))\nfor i in range(4):\n    ax[i].plot(eeg_t_drift, eeg_data_drift[:,i,:])\n    ax[i].plot(eeg_t_emg, eeg_data_emg[:,i,:])\n    ax[i].set_title('Channel {} from {}'.format(eeg_chan_drift[i], subj))\n    ax[i].set_xlim([200, 205])\nfig.supxlabel('Time (s)')\nfig.supylabel('Voltage (uV)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe orange signal with the EMG filter applied shows less EMG activity than before, a good sign. Some still remains, but this will be hard to eliminate without distorting the EEG signal or removing frequencies that we care about.\n EEG = Neural + EOG + ACPower \\cancel{\\ + EMG \\ }\\cancel{\\ +\\ Scalp} \n\n\nAC power line noise\nEven with excellent referencing, AC power interference can still leak into recordings and must be dealt with. In the US this is at 60 Hz, while in Europe, where this recording comes from, it is at 50 Hz. What does this signal look like? In our recording, channel FCz exhibited prominent 50 Hz AC noise.\n\n# Get an EEG epoch with a lot of AC noise\neeg_data_ac, eeg_t_ac, eeg_chan_ac = eeg.get_data(chans=['FCz'], start_t=200, dur_t=1)\n\n# Plot the data\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(eeg_t_ac, eeg_data_ac.squeeze())\nax.set_xlabel('Time (sec)')\nax.set_ylabel('Voltage (uV)')\nax.set_title('EEG data at FCz')\n\nText(0.5, 1.0, 'EEG data at FCz')\n\n\n\n\n\n\n\n\n\nNotice that the EEG oscillates at a regular rate, and that this masks the underlying irregular EEG activity that presumably arises from neural sources.\nThere are two filtering strategies we can use. The easiest way to eliminate it is to filter out any signals in the EEG that are at or above the frequency of the AC signal. For this we could apply the same filtering strategy we used for the EMG. But, keep in mind that if we set the cutoff frequency of the filter to the AC frequency, it will only attenuate it moderately. To further suppress the AC signal, we would need to set the cutoff to a lower frequency, perhaps 30 Hz in the case of our 50 Hz AC noise. This runs the risk of eliminating activity we care about, but let’s try it anyway.\n\n# load EEG signal with AC noise\neeg_data_ac, eeg_t_ac, eeg_chan_ac = eeg.get_data(chans=['FCz'], start_t=200, dur_t=1)\n\n# create low pass filter\nb_lp,a_lp = ss.butter(4, 30, 'low', fs=eeg.srate)\n\n# apply filter\neeg_data_lp = ss.filtfilt(b_lp,a_lp,eeg_data_ac, axis=0)\n\n# plot\nplt.figure(figsize=(10,3))\nplt.plot(eeg_t_ac, eeg_data_ac.squeeze(), label='raw', alpha=0.5)\nplt.plot(eeg_t_ac, eeg_data_lp.squeeze(), label='low pass')\nplt.legend()\nplt.xlabel('Time (sec)')\nplt.ylabel('Voltage (uV)')\nplt.show()\n\n\n\n\n\n\n\n\nThat certainly reduces the 50 Hz noise. However, the signal has now lost much of its sharpness because the cutoff for the low pass filter was set to 30 Hz. We are throwing out a lot of signal that may arise from neural sources.\nThe second filtering option is to use a notch filter. It suppresses frequencies centered on a specific value, such as 50 Hz, while allowing others to pass, whether they be higher or lower, to pass through unimpeded. This is useful for AC noise because its frequency is specific to just one value.\nA notch filter is specified by two parameters. Its center frequency, \\omega_0 (w0 in the code below), sets frequency at which the maximum attenuation occurs. The depth of this attenuation and its spread to adjacent frequencies is determined by its Q (for quality) factor. A higher Q will decrease the attenuation at \\omega_0 and its spread to adjacent frequencies. Thus, there is a trade off. The filter can strongly attenuate the frequency you want to reject, but at the cost of impacting other nearby frequencies. If you want to minimize that impact, you can lower Q, but at the cost of attenuating the center frequency less. Let’s use the iirnotch function to create a notch filter at 50 Hz with a moderate Q.\n\n# create notch filter coeffients\nb_nc, a_nc = ss.iirnotch(w0=50, Q=15, fs=eeg.srate)\n\n# apply filter\neeg_data_nc = ss.filtfilt(b_nc,a_nc,eeg_data_ac, axis=0)\n\n# plot\nplt.figure(figsize=(10,3))\nplt.plot(eeg_t_ac, eeg_data_ac.squeeze(),  label='raw', alpha=0.5)\nplt.plot(eeg_t_ac, eeg_data_lp.squeeze(),  label='low pass')\nplt.plot(eeg_t_ac, eeg_data_nc.squeeze(),  label='notch')\nplt.legend()\nplt.xlabel('Time (sec)')\nplt.ylabel('Voltage (uV)')\nplt.show()\n\n\n\n\n\n\n\n\nThe notch filter allows higher frequencies to get through, while still attenuating the 50 Hz noise. It is good to see a difference in filter performance by eye on the signals we want to process, but a more systematic approach would be to characterize the frequency response properties of the filters.\n\n# calculate frequency responses of filters\nw_lp, h_lp = ss.freqz(b_lp, a_lp, fs=eeg.srate)\nw_nc, h_nc = ss.freqz(b_nc, a_nc, fs=eeg.srate)\n\n# plot frequency response of filter\nfig, ax = plt.subplots(2,1, figsize=(10,5))\nax[0].plot(w_lp, 20 * np.log10(abs(h_lp)), label='low pass')\nax[0].plot(w_nc, 20 * np.log10(abs(h_nc)), label='notch')\nax[0].axvline(50, color='red', linestyle='--', label='50 Hz')\nax[0].set_xlim(0,100)\nax[0].set_ylim(-50,5)\nax[0].set_xlabel('Frequency (Hz)')\nax[0].set_ylabel('Gain (dB)')\nax[0].set_title('Frequency Response')\nax[0].legend()\n\n# plot the phase response of filter\nax[1].plot(w_lp, np.angle(h_lp))\nax[1].plot(w_nc, np.angle(h_nc))\nax[1].axvline(50, color='red', linestyle='--')\nax[1].set_xlim(0,100)\nax[1].set_ylim(-np.pi,np.pi)\nax[1].set_xlabel('Frequency (Hz)')\nax[1].set_ylabel('Phase (rad)')\nax[1].set_title('Phase Response')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nAt the AC frequency, the attenuation from both the low pass and notch are equivalent. However, the drawback of the low pass is apparent due to it attenuating signals beyond the AC frequency. Moreover, the low pass filter introduces substantial phase distortion in the pass band, which contains EEG signals we care about. This is not a problem if we are analyzing the data offline and can run the filter forwards and backwards with filtfilt, but would be a serious issue this was for a real-time BCI application that can only run in the forward direction.\nGiven that, let’s create a function for removing AC noise from the EEG using a notch filter.\n\n# function to remove AC noise from EEG data\ndef remove_ac(eeg, ac_freq=60): \n    # eeg: EEG data\n    # ac_freq: frequency of the AC noise (default: 60 Hz because we're in the US)\n    \n    # create a bandpass filter\n    b, a = ss.iirnotch(ac_freq, 15, fs=eeg.srate)\n    \n    # determine which channels are EEG\n    eeg_chans = eeg.chans['type'] == 'EEG'\n\n    # apply the filter to the data\n    eeg.data[eeg_chans,:] = ss.filtfilt(b, a, eeg.data[eeg_chans,:], axis=1)\n\n# remove the emg from the data\nremove_ac(eeg, 50)\n\n# pull out the sample channels again, centered on the time period \n# with the EMG artifact\neeg_data_ac, eeg_t_ac, eeg_chan_ac = eeg.get_data(chans=example_chans, start_t=10, dur_t=1)\n\n# overlay the AC filtered over the original signal\nfig, ax = plt.subplots(4,1,figsize=(5,7))\nfor i in range(4):\n    ax[i].plot(eeg_t_drift, eeg_data_drift[:,i,:], alpha=0.5)\n    ax[i].plot(eeg_t_ac, eeg_data_ac[:,i,:])\n    ax[i].set_title('Channel {} from {}'.format(eeg_chan_drift[i], subj))\n    ax[i].set_xlim([10,11])\n    ax[i].set_ylim([-25,25])\nfig.supxlabel('Time (s)')\nfig.supylabel('Voltage (uV)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nOur AC filter seems to work really well.\n EEG = Neural + EOG \\cancel{\\ + \\ ACPower} \\cancel{\\ + EMG \\ }\\cancel{\\ +\\ Scalp} \n\n\nEye blinks and movement artifacts\nYour eyeball is polarized, with large potential difference between the cornea and retina. This potential, referred to as the electrooculogram(EOG)affects electrodes on the scalp, with those at frontal sites just above the eyes, such as F or Fp, picking it up most strongly. So long as the eye remains motionless, this just causes a constant offset in the voltage at an EEG electrode. But eyes move, a lot, especially when subjects are engaged in the kinds of activities we want to relate back to the brain. Your eyes constantly dart around, called saccading, between different spots in your field of view. When the eye moves or blinks, the voltages at your EEG sites change. Since each EEG electrode will pick this up differently, one cannot just rely on referencing to eliminate. The signal on the reference electrode will look different from that on other electrodes. The simplest way to deal with these is to measure the eye movements directly by placing electrodes adjacent to the subject’s eyes, and then exclude periods when eye movements were present from subsequent analysis. More sophisticated approaches (e.g. independent component analysis) can isolate and remove the contribution of eye blinks to the EEG, allowing one to analyze periods where they occurred. These are beyond the purview of this course.\nTo detect eye blinks, investigators place recording electrodes near the subject’s eyes. This gives a relatively pure measure of the EOG. Our recording has two channels for this, VEO and HEO.\n\n# get EOG signal\neog_data, eog_t, eog_chans = eeg.get_data(chans=['VEO', 'HEO'])\n\n# plot EOG signal\nplt.plot(eog_t, eog_data.squeeze(), label=eog_chans)\nplt.xlabel('Time (s)')\nplt.ylabel('EOG (uV)')\nplt.title('EOG signals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWell, looks like we have drifting baselines in our EOG signals. If you look closely at the HEO trace, you can see little occasional ticks. These reflect eye blinks. They are not so apparent on the VEO trace. Remove the baseline from the HEO signal and zoom in on one of those eye blink events. (This recording is a little funny because eye blinks are usually best detected on the VEO channel positioned above the eye. It might be a mistake in how the channels were labeled or connected by the experimenter.)\n\nheo = eog_data[:, 1, 0].copy()\n\n# moving average window\nwindow = 1 # seconds\nheo = heo-moving_average(heo, eeg.srate*window)\n\nplt.plot(eog_t, heo)\nplt.xlim((112, 118))\nplt.xlabel('Time (s)')\nplt.ylabel('HEO (uV)')\nplt.title('Eyeblink events')\n\nText(0.5, 1.0, 'Eyeblink events')\n\n\n\n\n\n\n\n\n\nEach eyeblink is associated with a large voltage deflection (&gt;150 uV) that, at least near the eye, which dwarfs the EEG potentials we record from the scalp (~20 uV). To see how the EEG channels pick up the EOG, we will plot the same time period from some example EEG channels, which range from the front to the back of the scalp.\n\neeg_data, eeg_t, eeg_chans = eeg.get_data(chans=example_chans, start_t = 112, dur_t=6)\n\n# Plot each eeg data channel in a separate axis with the heo signal underneath\nfig, ax = plt.subplots(len(example_chans), 1, figsize=(5, 10), sharex=True)\nfor i, chan in enumerate(example_chans):\n    ax[i].plot(eog_t, heo, alpha=0.5, label='EOG')\n    ax[i].plot(eeg_t, eeg_data[:, i], label='EEG')\n    ax[i].set_title(chan)\n    ax[i].set_xlim([eeg_t[0], eeg_t[-1]])\n    ax[i].set_ylim([-200, 200])\nax[0].legend()\nfig.supylabel('Voltage (uV)')\nfig.supxlabel('Time (s)')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe EOG artifact is readily apparent on the frontal site, F1, which is closest to the eye. Its influence fades at more posterior sites (towards the back of the head), but it can still have an impact. This can become especially problematic when we want to analyze brain activity during the presentation of visual stimuli. Often we blink or move our eyes involuntarily during the presentation of visual stimuli, so we need a way to deal with this.\nOne approach is to discern a relationship between the EOG and EEG signals, and then use that relationship to predict how the EEG will be affected by EOG. Using this, one can subtract that predicted signal from the EEG, and hopefully completely excise the EOG’s influence. This is doable, but is tricky to do well and we will not consider it further (but to see how, check out this).\nThe simplest approach is to detect the eye blink event from the EOG recording and block out that period from subsequent analysis. For this, we need to detect peaks in the EOG signal that exceed a threshold. Scipy signals offers a useful function called findpeaks that does just that. It has a variety of arguments that allow you to tailor exactly how to detect peaks in a signal. The ones we are most concerned with are:\n\nheight: which specifies the minimum value a peak can have\ndistance: the minimum distance allowed between two peaks\n\nNotice that our EOG signal is negative-going, so we will have to negative the EOG signal we pass to findpeaks to ensure that it works properly. Given the example trace above, it appears that a height of 100 uV and a distance of 0.5 seconds should work.\n\nblink_h = 100 # minimum height of the blink peak in uV\nblink_d = 0.5*eeg.srate # minimum time between blinks in samples\n\n# Find the blinks using findpeaks from the scipy.signal package\nblink_idx, blink_props = ss.find_peaks(-heo, height=blink_h, distance=blink_d)\n\n# get the time for each blink peak\nblink_times = blink_idx / eeg.srate\n\n# get the peak height for each blink\nblink_heights = -blink_props['peak_heights']\n\n# Plot the blink peaks on top of the HEO signal\nfig, ax = plt.subplots(figsize=(12,4))\nax.plot(eog_t, heo)\nax.scatter(blink_times, blink_heights, color='red')\nax.set_xlabel('Time (s)')\nax.set_ylabel('HEO (uV)')\nax.set_title('Detected blink peaks')\n\nText(0.5, 1.0, 'Detected blink peaks')\n\n\n\n\n\n\n\n\n\nThe detection algorithm seems to be working pretty well. The only time where there may be problems is around 110 seconds, where a small peak is missed and a bunch of blinks have clustered close together, with one possibly evading detection. Let’s look closer:\n\n# Replot the figure above, but zoomed in on the potentially problematic region\nplt.figure(fig)\nax.set_xlim(108,118)\nplt.show()\n\n\n\n\n\n\n\n\nFortunately, it appears that our detection algorithm worked great during the cluster of eye blinks between 114 and 117 seconds. However, it did miss the small (partial?) blink at 110 seconds. To detect that, we would need to lower our threshold. Lets give that a try.\n\nblink_h = 30 # minimum height of the blink peak in uV\n\n# Find the blinks using findpeaks from the scipy.signal package\nblink_idx, blink_props = ss.find_peaks(-heo, height=blink_h, distance=blink_d)\n\n# get the time for each blink peak\nblink_times = blink_idx / eeg.srate\n\n# get the peak height for each blink\nblink_heights = -blink_props['peak_heights']\n\n# Plot the blink peaks on top of the HEO signal\nfig, ax = plt.subplots(figsize=(12,4))\nax.plot(eog_t, heo)\nax.scatter(blink_times, blink_heights, color='red')\nax.set_xlabel('Time (s)')\nax.set_ylabel('HEO (uV)')\nax.set_title('Detected blink peaks')\n\nText(0.5, 1.0, 'Detected blink peaks')\n\n\n\n\n\n\n\n\n\nMuch better. It has even picked up on a cluster of weak blinks around 200 seconds that escaped my attention before. Just to be careful, let’s check those out and make sure they are detected properly and nothing has been missed.\n\n# Replot the figure above, but zoomed in on the potentially problematic region\nplt.figure(fig)\nax.set_xlim(190,210)\nplt.show()\n\n\n\n\n\n\n\n\nUh oh, what looked like blinks when zoomed out turns out to be another kind of artifact, EMG. We have two options for dealing with this. One would be to just set the threshold high enough not to detect those EMG bursts. However, doing so might make us lose the weak eye blink earlier in the recording. Alternatively, we could filter out the EMG like we did for the EEG data prior to running the peak detection. Thus, to create a function for eye blink detection we need to pool together the following step:\n\nGet the EOG signal\nInvert the EOG signal\nRemove the baseline drift\nRemove the EMG\nFind peaks\n\nSteps 3 and 4 are both filtering operations, with removal of baseline drift a high pass filter with a very low frequency cutoff (~1 Hz), and EMG removal a low pass filter with a cutoff around 50 Hz. Instead of splitting these into two separate filtering steps, we can implement them as one filter, called a band pass filter. A band pass filter has a low and high frequency cutoff, and allows signals to pass between those. Let’s design our detect_blinks function with one of those.\n\ndef detect_blinks(eeg, eog_chan='HEO', threshold=40, ibi=0.5):\n    \"\"\"\n    Detect blinks in the EEG signal.\n\n    Parameters\n    ----------\n    eeg : our EEG object\n    eog_chan : str\n        The name of the EOG channel. Defaults to 'HEO'.\n    threshold : float\n        The threshold for detecting blinks. Defaults to 40.\n    ibi : float\n        The minimum inter-blink interval (in seconds). Defaults to 0.5.\n\n    Returns\n    -------\n    blink_times : array of floats\n        The times of the blinks in seconds.\n    blink_heights : array of floats\n        The height of the blinks.   \n    \"\"\"\n    # Format inputs\n    ibi = ibi * eeg.srate\n\n    # Find the EOG channel\n    eog_data, eog_t, _ = eeg.get_data(chans=eog_chan)\n\n    # Format EOG data for peak finding\n    eog_data = -eog_data.squeeze()\n\n    # Filter drifting baseline and EMG out of the EOG data\n    b, a = ss.butter(2, [1, 50], 'bandpass', fs=eeg.srate)\n    eog_data = ss.filtfilt(b, a, eog_data)\n\n    # Find the blinks\n    blink_times, blink_props = ss.find_peaks(eog_data, height=threshold, distance=ibi)\n\n    # Convert blink_times to seconds\n    blink_times = eog_t[blink_times]\n\n    # Get the blink heights\n    blink_heights = -blink_props['peak_heights']\n\n    return blink_times, blink_heights\n\nblink_times, blink_heights = detect_blinks(eeg)\n\n# Plot the blink peaks on top of the HEO signal\nfig, ax = plt.subplots(figsize=(12,4))\nax.plot(eog_t, heo)\nax.scatter(blink_times, blink_heights, color='red')\nax.set_xlabel('Time (s)')\nax.set_ylabel('HEO (uV)')\nax.set_title('Detected blink peaks')\n\nText(0.5, 1.0, 'Detected blink peaks')\n\n\n\n\n\n\n\n\n\nIncluding the EMG filter, as part of the band pass filter, fixed the problem of detecting EMG. Now we know what times to exclude in our subsequent analyses to ensure that our data is not contaminated by EOG artifacts.\n EEG = Neural \\cancel{\\ +\\ EOG} \\cancel{\\ + \\ ACPower} \\cancel{\\ + EMG \\ }\\cancel{\\ +\\ Scalp} \n\n\n\nA simple model of the EEG signal\nWe have just covered a variety of neural and non-neural sources for our EEG signal. Once we eliminated the noise and artifactual contributions to the EEG, we are left with just the neural source. This can be thought of as an equation:\n Neural = Neural_{evoked} + Neural_{spon} \nNeural_{evoked} is the neural activity that is time-locked to stimuli or behavioral events. Neural_{spon} is spontaneous brain activity that is not time-locked to events, or is ongoing.\nFor the rest of the lesson, we will go over how to eliminate or account for each these. In particular, we are interested in extracting Neural_{evoked} component."
  },
  {
    "objectID": "Week2.html#evoked-response-potentials",
    "href": "Week2.html#evoked-response-potentials",
    "title": "Week 2",
    "section": "Evoked response potentials",
    "text": "Evoked response potentials\nEvents, whether stimuli or actions, are associated with a chain of activation throughout the nervous system. For instance, stimuli activate peripheral sensory receptors (such as the retina), producing a cascade of neural activation moves towards the cortex. Upon reaching the cortex, it is conveyed for further processing to determine the features of the stimulus and its relevance. This chain of activity can be picked up by EEG electrodes as a stimulus evoked response, which has a stereotypical time course that, to some extent, is related to the properties of the stimulus and its meaning. Evoked responses were first demonstrated shortly after Berger discovered the EEG, and have remained a mainstay of understanding brain function.\nEvoked responses can be characterized using the event-related potential technique. A subject with EEG electrodes is seated comfortably and performs a task that features the repeated presentation of stimuli. Sometimes the task is passive, in which case the subject simply attends to the stimuli but does not respond. Other times the task can be active and demands a response, such as a button press, that depends on the presentation of the stimulus or one of its features. Each stimulus presentation is referred to as trial. The periods between trials, the inter-trial interval (ITIs), are usually several seconds long. This allows for the observable effects of the stimulus on brain state to fade and not contaminate the next stimulus presentation.\nOnce the task is over, the researcher extracts the epochs of the EEG surrounding each time an event occurs. They then average these with respect to time to generate the mean event-related potential (ERP). Often an ERP is comprised of a series of peaks and troughs, which are generally referred to as components. While it is tempting to ascribe each component to a different underlying neural process, this is not necessarily the case. Since the focus of this course is on decoding the electrical activity of the brain, we can side step this complication. The question we are faced with here is whether there is a pattern to the EEG that indicates the presence or identity of an event, not how the brain generated it.\n\n\n\nExample ERPs\n\n\nAbove is an example of a classic ERP. Two types of trials were presented to subjects. During CERTAIN trials, a ‘cueing’ stimulus indicated whether the ‘test’ stimulus that followed 3-5 seconds later would be a sound or a light. For UNCERTAIN trials, the cueing stimulus did not predict the test stimulus. This figure shows the ERP to the same sound test stimulus, with those presented on certain trials as a solid line, while the dashed line is for the test stimulus on uncertain trials. Each graph shows the ERPs for different subjects. The ERPs are slightly different between subjects, but in general we can see an quick initial negative peak around 100 ms, and a positive going longer peak at 300 ms. Note that the voltage scale is flipped, with positive facing down and negative facing up.\nThose peaks are named according to a nomenclature used by ERP researchers. In general, the names start with a ‘P’ (positive) or ‘N’ (negative), that indicate the sign of the peak. This is followed by a number, either denoting the order of the peak (1 = first, 2 = second, etc), or its latency from the event time in milliseconds. For our example, the initial negative peak is known as a N100, which is evoked by an auditory stimulus. The next major component is the P300, and is strengthened when a stimulus is unexpected. As you can see in the figure above, the ERP was substantially more positive for the UNCERTAIN condition.\n\nCalculating ERPs\nFor the data set we loaded, there was an auditory stimulus presented. To calculate the ERP, we need to know the times it occurred. These are stored in a file with the name ‘*_events.tsv’. We already covered how to read .tsv files when we loaded the channel information. Let’s do the same for the events data.\n\n# load the a *_channels.tsv file\nsubj = 'sub-AB58'\nevt_dir = ['.', 'data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_events.tsv'.format(subj)]\nevt_file = op.join(*evt_dir) # use * to unpack the list\nevents = pd.read_csv(evt_file, sep='\\t')\nprint(events)\n\n      onset  duration  sample trial_type  response_time  stim_file  value\n0    12.084       NaN    6042        cue            NaN        NaN      1\n1    19.384       NaN    9692        cue            NaN        NaN      1\n2    26.398       NaN   13199        cue            NaN        NaN      1\n3    34.224       NaN   17112        cue            NaN        NaN      1\n4    42.258       NaN   21129        cue            NaN        NaN      1\n5    50.148       NaN   25074        cue            NaN        NaN      1\n6    59.184       NaN   29592        cue            NaN        NaN      1\n7    66.008       NaN   33004        cue            NaN        NaN      1\n8    74.248       NaN   37124        cue            NaN        NaN      1\n9    81.408       NaN   40704        cue            NaN        NaN      1\n10   89.604       NaN   44802        cue            NaN        NaN      1\n11   97.628       NaN   48814        cue            NaN        NaN      1\n12  106.708       NaN   53354        cue            NaN        NaN      1\n13  114.678       NaN   57339        cue            NaN        NaN      1\n14  122.868       NaN   61434        cue            NaN        NaN      1\n15  130.924       NaN   65462        cue            NaN        NaN      1\n16  138.024       NaN   69012        cue            NaN        NaN      1\n17  147.120       NaN   73560        cue            NaN        NaN      1\n18  154.494       NaN   77247        cue            NaN        NaN      1\n19  161.784       NaN   80892        cue            NaN        NaN      1\n20  170.020       NaN   85010        cue            NaN        NaN      1\n21  177.394       NaN   88697        cue            NaN        NaN      1\n22  185.760       NaN   92880        cue            NaN        NaN      1\n23  193.554       NaN   96777        cue            NaN        NaN      1\n24  200.530       NaN  100265        cue            NaN        NaN      1\n25  209.654       NaN  104827        cue            NaN        NaN      1\n26  218.660       NaN  109330        cue            NaN        NaN      1\n27  227.440       NaN  113720        cue            NaN        NaN      1\n28  234.904       NaN  117452        cue            NaN        NaN      1\n29  242.270       NaN  121135        cue            NaN        NaN      1\n30  259.322       NaN  129661   task end            NaN        NaN     10\n\n\nThere are two columns we care about here. The ‘onset’ column gives the time when a stimulus was presented. These are the times we need to determine which epochs we want to record from. The other is ‘trial_type’, which indicates the type of stimulus given. For this data set, subjects were exposed to the same stimulus 30 times. The final event, ‘task end’, is just the end of the session.\nIt is the cue evoked ERP we want to calculate, so let’s pull out their times.\n\ncue_times = events['onset'][events['trial_type'] == 'cue'].values\n\nThe README file for this data set does not specify the time between cue stimuli, so let’s calculate that. This will tell us what the maximum epoch duration we can use to avoid having two stimuli in one epoch.\n\n# Calculate iternval between cue times\ncue_iti = np.diff(cue_times)\n\n# Plot histogram of cue_iti\nplt.hist(cue_iti, bins=np.linspace(0, 12, 24))\nplt.xlabel('Time (s)')\nplt.ylabel('Count')\nplt.title('Distribution of cue inter-trial intervals')\nplt.show()\n\n\n\n\n\n\n\n\nThe ITIs range from 7 to 9 seconds, so we have nothing to worry about since our ERP will probably last no longer than 0.5 seconds. For our cue epochs, we will set the time windows to start 100 ms before the cue till 500 ms after.\nNow that we have a cue window decided, we have to check whether any eye blinks overlap with our cue epochs. For those that do, we must remove them from analysis. Since blinks can last a couple hundred milliseconds, we will block out any epochs where a blink happened within 1 second of the stimulus.\n\nexcl_period = 0.6 # exclusion period in seconds\n\ndef remove_blink_epochs(epoch_times, blink_times, excl_period=excl_period):\n    \"\"\"Removes epochs from cue_times that are within excl_period of a blink.\n    \n    Parameters\n    ----------\n    epoch_times : 1d array\n        Epoch times in seconds.\n    blink_times : 1d array\n        Blink times in seconds.\n    excl_period : float\n        Exclusion period in seconds.\n    \n    Returns\n    -------\n    valid_epoch_times : 1d array\n        Epoch times that are not within excl_period of a blink.\n    percent_valid : float\n        Percentage of epochs that are valid.\n    \"\"\"\n    valid_epoch_times = []\n    for time in epoch_times:\n        # gets us the times of blink relative to cue onset\n        rel_blink_times = blink_times - time \n\n        # if none of the blinks are within the exclusion period, keep the epoch\n        if not np.any(np.abs(rel_blink_times) &lt; excl_period):\n            valid_epoch_times.append(time)\n    \n    return np.array(valid_epoch_times), len(valid_epoch_times)/len(epoch_times)*100\n\nvalid_cue_times, valid_percent = remove_blink_epochs(cue_times, blink_times, excl_period)\n\n# print the percentage of epochs we kept\nprint('There were {} blink free epochs'.format(len(valid_cue_times)))\nprint('Kept {:.2f} of epochs'.format(valid_percent))\n\nThere were 25 blink free epochs\nKept 83.33 of epochs\n\n\nWe lose just under 20% of our cues because of blinks, which is substantial but not so much that it would seriously impact our averaging. We can use the get_data method of our EEG class to load the EEG at all sites during the blink-free cue presentations.\n\n# window settings\npre_window = 0.1\npost_window = 0.5\nepoch_dur = pre_window + post_window\nepoch_starts = valid_cue_times - 0.1\n\n# load epochs\ncue_eeg = eeg.get_data(chans='eeg', start_t=epoch_starts, \n                       dur_t=epoch_dur, scale='relative')\n\n# split outputs\ncue_data = cue_eeg[0]\ncue_t = cue_eeg[1]-pre_window\ncue_ch = cue_eeg[2]\n\n# print the shape of each output\nprint('cue_data has the shape: {}'.format(cue_data.shape))\nprint('cue_t has the shape: {}'.format(cue_t.shape))\nprint('cue_ch has the shape: {}'.format(cue_ch.shape))\n\ncue_data has the shape: (300, 61, 25)\ncue_t has the shape: (300,)\ncue_ch has the shape: (61,)\n\n\nWe set the time scale to ‘relative’ because we care about the time of the EEG signal only with respect to cue delivery. Since the get_data function returns times with respect to start_t, and we have that set 100 ms before the cue, we subtract those 100 ms from the relative time. This way, the cue onset is at time 0, and time points before the cue have negative values, while those after the cue have positive values.\ncue_data contains our EEG signal as a tensor (a multidimensional array) with 3 dimensions: time, channel, and trial. Each row is a different time point with respect to cue onset. Each column is a different channel. And each ‘sheet’ (3rd dimension) is a different cue delivery epoch.\nSince we are not sure which recording site to focus on at first, we will calculate the average response across all sites. This is easy to do using numpy’s mean function. When calling it, we can indicate the dimensions along which we want the mean to be taken. To get the mean across all trials and channels, we set the axis parameter to (1,2), which corresponds to the channel and trial dimensions.\n\ncue_erp_all = np.mean(cue_data,axis=(1,2))\ncue_erp_all.shape\n\n(300,)\n\n\nTaking the mean of our cue_data tensor in this way yields a 1-dimensional array with a size that is the number of time points we have per trial:  \\begin{split}\n    &= 500\\frac{samples}{sec}(0.1 sec + 0.5 sec) \\\\\n    &= 500\\frac{samples}{sec} 0.6 sec \\\\\n    &= 300 samples\n    \\end{split}\n\nNext we want to see what the shape of our average ERP across all channels looks like. In addition, we want to know the latency for its strongest component.\n\n# get peak time of the cue ERP\nerp_max_idx = np.argmax(cue_erp_all)\nerp_peak_time = cue_t[erp_max_idx]\n\n# plot the cue ERP\nplt.plot(cue_t, cue_erp_all)\nplt.scatter(erp_peak_time, np.max(cue_erp_all), c='r')\n\nplt.text(erp_peak_time, np.max(cue_erp_all), 'Peak time: {} ms'.format( \\\n         str(round(erp_peak_time*1000))))\nplt.xlabel('Time (ms)')\nplt.ylabel('Voltage (uV)')\nplt.title('Cue ERP averaged across all channels')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nHere we have a robust ERP in response to the visual cue. It’s first component starts around 90 ms following the cue, and peaks at 120 ms. It is followed by a couple smaller components of opposite polarity.\nWith the peak time for the first, and strongest component, we can look across all electrodes to see where that component is strongest. A simple way to this is to get the mean ERP across trials for each channel, and then plot the ERP amplitude at that peak time across the scalp. To facilitate this, we created a plot_scalp method for our EEG class that allows us to color the EEG sites based on their voltage.\n\n# calculate mean ERP for each channel\ncue_erp_ch = np.mean(cue_data, axis=2)\nprint('cue_erp_ch has the shape {}'.format(cue_erp_ch.shape))\n\n# for each channel, get the ERP voltage at the time of the ERP peak\ncue_scalp = cue_erp_ch[erp_max_idx, :]\nprint('cue_scalp has the shape {}'.format(cue_scalp.shape))\n\ncue_erp_ch has the shape (300, 61)\ncue_scalp has the shape (61,)\n\n\nWhen taking the mean of cue_data along the 2nd axis, the trial axis, we get a an array cue_erp_ch that has the ERP for each EEG channel. To find the ERP voltage at each channel for the peak time we identified above, we just get the values from the corresponding row. This gives a vector with with a size equal to the number of channels. We can pass this to the plot_scalp method to visualize the distribution of the first component strengths across the scalp.\n\n# plot the ERP scalp topography\nscalp_ax = eeg.plot_scalp(colors=cue_scalp)\nplt.colorbar(scalp_ax.collections[0], label='Amplitude (uV)')\n\n\n\n\n\n\n\n\nAmplitudes are colored from yellow to blue, with yellow being the most positive values and blue the lowest/most negative. There is an obvious gradient across the scalp, with sites over the occipital and parietal cortices (O, PO, and P) showing the strongest amplitudes. Those areas where most early visual processing takes place, however the cue stimulus was a sound. Auditory processing tends to occur in the temporal lobe, but the T sites that lie nearest to that structure do not show the strongest response! This underscores the complex relationship between ERP components and the neural activations that underlie them. The expression of a neural activity as scalp potentials depends on the timing and location of that activity, how it is oriented with respect to the curved surface of the cortex, its synchrony with other activities across the cortex, and the its smearing by the dura, cerebrospinal fluid, skull, and skin of the scalp. In theory, if you know all of these things you should be able to perfectly predict the ERP distribution on the scalp, in practice you often only know a few of these factors. Thus, when analyzing ERPs components it is safest to avoid making direct neural inferences about them, e.g. a component over occipital cortex does not necessarily reflect activation of occipital cortex.\nLet’s now plot the ERP for the channel it showed its strongest signal. It is hard to tell which one is that from the scalp map above, so instead we will create a list of channels sorted by the strength of their ERP peak voltage from strongest to weakest.\n\n# get sorted indices of cue_scalp\nmax_ch_idx = np.argsort(-cue_scalp) # negate to sort in descending order\nmax_ch = cue_ch[max_ch_idx]\n\nprint('Strongest -&gt; weakest \\n{}'.format(max_ch))\n\nStrongest -&gt; weakest \n['M1' 'M2' 'PO7' 'PO5' 'P7' 'CB1' 'O1' 'O2' 'F8' 'CB2' 'PO6' 'PO8' 'P8'\n 'PO4' 'Oz' 'P5' 'PO3' 'POz' 'TP7' 'P3' 'P2' 'P4' 'P6' 'TP8' 'T7' 'Pz'\n 'CP5' 'FT8' 'T8' 'P1' 'C5' 'AF4' 'CP6' 'CP2' 'CP4' 'CP3' 'C6' 'CP1' 'CPz'\n 'FT7' 'F7' 'F5' 'FC6' 'FC5' 'AF3' 'FC3' 'C3' 'C1' 'FC1' 'C2' 'C4' 'Cz'\n 'F3' 'F1' 'F2' 'F4' 'F6' 'FC4' 'Fz' 'FC2' 'FCz']\n\n\nNotice that M1 and M2, the mastoid sites, came out on top. These are frequently used as reference sites, and so we will instead go to the next strongest site, PO7.\nWe will plot the ERP on PO7, along with each individual trial so we can visualize the variability of the ERP.\n\n# get PO7 index\nsel_chan = 'PO7'\npo7_idx = np.where(cue_ch == sel_chan)[0][0]\npo7_idx\n\n# plot PO7 ERP\nplt.plot(cue_t, cue_data[:, po7_idx, :], color='k', alpha=0.2)\nplt.plot(cue_t, cue_erp_ch[:, po7_idx], label='Mean ERP', color='r')\nplt.xlabel('Time (s)')\nplt.ylabel('Voltage (uV)')\nplt.title('{} ERP'.format(sel_chan))\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe red line is the mean ERP across all trials and each faded black line is the response on a single trial. There is a remarkable similarity in the ERP shape across trials. Also note the variability in activity before and several hundred milliseconds after the cue. Those activities are spontaneous, and because they lack a consistent relationship with cue onset tend to average out to zero. Thus, using averaging across trials we can isolate the evoked component of the EEG.\n ERP = Neural_{evoked} + \\cancel{Neural_{spon}}"
  },
  {
    "objectID": "Week2.html#intersubject-variability",
    "href": "Week2.html#intersubject-variability",
    "title": "Week 2",
    "section": "Intersubject variability",
    "text": "Intersubject variability\nAn ERP can be consistent across trials and sessions when taken from the same subject, but between subjects there may be a great deal of variability. This goes back to the complex relationship between patterns of brain activation, geometry of the brain surface and skull, and their reflection on the scalp. Any variations in these factors between individuals that affect that relationship can impact the expression of the ERP. As a consequence, it is important to examine the ERP within each subject separately. Now that we have all the code worked out for loading, preprocessing, and extracting ERPs, we can quickly apply them to a new subject.\n\n# get EEG file names for subject AB64\n# get file paths\nsubj2 = 'sub-AB64'\nchan_dir2 = ['.', 'data', 'eeg', 'ds003690', subj2, 'eeg', '{}_task-passive_run-1_channels.tsv'.format(subj2)]\nchan_file2 = op.join(*chan_dir2) # use * to unpack the list\ndata_dir2 = ['.', 'data', 'eeg', 'ds003690', subj2, 'eeg', '{}_task-passive_run-1_eeg.set'.format(subj2)]\neeg_file2 = op.join(*data_dir2) # use * to unpack the list\nevt_dir2 = ['.', 'data', 'eeg', 'ds003690', subj2, 'eeg', '{}_task-passive_run-1_events.tsv'.format(subj2)]\nevt_file2 = op.join(*evt_dir2) # use * to unpack the list\nevents2 = pd.read_csv(evt_file2, sep='\\t')\n\n# create an EEG object\neeg2 = EEG(eeg_file2, chan_file2)\n\n# remove noise from EEG data\nremove_baseline_drift(eeg2)\nremove_emg(eeg2)\nremove_ac(eeg2, 50)\n\n# exclude cue events near blinks\n# get cue times\ncue_times2 = events2.loc[events2['trial_type'] == 'cue']['onset'].values\nblink_times2, _ = detect_blinks(eeg2, eog_chan='HEO')\nvalid_cue_times2, valid_percent2 = remove_blink_epochs(cue_times2, blink_times2, excl_period)\nprint('Valid cue times percentage: {}%'.format(valid_percent2))\n\n# get epochs\n# window settings\npre_window = 0.1\npost_window = 0.5\nepoch_dur = pre_window + post_window\nepoch_starts2 = valid_cue_times2 - 0.1\n\n# load epochs\ncue_data2, cue_t2, cue_ch2 = eeg2.get_data(chans='eeg', start_t=epoch_starts2, \n                       dur_t=epoch_dur, scale='relative')\n\n# Mean ERP across all sites\ncue_erp_all2 = np.mean(cue_data2, axis=(1,2))\nerp_max_idx2 = np.argmax(cue_erp_all2)\nerp_peak_time2 = cue_t2[erp_max_idx2]-pre_window\n\nfig, ax = plt.subplots(1,2, figsize=(10,5))\nax[0].plot(cue_t2-pre_window, cue_erp_all2)\nax[0].scatter(erp_peak_time2, np.max(cue_erp_all2), c='r')\n\nax[0].text(erp_peak_time2, np.max(cue_erp_all2), 'Peak time: {} ms'.format( \\\n         str(round(erp_peak_time2*1000))))\nax[0].set_xlabel('Time (sec)')\nax[0].set_ylabel('Voltage (uV)')\nax[0].set_title('Cue ERP averaged across all channels')\nax[0].grid(True)\n\n# Plot scalp topography of the ERP at the peak time\ncue_erp_ch2 = np.mean(cue_data2, axis=2)\ncue_scalp2 = cue_erp_ch2[erp_max_idx2, :]\neeg2.plot_scalp(ax=ax[1], colors=cue_scalp2)\nplt.colorbar(ax[1].collections[0], label='Amplitude (uV)')\n\n\n# Plot ERP for the channel with the strongest ERP\ncue_max_data, cue_max_t, cue_max_ch = eeg2.get_data(chans='O1', start_t=epoch_starts2, \n                       dur_t=epoch_dur, scale='relative')\ncue_max_erp = np.mean(cue_max_data, axis=2)\nfig, ax = plt.subplots(1,1, figsize=(10,5))\nax.plot(cue_t2, cue_max_data.squeeze(), color='k', alpha=0.2)\nax.plot(cue_max_t, cue_max_erp.squeeze(), color='r')\nax.set_xlabel('Time (sec)')\nax.set_ylabel('Voltage (uV)')\nax.set_title('Cue ERP for channel {}'.format(cue_max_ch))\nax.grid(True)\n\nValid cue times percentage: 100.0%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe time course of the ERP in this subject is substantially different from the previous one. Here we only see one prominent peak, and at a longer latency (~100 ms later). On the other hand, the scalp topography of that component is similar, with the strongest sites being occipital and parietal. The ERP does appear more lateralized, however, with the ERP stronger in the left hemisphere.\nLet’s not waste this hard work, we should try to save our data for use in Week 3. Fortunately, Numpy arrays can be saved as their own files.\n\nsave_dir = os.path.join(*['.', 'Week3', 'data', 'ds003690'])\n\n# Only write out the data if the directory doesn't exist\n# This is to prevent overwriting the data\nif not os.path.exists(save_dir):\n    os.mkdir(save_dir)\n    subj = 'AB58'\n    np.save(os.path.join(save_dir, subj + '_data.npy'), eeg.data)\n    np.save(os.path.join(save_dir, subj + '_srate.npy'), eeg.srate)\n    np.save(os.path.join(save_dir, subj + '_blinks.npy'), blink_times)\n    np.save(os.path.join(save_dir, subj + '_cues.npy'), cue_times)\n    eeg.chans.to_csv(os.path.join(save_dir, subj + '_chans.tsv'), sep='\\t')\n\n    subj = 'AB64'\n    np.save(os.path.join(save_dir, subj + '_data.npy'), eeg2.data)\n    np.save(os.path.join(save_dir, subj + '_srate.npy'), eeg2.srate)\n    np.save(os.path.join(save_dir, subj + '_blinks.npy'), blink_times2)\n    np.save(os.path.join(save_dir, subj + '_cues.npy'), cue_times2)\n    eeg2.chans.to_csv(os.path.join(save_dir, subj + '_chans.tsv'), sep='\\t')"
  },
  {
    "objectID": "Week1.html",
    "href": "Week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Welcome to the course Decoding the Brain. We will cover how to process electrical signals generated by neural activity, use machine learning approaches to infer the presentation of stimuli or an intended motor action from that activity, and evaluate the performance of these inferences.\nThis course is designed so that students gain a practical understanding of how to design the algorithms used for brain-computer interfaces (BCI). By the end of the course, you should understand enough to program your own BCI algorithms for non-invasive (scalp) and invasive (electrodes implanted in the brain) measures of brain activity. These activities can be weak electrical activity recorded from the scalp (EEG), high-frequency oscillations recorded on the brain surface (ECoG), and the firing of individual neurons (units). Decoding each of these types of signals requires several steps we will cover. First, you will be shown how to process and clean them for subsequent decoding. Then, we will cover the theory behind how the decoding algorithms work and code basic versions of them from scratch in using the Python packages Numpy or PyTorch. Next, we will expand their capabilities to handle more complex patterns of brain activity or decode multiple events. Finally, we will evaluate the decoder’s performance.\nExcept for cursory discussion, we will not cover the hardware, surgical, and biocompatibility issues of recording brain activity. Those topics depend upon fundamentally different skills from the design of BCI algorithms.\nThe two principal algorithms covered in this course, logistic regression and naive Bayes, are useful outside of BCIs. Thus, the understanding you get of them here will apply to many other domains."
  },
  {
    "objectID": "Week1.html#syllabus-and-class-structure",
    "href": "Week1.html#syllabus-and-class-structure",
    "title": "Week 1",
    "section": "",
    "text": "Welcome to the course Decoding the Brain. We will cover how to process electrical signals generated by neural activity, use machine learning approaches to infer the presentation of stimuli or an intended motor action from that activity, and evaluate the performance of these inferences.\nThis course is designed so that students gain a practical understanding of how to design the algorithms used for brain-computer interfaces (BCI). By the end of the course, you should understand enough to program your own BCI algorithms for non-invasive (scalp) and invasive (electrodes implanted in the brain) measures of brain activity. These activities can be weak electrical activity recorded from the scalp (EEG), high-frequency oscillations recorded on the brain surface (ECoG), and the firing of individual neurons (units). Decoding each of these types of signals requires several steps we will cover. First, you will be shown how to process and clean them for subsequent decoding. Then, we will cover the theory behind how the decoding algorithms work and code basic versions of them from scratch in using the Python packages Numpy or PyTorch. Next, we will expand their capabilities to handle more complex patterns of brain activity or decode multiple events. Finally, we will evaluate the decoder’s performance.\nExcept for cursory discussion, we will not cover the hardware, surgical, and biocompatibility issues of recording brain activity. Those topics depend upon fundamentally different skills from the design of BCI algorithms.\nThe two principal algorithms covered in this course, logistic regression and naive Bayes, are useful outside of BCIs. Thus, the understanding you get of them here will apply to many other domains."
  },
  {
    "objectID": "Week1.html#mixing-code-theory-and-practice",
    "href": "Week1.html#mixing-code-theory-and-practice",
    "title": "Week 1",
    "section": "Mixing code, theory, and practice",
    "text": "Mixing code, theory, and practice\nLectures will introduce the theory behind what we do, practical details of how to go about doing it, and code that does the doing. The astounding thing about BCI design is how accessible the algorithms and plentiful the data sources are. Of course, a real BCI system uses specialized hardware implanted in or on a person and is run in near real-time, which is not feasible in a classroom setting.\nWe will use Python throughout this course, and publicly available data sets.\n\n# This will be the standard set of packages we import for most lectures. \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.signal as sig\nfrom IPython.display import display, Math, Latex"
  },
  {
    "objectID": "Week1.html#general-principles-of-bci-design",
    "href": "Week1.html#general-principles-of-bci-design",
    "title": "Week 1",
    "section": "General principles of BCI design",
    "text": "General principles of BCI design\nThe goal of BCI is to attribute electrical activity in a subject’s brain to stimuli they received or actions they intended to produce. Grounding this is the fundamental assumption in neuroscience that all we experience and do arises from patterns of activity in the brain. These patterns are manifested at multiple levels, from single neurons to large portions of the cortical surface. Every stimulus produces a chain of activities starting at peripheral receptors, ascending through chains of neurons that eventually reach the neocortex, where they are processed and elaborated, linked with your expectations, memories, and goals. At every level specific groups of neurons are activated reflecting the properties of the stimuli or your cognitive state. Sometimes these trigger actions - e.g. moving your arm - that depends on a chain of activities stretching back from the muscles to your spinal cord, then brain stem, then cortex. Remarkably, in a mouse it only takes the activation of just over a dozen neurons to influence perception.\nGiven that there is a physical correlate in the brain of what we perceive and do, then it is possible to ‘tap’ them. The limitations are technical such as how well can be extract neural activities and our computational resources for attributing them to the external world. Much work has been spent on these problems. This is often conceptualized in two different ways.\n.\nOne is treat it as an encoding problem, where we try to understand how events in the environment are reflected in neural activities. The other is the decoding approach, where we use neural activities to infer what is happening in the environment. BCIs make extensive use of decoding analyses.\n\n\n\nBCI schema\n\n\nA BCI involves measuring some activity in the brain and then transforming it so that it can be used to infer whether a certain action was intended or stimulus occurred. The transformation, referred to as the decoding step in the above figure, involves three steps. 1. Signal preprocesing: The raw neural activity measurements are cleaned and filtered to remove non-neural artifacts and highlight those aspects of the signal that reflect the processing we care about. 2. Feature extraction: Patterns are identified in the activities that are related to the events we wish to decode. 3. Classification: Based on the detected patterns, a decision is made about whether, or what kind, of event occurred.\nFor each type of brain signal involves different preprocessing and feature extraction steps. Generally the classification step can be used interchangeably between signal types, but we will explore two fundamentally different approaches."
  },
  {
    "objectID": "Week1.html#neuroscience-basics",
    "href": "Week1.html#neuroscience-basics",
    "title": "Week 1",
    "section": "Neuroscience basics",
    "text": "Neuroscience basics\nThe brain is an electrochemical machine that allows vertebrates to interact with a dynamic complex environment. It does this using specialized cells called neurons. They have the ability to receive chemical signals, neurotransmitters, that affect their electrical activity. This activity is transmitted to other neurons via an axon. When multiple neurons are connected together they form a network that supports the transformation of incoming activity and spontaneous generation of new activities.\n\nBrain anatomy\nThe brain is composed of numerous regions, but its cortex is generally divided into four principal areas: \n\nFrontal lobe: Reasoning and motor control\nParietal lobe: Touch and visual spatial processing\nOccipital lobe: Low level visual processing\nTemporal lobe: Audition, high level visual processing, memory\n\nKeep in mind these are gross generalizations regarding the function of these areas.\n\n\n\nBrain cross section\n\n\nThe brain is situated in the skull and is separated by a couple centimeters from the surface of the scalp. Above it lies several layers of connective tissue (pia, dura, arachnoid) and cerebrospinal fluid that act as a protective cushion. The skull itself has multiple layers, followed by fatty tissue and the skin.\n\n\nNeurons\n\n\n\nNeuron schematic\n\n\nNeurons are the principal cell type in the brain for processing information. Neurons are tiny, ~10 microns, which is 5-10 times smaller than the diameter of a human hair. They are comprised of a soma (cell body), dendrites, and an axon. The dendrites take in signals from other neurons, integrate them, and convey them to the soma. When these signals exceed a threshold, the soma can generate an action potential that is conveyed down the axon towards other neurons.\n\n\nMembranes and ions\n Like all eukaryotic cells, neurons have a lipid membrane that seals in their internal, or intracellular, fluid (also known as cytoplasm) and organelles from the extracellular environment. This membrane also acts as an electrical insulator, blocking the flow of electrical charges across it. Electrical charge in the brain is carried by ions in the intracellular and extracellular fluids (cerebrospinal fluid). Ions are atoms or molecules that are either positively or negatively charged depending on whether they have a fewer or greater number of electrons than protons. The ones that are most determinative of neural activity are Na+, K+, Cl-, and Ca2+. Between the intracellular and extracellular fluids, the concentrations of these ions are different. Na+,Cl-, and Ca2+ have higher concentrations in the extracellular fluid, while K+ ions have a higher concentration in the intracellular fluid.\n\n\n\nMembrane and ions\n\n\n\nion_props = pd.DataFrame({'Ion': ['Na', 'K', 'Cl', 'Ca'],\n                          'Out_mM': [140, 3, 140, 1.5],\n                          'In_mM': [7, 140, 7, 0.0001],\n                          'Charge': [1, 1, -1, 2]})\nion_props.set_index('Ion', inplace=True)\n\nprint(ion_props)\n\n     Out_mM     In_mM  Charge\nIon                          \nNa    140.0    7.0000       1\nK       3.0  140.0000       1\nCl    140.0    7.0000      -1\nCa      1.5    0.0001       2\n\n\nThis unequal distribution of ions across the membrane is unstable because ions tend to flow from where they are highly concentrated to where they are less concentrated. To cross the membrane, ions flow through channels, unsurprisingly called ion channels. These are proteins that span the inside and outside of the neuron with a central hole, known as a pore, through which the ions flow. The pore’s shape and charge determine its selectivity for specific ionic species. Additional factors can control the opening of the pore. Some ion channel pores are controlled by the voltage across the membrane, others by substances such as neurotransmitters or hormones that bind to them, or even other ions like Mg2+.\n\n\n\nNa channel\n\n\nSince ions move from high to low concentrations, Na+, Cl-, and Ca2+ are inclined to flow into neurons. K+, on the other hand, wants to flow out of the neuron. As these ions flow through the membrane they build up on its surface. This create a force that opposes further flow from diffusion, because ions with the same sign to their charge tend to push away from each other. The work needed to move a charge from one place to another is measured as a voltage or potential. The potential at which an ion’s electrical forces from the buildup of ions balance out the diffusion force is referred to as the reversal or equilibrium potential, E. In the case here, we are measuring potential between the inside and outside of the neuron. We can calculate the strength of the potentials that reflect these drives using the Nernst equation.\nE = \\frac{RT}{zF}ln\\frac{[Out]}{[In]}\nR is the gas constant and F is Faraday’s constant, while T is temperature in Kelvin and z is the charge of the ion. [Out] and [In] are the extracellular and intracellular concentrations of that ion, respectively.\nMost of the time when an equation is introduced, we will try to translate it into Python. A method that calculates the equilibrium potential of a given ion can be given as:\n\n# calculate Nernst equilibrium potential for an ion\ndef nernst(conco=3, conci=140, z=1, t=310):\n    \"\"\"Calculate Nernst potential for an ion.\n    \n    Parameters\n    ----------\n    conco : float\n        Concentration of ion outside cell (mM). Default is for K+.\n    conci : float\n        Concentration of ion inside cell (mM). Default is for K+.\n    z : int\n        Charge of ion.  Default is for K+.\n    t : float\n        Temperature (Kelvin). Default is body temperature.\n    \n    Returns\n    -------\n    float\n        Nernst potential for ion (mV).\n    \"\"\"\n    r = 8.314 # J/mol/K, gas constant\n    f = 96485 # C/mol, Faraday's constant\n    return (r*t)/(z*f) * np.log(conco/conci) * 1000\n\nUsing this function and the dataframe of ionic concentrations above, we can calculate the equilibrium potentials for each of the major ionic species found in the nervous system.\n\nion_props['E'] = ion_props.apply(lambda ion: nernst(ion['Out_mM'], ion['In_mM'], ion['Charge']), axis=1)\nprint(ion_props)\n\n     Out_mM     In_mM  Charge           E\nIon                                      \nNa    140.0    7.0000       1   80.023015\nK       3.0  140.0000       1 -102.656323\nCl    140.0    7.0000      -1  -80.023015\nCa      1.5    0.0001       2  128.430326\n\n\nSeveral insights are readily apparent from cursory inspection of the calculated equilibrium potentials and consideration of the Nernst equation. First, and most importantly, positive ions that have a higher concentration outside the neuron will have positive potentials, while those more concentrated inside the neuron have negative potentials. This is stands out when comparing the potential of Na+, whose potential is 80 mV and K+, whose potential is -80 mV. Second, since we are taking the ratio between [Out] and [In], the overall amount of ions in our system does not affect the potential, only their relative proportion. Put another way, you could decrease the concentration of Na+ ions inside and outside the neuron by ten times and as long as they have the same proportion, the potential will be unchanged. Indeed, Ca2+ has a much lower overall concentration inside and outside the membrane, but its reversal potential is higher than that for Na+. Third, if an ion has a negative charge, z, it flips the sign of the potential. For instance, Cl- ions have a higher concentration outside the neuron, just like Na+, but their potential is negative.\nThe potential across a neuron’s membrane is the mean of all these potentials, each weighted by how easy it is for the ion to cross the membrane, which is related to the number of open ion channels allowing that ion to flow. In a neuron at rest, mostly K+ ion channels are open, so its reversal potential dominates. Neurons in the cerebral cortex usually have a resting potential around -65 mV."
  },
  {
    "objectID": "Week1.html#electrical-model-of-the-neuronal-membrane",
    "href": "Week1.html#electrical-model-of-the-neuronal-membrane",
    "title": "Week 1",
    "section": "Electrical model of the neuronal membrane",
    "text": "Electrical model of the neuronal membrane\nThe electrical behavior of the membrane can be approximated by a relatively simple circuit. While this is not an electronics course, I will briefly describe this model and use it to construct a simulated neuron. This should allow us to explore some of the response properties of neurons and how they are reflected in the electrical activities detected with electrodes.\n\nEquilibrium potential as a battery and conductor\nThe equilibrium potential, E_{r}, behaves like a battery. The ionic current it produces flows through open ion channels, which we will refer to as conductors. As we discussed above, when a neuron is at rest it primarily allows K+ ions to flow. This passive leakage of ions across the membrane has given the conductance the name leak conductance. This can be schematized below as a battery and conductor in series that bridge the intracellular and extracellular spaces.\n\n\n\nMembrane with battery\n\n\nBy itself, this circuit will fix the voltage across the neuronal membrane (measured between the inside and outside) at the resting potential. Normally neurons do not remain at rest, but have a ongoing fluctuations in their membrane voltage due to the thousands of synaptic inputs they recieve. This means that the membrane potential, V_{m}, can be different from E_{r}. When V_{m} is different from E_{r}, there is a net flow of current across the membrane, since the electrical and diffusion forces are no longer balanced (think back to the Nernst equation). The ionic current can be described using Ohm’s law:\n V = IR \nwhere V is voltage (volts, V), I is current (amperes, A), and R is resistance (ohms, \\Omega). Rewriting the equation to solve for I gives us:  I = \\frac{V}{R}  The inverse of resistance is conductance (siemens, S), so this equation can be rewritten as:  I = gV  Here g is the ease with which our conductor allows current to flow. Now, if we include the effect of the reversal potential, E_{r} acting as a battery, then the equation can be written as:  I_{leak} = g_{leak}(V_{m}-E_{r}) \\tag{1}  Let’s consider the behavior of this equation. It shows that as we increase the ion channel’s conductance, g_{leak}, the current will increase as well. It also shows that the magnitude and direction of the current will depend on the membrane voltage, V_{m}. If V_{m} is below E_{r}, current will be negative. If it is above, then it will be positive. As V_{m} approaches E_{r}, the current gets smaller. When they are equal, no current flows because the electrical forces perfectly balance the concentration gradient.\nLet’s graph these relationships between V_{m} and I_{leak} by systematically varying E_{r} and g_{leak}.\n\n# Here are somme realistic parameters for a neuron\nmem_r = 25e3 # ohm*cm^2, taken from Egger et al 2020\ncell_radius = 30e-4 # cm, 30 microns, this is larger than the cell body itself to account for membrane from dendrites\ncell_area = 4*np.pi*cell_radius**2 # cm^2\ng_leak = (1/mem_r) * cell_area # S/cm^2\ne_rest = -0.065 # V, 65 mV\n\n# code for solving for leak current\ndef ionic_current(v=0, g=g_leak, e=e_rest):\n    \"\"\"Calculate ionic current.\n    \n    Parameters\n    ----------\n    v : float\n        Membrane potential (V). Default is 0.\n    g : float\n        Conductance (S). Default is leak conductance.\n    e : float\n        Equilibrium potential (V). Default is resting potential.\n    \n    Returns\n    -------\n    float\n        Ionic current (A).\n    \"\"\"\n    return g * (v - e)\n\n\n# Examine how varying the conductance affects the I-V curve\nv = np.linspace(-0.1, 0.1, 100)\ng_factors = [0.1, 0.5, 1, 2, 5]\nfor ind, curr_factor in enumerate(g_factors):\n    curr_g = g_leak*curr_factor\n    plt.plot(v*1e3, ionic_current(v, g=curr_g)*1e12, \n             label=round(curr_g*1e12), color=[ind/len(g_factors), 0,0])\n    plt.text(v[-1]*1e3+2, ionic_current(v[-1], g=curr_g)*1e12,\n                str(round(curr_g*1e12))+' pS', color=[ind/len(g_factors), 0,0])\nplt.xlim(-100, 135)\nplt.xlabel('Membrane potential (mV)')\nplt.ylabel('Ionic current (pA)')\nplt.grid()\nplt.title('Effect of conductance on ionic current')\n\nText(0.5, 1.0, 'Effect of conductance on ionic current')\n\n\n\n\n\n\n\n\n\nIt is evident that increasing the conductance dramatically increases the ionic current. Now let’s examine the effect of equilibrium potential.\n\n# Examine how varying the equilibrium potential affects the I-V curve\nv = np.linspace(-0.1, 0.1, 100)\ne_values = [-0.070, -0.035, 0, 0.035, 0.070]\nfor ind, curr_e in enumerate(e_values):\n    plt.plot(v*1e3, ionic_current(v, e=curr_e)*1e12, \n             label=curr_e*1e3, color=[0,0,ind/len(e_values)])\n    plt.text(v[-1]*1e3+2, ionic_current(v[-1], e=curr_e)*1e12,\n                str(round(curr_e*1e3))+' mV', color=[0,0,ind/len(e_values)])\nplt.xlim([-100, 135])\nplt.xlabel('Membrane potential (mV)')\nplt.ylabel('Ionic current (pA)')\nplt.grid()\nplt.title('Effect of reversal potential on ionic current')\n\nText(0.5, 1.0, 'Effect of reversal potential on ionic current')\n\n\n\n\n\n\n\n\n\nAs we shift the value of the equilibrium potential, the voltage at which the ionic current shifts from negative to positive shifts in tandem.\nBy itself, this circuit does not produce any interesting temporal dynamics or integrative abilities. To begin to capture those, we need to incorporate another detail about the electrical properties of the neuronal membrane.\n\n\nMembrane as a capacitor\nThe neuron’s membrane is conceived of as a capacitor (see this paper for more info). A capacitor is composed of an insulator sandwiched between two conductors, in our case the lipid membrane acts as the insulator since charge cannot cross it, and the extracellular and intracellular ionic solutions are the conductors. Since each charge puts out an electric field attracts the opposite charge and repels the same charge, and this field decays with distance from the charge, the thinner the insulator is the stronger charges on either side of it can influence each other. If positive charges build up on one side of the capacitor, then they will draw negative charges to build up on the opposite side. The ability of a capacitor to store charge is quantified by its capacitance:\n C = \\frac{\\epsilon\\epsilon_{0}A}{d} \nwhere \\epsilon is the dielectric constant of the insulation material, \\epsilon_{0} is the polarizability of free space, A is the surface area of the capacitor, and d is the thickness of the insulator. Capacitance is measured in the unit Farads (F). This equation tells us that to increase capacitance one should enlarge the surface area of the capacitor, allowing more space to accommodate charge, and shrink the thickness of the insulator, making it easier for charges on either side to interact.\nWhat value does the capacitance take for neurons? Since all membranes are composed of a lipid bilayer, they do not differ in their material composition so their \\epsilon stays the same, and \\epsilon_{0} is a physical constant that does not change. The thickness of the neuronal membrane, d, our insulator, is also consistent across neurons, with a value of ~9 nm (for comparison, that is about eleven thousand times thinner than the thickness of printer paper). But, since neurons can vary in size, A varies greatly across neurons. So, we often use ‘specific capacitance’, which is the ratio between capacitance and area. The specific capacitance for neurons is generally around 1 𝜇F/cm2. If we approximate a neuron as a sphere, then its capacitance can be calculated using its radius to calculate the area of the sphere (A=4 \\pi r^2), and multiplying that by the 1 𝜇F/cm2. For instance, a neuron with a radius of 10 um has a capacitance of 12.5 pF.\n\n\nPassive electrical model of the membrane\n\n\n\nmembrane as capacitor\n\n\nExpanding our electrical schematic we add the membrane capacitor in parallel with the resting potential battery and conductor. This is the passive electrical model of the membrane. Many classic phenomena of neural integration arise from the passive electrical behavior of its membrane.\n\n\n\nmembrane rest\n\n\nThe battery charges up the capacitor, forcing it to adopt a potential equal to the resting potential. Once it has reached this stable state, what if we were to artificially inject a current into the neuron? To calculate this, we can use a rule from electrical theory called Kirchoff’s current law (see Kirchoff’s current law). It states that the current flowing into a node of an electrical circuit must equal the current flowing out of it. We already have an equation (Ohm’s law) that describes the current produced by our ionic leak current, now we need to know how to calculate the current produced by the capacitor.\nThe relationship between current and voltage for a capacitor is described by the equation:\n I = C\\frac{dV}{dt} \\tag{2}\nWhat this means is that the current produced across the capacitor is proportional to the change in voltage.\n\n\n\nmembrane charging with resistance\n\n\nThe passive electrical circuit model is also known as an RC circuit, because it contains a resistor and capacitor. What is its behavior? When it is first put together, there is no voltage across the capacitor, so the battery will charge it up. However, the resistor places a limit on the current, meaning the capacitor will not fully charge instantly. To determine the trajectory the voltage will take, we can model this with an equation by combining equations 1 and 2 using Kirchoff’s current law. For the circuit above, this yields the equation:  \\begin{align}\n    \\notag\n    0 &= I_{rest} + I_{C} \\\\ \\notag\n    0 &=g_{m}(V_{m}-E_{rest}) + C_{m}\\frac{dV_{m}}{dt} \\\\ \\notag\n    -C\\frac{dV_{m}}{dt}&=g_{m}(V_{m}-E_{rest}) \\\\ \\notag\n    \\frac{dV_{m}}{dt}&=-\\frac{g_{m}}{C}(V_{m}-E_{rest})\n    \\end{align}\n\nThis is a differential equation that can be solved to give voltage as a function of time:  V_{m}(t) = E_{rest}(1-e^{\\frac{-t}{\\frac{g_{m}}{C_{m}}}}) \nRewriting g_{m}, as its inverse, referred to as membrane resistance, R_{m}, we get the equation:  V_{m}(t) = E_{rest}(1-e^{\\frac{-t}{R_{m}C_{m}}}) \\tag{3}\nThe product of R_{m} and C_{m} sets how fast the membrane charges. Larger it is, the slower the membrane capacitor will charge, and the smaller it is, the faster it charges. Given realistic values of R_{m} and C_{m}, what would be a reasonable time constant to expect from a neuron?\n\n# Calculate membrane resistance and capacitance\nmem_cap = 1e-6 # F/cm^2\ncell_c = mem_cap * cell_area # F\ncell_r = mem_r / cell_area # ohm\n\ncell_tau = cell_r * cell_c # sec\n\nprint('Membrane capacitance: {:.0f} pF'.format(cell_c*1e12))\nprint('Membrane resistance: {:.0f} MOhm'.format(cell_r*1e-6))\nprint('Membrane time constant: {:.0f} ms'.format(cell_tau*1e3))\n\nMembrane capacitance: 113 pF\nMembrane resistance: 221 MOhm\nMembrane time constant: 25 ms\n\n\n\n# Plot examples of V changing over time for different RC time constants\ntau_factors = [0.1, 0.5, 1, 2, 5]\nt = np.linspace(0, 0.1, 100)\nfor ind, curr_factor in enumerate(tau_factors):\n    curr_tau = cell_tau*curr_factor\n    plt.plot(t*1e3, e_rest*(1-np.exp(-t/curr_tau))*1e3, \n             label=curr_tau*1e3, color=[0,0,ind/len(tau_factors)])\n    plt.text(t[-1]*1e3+2, e_rest*(1-np.exp(-t[-1]/curr_tau))*1e3,\n                str(round(curr_tau*1e3,2))+' ms', color=[0,0,ind/len(tau_factors)])\n    \n    \nplt.xlim([-5, 125])\nplt.xlabel('Time (ms)')\nplt.ylabel('Membrane potential (mV)')\nplt.grid()\nplt.title('Effect of RC time constant on membrane potential charging')\n\nText(0.5, 1.0, 'Effect of RC time constant on membrane potential charging')\n\n\n\n\n\n\n\n\n\nWhat this shows is that longer membrane time constants, \\tau, slow the rate of change in the membrane potential.\n\n\nCreating a passive neuron model\nEquation 3 showed us the relationship between time and membrane potential at the moment the membrane capacitance was connected with the leak current. This is, of course, not realistic. Instead, neurons start at the resting potential and recieve occasional inputs that inject currents into the neuron. To capture this, we can use the following equation:\n \\begin{align}\n    \\notag\n    0 &= I_{rest} + I_{C} + I_{in}\\\\ \\notag\n    0 &=g_{m}(V_{m}-E_{rest}) + C_{m}\\frac{dV_{m}}{dt} + I_{in}\\\\ \\notag\n    -C_{m}\\frac{dV_{m}}{dt}&=g_{m}(V_{m}-E_{rest}) + I_{in}\\\\ \\notag\n    \\frac{dV_{m}}{dt}&=-\\frac{1}{C_{m}}({g_{m}}(V_{m}-E_{rest}) + I_{in})  \\tag{4}\n    \\end{align}\n\nSince we know how the membrane voltage will change, \\frac{dV_{m}}{dt}, based on its present value, V_{m}, and the current injected, I_{in}, we can simulate its response. To do this, we will create a class, PassiveNeuron. This class will encapsulate the data and methods needed to simulate the passive properties of a neuron’s membrane.\n\nclass PassiveNeuron:\n    def __init__(self, v_rest=-65, c_m=1, r_m=25, radius=30):\n        self._vrest = v_rest/1000 # mV\n        self._cm = c_m * 1e-6 # uF/cm^2\n        self._rm = r_m * 1e3 # kOhm*cm^2\n        self._radius = radius * 1e-4 # cm\n\n        self._area = 4 * np.pi * self._radius**2 # cm^2\n        self._c = self._cm * self._area # F  \n        self._gleak = 1 / (self._rm / self._area) # S\n\n        self._vm = self._vrest\n        self._im = 0\n        self._add = 0 # holds additional current to deliver to the neuron\n        self._dt = 0.0001 # sec\n\n    def get_tau(self):\n        return self._cm * self._rm\n    \n    def set_input(self, inp=0):\n        self._add = inp\n    \n    def reset_state(self):\n        self._vm = self._vrest\n        self._im = 0\n        self._add = 0 # holds additional current to deliver to the neuron\n    \n    def get_state(self):\n        # return membrane potential in mV, membrane current in nA, and spike status\n        return self._vm*1e3, self._im*1e9\n    \n    def get_t_vec(self, dur = 0.1, dt = 0.0001):\n        return np.arange(0, dur, dt)\n    \n    def update(self):\n        # solve for transmembrane currents\n        self._im = (self._gleak * (self._vm - self._vrest)) + self._add\n        \n        # update membrane potential\n        self._vm = self._vm + -(self._im / self._c) * self._dt\n    \n    def run(self, dur=0.1, dt=0.0001, inp=0):\n\n        self.reset_state() # reset state\n        \n        # initialize arrays to store values\n        t = self.get_t_vec(dur, dt) # time array\n        v = np.zeros(len(t)) # voltage array\n        i = np.zeros(len(t)) # current array\n        self._dt = dt # set time step\n\n        # if input is scalar, make it an array\n        if isinstance(inp, (int, float)):\n            inp = np.ones(len(t)) * inp\n        \n        # run simulation\n        for ind, _ in enumerate(t):\n            self.set_input(inp[ind]) # set input current\n            self.update() # update membrane potential and current\n            v[ind], i[ind] = self.get_state() # store values\n\n        return v, i\n\n\n# Create a neuron with default parameters\npas_nrn = PassiveNeuron()\nsim_dur = 0.2 # sec\nsim_dt = 0.0001 # sec\nt_pts = pas_nrn.get_t_vec(dur=sim_dur, dt=sim_dt)\n\n# Create a step current input\ndef step_current(t, start=0.025, end=0.125, amp=-1e-10):\n    step = np.zeros(len(t))\n    step[np.where((t&gt;=start)&(t&lt;end))[0]] = amp\n    return step\n\nin_step = step_current(t_pts)\npas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)\n\ndef plot_sim(t, v, i, inp, title=''):\n    plt.subplot(3,1,1)\n    plt.plot(t*1e3, v, color='b')\n    plt.yticks(color='b')\n    plt.ylabel('mV', color='b')\n    plt.grid()\n    plt.title('Membrane potential')\n\n    plt.subplot(3,1,2)\n    plt.plot(t*1e3, i, color='r')\n    plt.yticks(color='r')\n    plt.ylabel('nA', color='r')\n    plt.title('Membrane current')\n    plt.grid()\n\n    plt.subplot(3,1,3)\n    plt.plot(t*1e3, inp*1e9, color='k')\n    plt.xlabel('Time (ms)')\n    plt.yticks(color='k')\n    plt.ylabel('nA', color='k')\n    plt.title('Input current')\n    plt.grid()\n    plt.suptitle(title)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\nplot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron')\n\n\n\n\n\n\n\n\nWhen we inject a current step into the neuron, the membrane begins to depolarize from its resting potential towards a new voltage. This is accompanied by an strong current crossing the membrane, which reflects the injected current charging up the membrane capacitance. As the membrane capacitance charges up, this current decays because it is offset by the leak current, which flows in the opposite direction of the injected current. The leak current is driven by the difference between V_{m} and E_{r}. For the leak current to balance out the injected current, the membrane potential must reach:\n V_{m} = \\frac{I_{in}}{g_{leak}}+E_{r}\nThus, a increasing g_{leak} (or lowering membrane resistance, R_{m}), lowers the change in membrane potential to an injected current. We can see this here:\n\npas_nrn_hig = PassiveNeuron(v_rest=-65, c_m=1, r_m=5, radius=30) # we lowered the membrane resistance, r_m, which increased the membrane conductance, g_m\npas_v_hig, pas_i_hig = pas_nrn_hig.run(dur=sim_dur, dt=sim_dt, inp=in_step)\nplot_sim(t_pts, pas_v_hig, pas_i_hig, in_step, title='Passive neuron with higher $g_{leak}$')\n\n# Get values of g_leak for each model\nprint('g_leak for regular model: {:.0f} nS'.format(pas_nrn._gleak*1e9))\nprint('g_leak for high g model: {:.0f} nS'.format(pas_nrn_hig._gleak*1e9))\n\ng_leak for regular model: 5 nS\ng_leak for high g model: 23 nS\n\n\n\n\n\n\n\n\n\nIncreasing g_{leak} lowered the change in membrane potential from around 12 mV to 3 mV. Notice also that V_{m} changes much faster now, because the membrane time constant is shorter.\nIncreasing the amount of current we inject also increases the change in V_{m}.\n\nin_step = step_current(t_pts, amp=-2e-10) # increase input current by 2x\npas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)\nplot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron with $2xI_{in}$')\n\n\n\n\n\n\n\n\nSo far we have been giving negative current injections, which pushes the membrane towards positive values. If V_{m} moves towards positive values, we say it is depolarizing. By contrast, if it is pushed towards negative values, it is hyperpolarizing. To hyperpolarize, we inject a positive current.\n\n# hyperpolarize the membrane potential\nin_step = step_current(t_pts, amp=1e-10)\npas_v, pas_i = pas_nrn.run(dur=sim_dur, dt=sim_dt, inp=in_step)\nplot_sim(t_pts, pas_v, pas_i, in_step, title='Passive neuron with being hyperpolarized')\n\n\n\n\n\n\n\n\nAll the activity we have seen so far is slow changes in membrane potential. These are not conveyed to downstream neurons. Instead, neurons have to fire action potentials to influence their neighbors.\n\n\nAction potentials\nAction potentials are large positive excursions of the membrane potential. They last less than 2 ms and propagate from the cell body down the axon, eventually triggering release of neurotransmitter at the synapse. They are the principal output of neurons and, for the most part, the only information about the state of a neuron that is conveyed to the rest of the network.\n\n\n\nmembrane with action potential\n\n\nTo generate an action potential, a neuron is first depolarized to a sufficient level that starts to open voltage-gated Na+ ion channels. Recalling that Na+ has a positive reversal potential, their opening will depolarize the membrane voltage. This depolarization recruits more voltage-gated Na+ ion channels, reinforcing the depolarization and driving the membrane potential towards the Na+ reversal potential (~40 mV). Then, these channels begin to inactivate, closing and bringing the membrane back towards its resting potential. This fall back towards rest is accelerated by voltage-dependent K+ channels, which open during the depolarization of the action potential and drive the membrane towards the K+ reversal potential (~-70 mV). During this period, the neuron enters a refractory period. The first phase of this is absolute, where the neuron cannot fire an action potential due to the inactivation of Na+ channels. This is followed by a relative period, where the an action potential can be generated again, but the K+ current counteracts this, effectively increasing the amount of current required to cross the voltage threshold.\nThe voltage-gated Na+ and K+ currents are similar in principal to the leak current discussed earlier. They are a battery, the equilibrium potential of Na+ or K+, in series with a conductor. The difference is that the conductor depends on V_{m} and time. We can expand our electrical model of the passive neuron to incorporate these voltage-gated currents, thus allowing us to model action potentials.\nThe equation that captures this is:\n \\frac{dV_{m}}{dt}=-\\frac{1}{C}(g_{m}(V_{m}-E_{rest}) + g_{Na_{V,t}}(V_{m}-E_{Na}) + g_{K_{V,t}}(V_{m}-E_{K}) + I_{in}) \\tag{5} \nHere g_{Na_{V,t}} is the voltage and time dependent conductance for Na+, and similar for g_{K_{V,t}}. The driving force for those currents depends on how far V_{m} is from the respective equilibrium potentials of those ions. In essence, increasing g_{Na_{V,t}} or g_{K_{V,t}} pulls V_{m} towards the corresponding equilibrium potential.\n\n\n\n\n\n\nA note about coding and neuronal firing\n\n\n\nAction potentials are often described as all-or-none events. Either they occur or they do not, and when they do, they tend to always have the same strength and duration. Since they are stereotyped, it is thought that they convey information by their timing. Several different terms are often used to describe the codes offered by action potentials. A time code is when the exact time of an action potential indicates the occurrence of some event (whether it be environmental or behavioral). This is often counterposed to a rate code, where the informational signal is how many action potentials occurred in some short amount of time, with a higher rate indicating a ‘stronger’ signal. Alternatively, a population code uses the set of neurons firing action potentials at any given moment to encode information. Different ensembles of activated neurons signal different events.\n\n\n\n\nCreating a simplified active neuron model\nWhen we include voltage-gated ion channels in a model neuron, it goes from being a passive model to an active model. We can build upon our existing PassiveNeuron class to create an ActiveNeuron class that generates an approximation of action potentials. To do this, we will add the voltage-dependent Na+ and K+ currents. This requires specifying the equilibrium potential for those ions, a V_{m} threshold that initiates the action potential, and designing voltage- and time-dependent conductances to gate the active currents.\n\n# create active neuron class that inheriting from the passive neuron model\nclass ActiveNeuron(PassiveNeuron):\n    def __init__(self, v_thresh=-50, ena=50, ek=-90, gna=8e-8, gk=4e-8, **kwargs):\n        super().__init__(**kwargs) # allows us to pass arguments for the passive properties of the neuron\n        self._vthresh = v_thresh / 1000 # mV\n        self._ena = ena / 1000 # mV\n        self._gna = gna # S\n        self._ek = ek / 1000 # mV\n        self._gk = gk # S\n        self._spk_timer = 0\n        self._spk = False\n    \n    def reset_state(self):\n        super().reset_state()\n        self._spk_timer = 0\n        self._spk = False\n\n    def gen_ap(self):\n        # action potential mechanism\n        if (self._vm &gt; self._vthresh) & (self._spk_timer &lt;= 0):\n            self._spk_timer = 0.004 # start countdown timer for duration of action potential\n            self._spk = True\n        elif self._spk_timer &gt; 0.003: # open up sodium conductance for first 1 ms\n            self._add = self._add + self._gna * (self._vm - self._ena)\n            self._spk = False\n            self._spk_timer -= self._dt\n        elif self._spk_timer &gt; 0: # open up potassium conductance for next 3 ms\n            self._add = self._add + self._gk * (self._vm - self._ek)\n            self._spk = False\n            self._spk_timer -= self._dt\n\n    def run(self, dur=0.1, dt=0.0001, inp=0):\n\n        self.reset_state() # reset state\n        \n        # initialize arrays to store values\n        t = self.get_t_vec(dur, dt) # time array\n        v = np.zeros(len(t)) # voltage array\n        i = np.zeros(len(t)) # current array\n        self._dt = dt # set time step\n\n        # if input is scalar, make it an array\n        if isinstance(inp, (int, float)):\n            inp = np.ones(len(t)) * inp\n        \n        # run simulation\n        for ind, _ in enumerate(t):\n            self.set_input(inp[ind]) # set input current\n            self.gen_ap() # generate action potential &lt;-- NEW\n            self.update() # update membrane potential and current\n            v[ind], i[ind] = self.get_state() # store values\n\n        return v, i\n\n\n# Create an active neuron and deliver a subthreshold and over threshold current\nact_nrn = ActiveNeuron()\nsim_dur_act = 0.5\nt_pts = act_nrn.get_t_vec(dur=sim_dur_act, dt=sim_dt)\nin_step_act = step_current(t_pts, start=0.1, end=0.2, amp=-5e-11) + \\\n                step_current(t_pts, start=0.3, end=0.4, amp=-1e-10)\n\nact_v, act_i = act_nrn.run(dur=sim_dur_act, dt=sim_dt, inp=in_step_act)\nplot_sim(t_pts, act_v, act_i, in_step_act, title='Active neuron with subthreshold and overthreshold current')\n\n\n\n\n\n\n\n\nFor this model we set the action potential threshold to -50 mV, and if we depolarize below that level we get the passive subthreshold depolarization that we saw in the previous model. If the current step is increased, then the membrane depolarizes passively until the threshold is reached, at which point an action potential is triggered. The action potential voltage approaches the equilibrium potential for Na+, and then quickly reverses towards the equilibrium potential of K+. The transmembrane currents associated with the action potential are substantially greater than those from the subthreshold depolarization. This is due to the very high conductance of the voltage-gated channels (Na+ channel: 80 nS vs. g_{leak}: 5 nS).\nMore than one action potential is emitted during the 100 ms over-threshold current step. The time between action potentials depends on two factors. First is the refractory period, with a stronger or longer refractory period delaying the time to the next action potential. The other is the strength of the injected current, with a stronger current able to overcome the refractory effect and elicit another action potential sooner. We can explore the effect of current strength by delivering a series of current pulses to our model with increasing amplitudes.\n\n# Examine how firing rate varies with curent injected\nact_nrn = ActiveNeuron()\nsim_dur_act = 1.1\nt_pts = act_nrn.get_t_vec(dur=sim_dur_act, dt=sim_dt)\n\n# create a series of step currents with increasing amplitudes\nin_step_act = step_current(t_pts, start=0.1, end=0.2, amp=-5e-11) + \\\n                step_current(t_pts, start=0.3, end=0.4, amp=-8e-11) + \\\n                step_current(t_pts, start=0.5, end=0.6, amp=-1.6e-10) + \\\n                step_current(t_pts, start=0.7, end=0.8, amp=-2.4e-10) + \\\n                step_current(t_pts, start=0.9, end=1.0, amp=-3.2e-10)\n\nact_v, act_i = act_nrn.run(dur=sim_dur_act, dt=sim_dt, inp=in_step_act)\nplot_sim(t_pts, act_v, act_i, in_step_act, title='Reponse to increasing current steps')\n\n\n\n\n\n\n\n\nIncreasing the current step amplitude increased the number of action potentials emitted, and shortened the time between them. Thus, the rate of action potential generation is proportional to the excitatory drive the neuron receives.\nHowever, it is important to remember that neurons are not normally driven by artificial current steps. Instead, they have synapses that are driven by the action potentials from other neurons.\n\n\nSynapses\nSince neurons form networks that share electrical signals, there must be a means for these signals to be passed from one neuron to another. This exchange occurs at synapses, where the axon from the presynaptic neuron forms a terminal on the dendrite of the postsynaptic neuron. Between them is a narrow space called the synaptic cleft, where neurotransmitters released by the presynaptic terminal crosses to binds to ion channels on the postsynaptic neuron. When the neurotransmitter binds it opens an ion channel, which allows a current composed of the ions that channel is permeable to to flow. Channels that are found at excitatory synapses, which drive the membrane potential towards positive values (depolarize), tend to be permeable positive ions, Na+, K+, and Ca2+. Inhibitory synapses rely on ion channels selective for the negative Cl- ion and can counteract the excitatory depolarization.\nCortical neurons are normally bombarded by an ongoing stream of excitatory and inhibitory synaptic activity. Usually these are balanced, so that the membrane potential shows only small changes in its level. However, if the excitatory synaptic drive overwhelms the inhibition it can push the membrane potential towards positive values and potentially trigger an action potential.\nThe ion channels that support synaptic transmission can be incorporated in our circuit model as just another branch, with an equilibrium potential reflecting their ionic permeability and a conductance that depends on number of open ion channels.\n\n\n\nMembrane with synapse\n\n\nAs for the leak and action potential related currents, the ionic current arising from synaptic transmission has the form:  I_{syn} = g_{syn}r(V_{m}-E_{syn})\nHere g_{Syn} is the total possible conductance, and r is the proportion of synaptic ion channels that are open. This depends on the presence of neurotransmitter in the synaptic cleft and time, and can be modeled as a reaction system, with separate rates describing the binding and dissociating of neurotransmitter to the receptor.\n\n\n\n\n\n\nModeling a synaptic current\n\n\n\nThe shape of the ionic current arising from activating a synapse is described as having an \\alpha- or double exponential shape. Our passive neuron model has a single exponential shape: there is only one time constant, \\tau, that describes the timing of its rise and fall in response to a current input. For a double exponential, there are two time constants, one for the rise and a different one for the fall. The time constant for the rising phase is faster than that for the falling phase, leading to a skewed response. The differential equation that captures this is:\n \\frac{dr}{dt} = \\alpha[T]r - \\beta(1-r) \nThe equation states that the change in the proportion of open ion channels, r, increases at a rate \\alpha when neurotransmitter, [T], is present in the synapse. When neurotransmitter is absent from the synapse, r then decreases at the rate of \\beta. (For more details, see Destexhe, Mainen & Sejnowski 1994).\n\n\nWe can incorporate synapses into our active model to explore they affect the membrane potential and drive action potentials.\n\n# create an active neuron with synapse class that inherits from the active neuron model\nclass SynapticNeuron(ActiveNeuron):\n    def __init__(self, esyn=50, gsyn=5e-9, asyn=900, bsyn=500, tdur=3, **kwargs):\n        super().__init__(**kwargs) # allows us to pass arguments for the passive properties of the neuron\n        self._esyn = esyn / 1000 # mV\n        self._gsyn = gsyn # S\n        self._asyn = asyn \n        self._bsyn = bsyn\n        self._tdur = tdur / 1000 # ms, duration of transmitter release\n        self._syn_timer = 0 # timer for transmitter release\n        self._r = 0 # fraction of open channels\n\n    def reset_state(self):\n        super().reset_state()\n        self._syn_timer = 0 # timer for transmitter release\n        self._r = 0 # fraction of open channels\n\n    def gen_syn(self, prespk=False):\n        # synaptic mechanism\n        if prespk:\n            self._syn_timer = self._tdur\n        elif self._syn_timer &gt; 0:\n            self._syn_timer -= self._dt\n        \n        # update fraction of open channels\n        self._r = self._r + self._dt * ((self._asyn * (self._syn_timer&gt;0) * (1-self._r)) \\\n                                        - (self._bsyn * (self._r)))\n        \n        # add synaptic current\n        self._add = self._add + self._gsyn * self._r * (self._vm - self._esyn)\n\n    def run(self, dur=0.1, dt=0.0001, inp=0, prespk=False): # added prespk to drive synapse\n\n        self.reset_state() # reset state\n        \n        # initialize arrays to store values\n        t = self.get_t_vec(dur, dt) # time array\n        v = np.zeros(len(t)) # voltage array\n        i = np.zeros(len(t)) # current array\n        self._dt = dt # set time step\n\n        # if input is scalar, make it an array\n        if isinstance(inp, (int, float)):\n            inp = np.ones(len(t)) * inp\n\n        if isinstance(prespk, (bool, int, float)):\n            prespk = np.ones(len(t)) * prespk\n        \n        # run simulation\n        for ind, _ in enumerate(t):\n            self.set_input(inp[ind]) # set input current\n            self.gen_syn(prespk[ind]) # generate synaptic response &lt;-- NEW\n            self.gen_ap() # generate action potential\n            self.update() # update membrane potential and current\n            v[ind], i[ind] = self.get_state() # store values\n\n        return v, i\n\n\n# MODIFY PLOTTING CODE HERE\ndef plot_sim_prespk(t, v, i, inp, spk, title=''):\n\n    # ensure neuron inputs have same length as time vector\n    if isinstance(inp, (int, float)):\n        inp = np.ones(len(t)) * inp\n    \n    if isinstance(spk, (bool, int, float)):\n        spk = np.zeros(len(t))\n    \n    plt.subplot(2,1,1)\n    plt.plot(t*1e3, v, color='b')\n    plt.vlines(t[spk==1]*1e3, plt.ylim()[0], plt.ylim()[1], color='k')\n    plt.yticks(color='b')\n    plt.ylabel('mV', color='b')\n    plt.grid()\n    plt.title('Membrane potential')\n\n    plt.subplot(2,1,2)\n    plt.plot(t*1e3, i, color='r')\n    plt.vlines(t[spk==1]*1e3, plt.ylim()[0], plt.ylim()[1], color='k')\n    plt.yticks(color='r')\n    plt.ylabel('nA', color='r')\n    plt.title('Membrane current')\n    plt.grid()\n\n    plt.suptitle(title)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n# Function for generating spike trains\ndef spk_train(t, spk_times=0.1):\n    spk_times = np.array([spk_times]) # ensures spk_times is a numpy array\n    spk = np.zeros(len(t))\n    spk[np.round(spk_times / sim_dt).astype(int)] = 1\n    return spk\n\n\n# examine response to a single presynaptic spike\nsyn_nrn = SynapticNeuron()\nsim_dur_syn = 0.2\nt_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)\n\nin_prespk = spk_train(t_pts, [0.05])\n\nsyn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)\nplot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to a single presynaptic spike')\n\n\n\n\n\n\n\n\nActivating the synapse at 50 ms leads to a rapid negative current that charges the membrane capacitance and depolarizes V_{m}. This is soon followed by a decay in the synaptic current, as the proportion of synaptic ion channels that were open shrinks. Within ~10 ms the synaptic current is finished, and replaced by a positive leak current, which is the membrane returning to its resting potential. The rate of this return to rest is determined by the membrane time constant.\nA single synaptic input is usually insufficient to drive an action potential. Typically multiple synaptic events have to be combined in close temporal proximity. We can explore this by delivering two presynaptic spikes and varying the time between them.\n\n# delivery multiple presynaptic spikes with varying inter-spike intervals\nsim_dur_syn = 1.1\nt_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)\n\nin_prespk = spk_train(t_pts, [0.1, 0.15, 0.4, 0.425, 0.7, 0.7125, 1.0, 1.00625])\n\nsyn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)\nplot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to multiple presynaptic spikes')\nplt.gcf().set_size_inches((12, 6))\nplt.gcf().axes[0].axhline(syn_nrn._vthresh*1e3, color='k', linestyle='--')\n\n\n\n\n\n\n\n\nThe dashed line for the membrane potential plot is the action potential threshold. A single synaptic response is not sufficient to cross this threshold. If two are delivered, they must be close enough together, less than 25 ms between them, to trigger an action potential. Note also that making them closer does not increase the number of action potentials, only one is generated. To elicit multiple action potentials would require a sustained train of presynaptic spikes, such as below:\n\n# deliver a train of presynaptic spikes at 40 hz\nsim_dur_syn = 0.5\nt_pts = syn_nrn.get_t_vec(dur=sim_dur_syn, dt=sim_dt)\n\nin_prespk = spk_train(t_pts, np.arange(0.05, 0.4, 1/40))\n\nsyn_v, syn_i = syn_nrn.run(dur=sim_dur_syn, dt=sim_dt, inp=0, prespk=in_prespk)\nplot_sim_prespk(t_pts, syn_v, syn_i, 0, in_prespk, title='Synaptic response to a 40 Hz spike train')\n\n\n\n\n\n\n\n\nDelivering a train of presynaptic spikes at 40 Hz elicits a steady stream of action potentials in the postsynaptic neuron. Not every presynaptic spike elicits an action potential, instead they have to summate. In this case, it took 3 presynaptic spikes to elicit the first action potential, and subsequently every 2 spikes drove an action potential."
  },
  {
    "objectID": "Week1.html#detecting-the-activity-of-neurons",
    "href": "Week1.html#detecting-the-activity-of-neurons",
    "title": "Week 1",
    "section": "Detecting the activity of neurons",
    "text": "Detecting the activity of neurons\nSo far, we have been tracking neural activity by measuring the voltage and currents across the membrane. Experimentally, this is done using fine glass pipettes that are inserted into or make a hole in single neurons. This approach, known as intracellular or whole-cell patch recording, is not practical in humans, and exceptionally difficult in awake behaving animals. Instead, we typically use metal electrodes that do not need to be in direct contact with the neurons whose activity we want to detect. These electrodes pick up weak electrical signals emanating from nearby neurons, a technique referred to as extracellular recording. How does this work?\n\nPhysics of extracellular potentials\nAs we discussed above, the currents flowing across the membrane are composed of ions. Each ion has a charge, and charge produces an electric field that exerts a force on other charges. The force required move a charge through an electric field can be picked up as a potential on an electrode. Since the neuronal lipid membrane is an insulator, our electrode can only pick up the movement of charges across the membrane. If positive charges flow into the neuron, that is a net negative of charge on the outside an a negative potential results. If positive charges flow out of the neuron, that is a negative positive of charge on the outside, causing a positive potential.\n\n\n\nExtracellular field of a cat motor neuron firing an action potential (numbers are in millivolts). From Rall 1962.\n\n\nThus, the currents flowing through the membrane during synaptic barrages or action potentials can be detected by nearby electrodes. Fortunately for us, these transmembrane currents are so weak and so slow that we can use a simple equation to describe their influence on the voltage picked up by a nearby electrode. This is given by the equation:\n V = \\frac{1}{4\\pi\\sigma}\\frac{I}{d} \nHere \\sigma is the conductivity of neural tissue, I is a transmembrane current, and d is the distance between the electrode and the current. A simplifying assumption we make when using this equation is that the the conductivity of the brain is homogeneous (sometimes referred to as being Ohmic). A few trends are clear from this equation. The influence that a transmembrane current has on an electrode will only depend on the the distance the electrode has from the current, and the current’s sign and magnitude. The further a current is from the electrode, the weaker it gets, with the greatest fall off happening at short distances. If a current is flowing into the neuron (excitatory), then it will produce a negative voltage at the electrode, while a current flowing out of the neuron (inhibitory) produces a positive voltage. The magnitude of the voltage signal will be linearly proportional to the voltage, e.g. doubling the current will double the voltage.\nThere are multiple current sources in the brain. Every open ion channel is a potential current source and these carpet the entire neuronal membrane. To capture their collective influence on a recording electrode, we can treat each as producing their own voltage and add them together.\n V = \\frac{1}{4\\pi\\sigma}\\sum_{i=0}^{n}\\frac{I_i}{d_e - d_i} \\tag{6}\nIn this equation we sum the resulting voltages from all currents, with each current I_i divided by its distance from the recording electrode, d_e - d_i. Stronger currents will tend to dominate over weaker ones. However, if there are a large number widely distributed of weak currents with similar waveforms then when added together they can produce a prominent potential.\n\n\nModeling the relationship between electrode distance and recorded potential\nUsing our SynapticNeuron model, we can explore how different types of neural activity are detected by extracellular electrodes. This model will simulate a collection of neurons arranged in a circular sheet. To capture the kinds of activities observed in the brain, we will include a slow global fluctuation in membrane potential across all the neurons (slow_noise), noise that is unique to each neuron (noise), and a pulse of synaptic drive (mean_t and std_t). It will also require a method to calculate the extracellular potential, using equation 6 (calc_extracell).\n\n\n\n\n\n\nA caveat to our extracellular recording model\n\n\n\nThe extracellular recording model we implement below is technically incorrect. Recall that Kirchoff’s current law requires the total current flowing into a point in a circuit to equal the amount flowing out. For our extracellular model, we measure the current flowing across the membrane on the extracellular side, which is composed of all the passive, action potential, and synaptic currents, which must sum to 0. This means that the total current flowing across the membrane in our model is 0, which would produce no extracellular field given equation 6. So, why do we detect extracellular potentials in the real world? This is because neurons are composed of more than just their cell body. They also have dendrites and axons. This allows currents to flow not only across the membrane, but also within the neuron itself. Thus, the total current flowing across a single segment of the membrane does not have to balance, since part of that current will flow inside the neuron to another part of the neuron, and eventually flow out of the membrane in a different segment. If you want to realistically model the extracellular field, this requires your neuron model to have multiple compartments that are connected together, each modeled using the formalism we use here. In contrast, our model is a single compartment model. To get an extracellular field, we will just measure the leak current, which responds to all the other currents crossing the membrane.\n\n\n\nclass ExtracellRec():\n    # class for extracellular recording of a sheet of neurons using the SynapticNeuron class\n    # user can set the radius of the sheet, the density of neurons, the mean time and standard deviation \n    # around when a synaptic input arrives, the level of a shared slow input noise, and an individual gaussian noise\n    # level for each neuron, electrode distance from the sheet\n    def __init__(self, radius=2, density=1000, mean_t=0.1, std_t=0.005, slow_noise=25, \\\n                 noise=200, extra_cond=0.3, **kwargs):\n        self._radius = radius # cm\n        self._density = density # neurons per cm^2\n        self._mean_t = mean_t # sec\n        self._std_t = std_t # sec\n        self._slow_noise = slow_noise * 1e-12 # slow shared noise standard deviation in pA\n        self._noise = noise * 1e-12 # individual neuron gaussian standard deviation in pA\n        self._extra_cond = extra_cond # extracellular conductivity in S/m\n        self._v = [] # voltage array\n        self._i = [] # current array\n        self._t = [] # time array\n\n        # calculate number of neurons\n        self._n_neurons = int(np.round(np.pi * self._radius**2 * self._density))\n\n        # calculate positions of neurons, uniformly distributed in the circular sheet\n        # place each neuron by setting a random angle and radius from the center\n        self._neuron_pos = np.zeros((self._n_neurons, 2))\n        for ind in range(self._n_neurons):\n            curr_ang = np.random.rand() * 2 * np.pi\n            curr_rad = np.sqrt(np.random.rand()) * self._radius\n            self._neuron_pos[ind, 0] = np.cos(curr_ang) * curr_rad\n            self._neuron_pos[ind, 1] = np.sin(curr_ang) * curr_rad\n        # force one neuron to be at the center, so it can be easily picked up by the electrode\n        self._neuron_pos[0, :] = 0\n\n        # create neurons\n        self._neurons = []\n        for ind in range(self._n_neurons):\n            self._neurons.append(SynapticNeuron(**kwargs))\n\n    # method to calculate extracellular potential based on currents from each neuron\n    def calc_extracell(self, h=1):\n        dists = np.sqrt(np.sum(self._neuron_pos**2,axis=1)+h**2)\n        return 1/(4*np.pi*self._extra_cond) * np.sum((self._i.T * 1e-9) / dists, axis=1)\n\n\n        \n    # create run function that will run all neurons for a given duration\n    def run(self, dur=0.2, dt=0.0001, seed=47):\n        # set random seed\n        np.random.seed(seed)\n\n        # initialize arrays to store values\n        t = self._neurons[0].get_t_vec(dur, dt) # time array\n        num_t = len(t)\n        v = np.zeros((self._n_neurons, num_t)) # voltage array\n        i = np.zeros((self._n_neurons, num_t)) # current array\n        \n\n        # create synaptic inputs\n        in_prespk = []\n        for ind in range(self._n_neurons):\n            in_prespk.append(spk_train(t, np.random.normal(self._mean_t, self._std_t, 1)))\n        \n        # create slow noise input\n        slow_noise = np.random.normal(0, 1, num_t)\n        slow_noise = sig.detrend(np.cumsum(slow_noise))\n        slow_noise = (slow_noise/(np.std(slow_noise))) * self._slow_noise\n\n        # create individual noise inputs\n        indiv_noise = np.random.normal(0, self._noise, (self._n_neurons, len(t)))\n\n        # run simulation\n        for ind, curr_nrn in enumerate(self._neurons):\n            inp_sig = slow_noise + indiv_noise[ind, :]\n            v[ind, :], i[ind, :] = curr_nrn.run(dur=dur, dt=dt, inp=inp_sig, prespk=in_prespk[ind])\n\n        # save simulation results\n        self._t = t\n        self._v = v\n        self._i = i\n\nNow that our simulation class is all setup, let’s use it. We will create a sheet of neurons with a radius of 2 cm and 1000 neurons per cm2. Using **kwargs, we are also able to pass arguments to SynapticNeuron, to configure its electrophysiological properties. We will take advantage of this by increasing the synaptic conductance, gsyn, so that action potentials are more reliably driven. The timing of the presynaptic spikes is set by a normal distribution, with the mean time of their emission at 100 ms, and a standard deviation of 5 ms. Since the inputs to our model neurons are generated randomly, and we want to replicate our results, we will fix the random seed of the model to ensure that the same output is generated each time we run it.\n\ntest = ExtracellRec(gsyn=7e-9)\ntest.run(seed=41)\n\nThat is all it took to run our simulation. The more neurons we include, either by increasing the size of our sheet with the radius parameter or their packing with the density parameter, longer it takes to run. Once the simulation is finished running, we can calculate the extracellular potential for electrodes at different distances from the center of the model. We will choose a couple key values that are relevant to the recordings we will perform. An electrode on the scalp would be ~2 cm away from the surface of the brain. Intracranial surface electrodes that rest on the brain surface are picking up the activity of neurons with 0.5 mm. Lastly, depth electrodes that are inserted into the brain will detect neurons within tens of microns.\n\nelec_dists = [2, 0.05, 0.001] # cm\ndist_labels = ['2 cm', '0.5 mm', '10 um']\n\nfig, ax = plt.subplots(len(elec_dists),1, figsize=(5, 12), sharey=True)\nfor ind, curr_dist in enumerate(elec_dists):\n    ax[ind].plot(test._t*1e3, test.calc_extracell(h=curr_dist)*1e6)\n    ax[ind].set_title(dist_labels[ind])\n    ax[ind].set_xlabel('Time (ms)')\n    ax[ind].set_ylabel('Extracellular potential (uV)')\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\nEach of these distances reveal different features of neural activity. The most obvious difference is that the signals tend to get larger as the electrode is moved closer to the sheet of neurons. At 2 cm, slow potentials, both the slow global signal and the synaptic drive, are evident. Then, moving to 0.5 mm we start to pick up some weak local spiking, and still can reliably detect synaptic currents. Note that the weak spiking we see is concentrated at the peak of the synaptic drive, with numerous overlapping spikes filling in the negative trough made by the depolarizing synaptic current. If we push the electrode to within 10 um of a neuron, the action potential becomes the dominant part of the signal. While the synaptic current is still visible prior to the spike, it is masked by the action potential once that occurs.\nAnd going a little further, we can push the electrode right against the cell body of the neuron. This is juxtacellular recording, and it has a distinctive appearance where global and synaptic potentials are towered over by the robust currents generated by the action potential.\n\nplt.plot(test._t*1e3, test.calc_extracell(h=0.0001)*1e6)\nplt.title('1 um')\nplt.xlabel('Time (ms)')\nplt.ylabel('Extracellular potential (uV)')\n\nText(0, 0.5, 'Extracellular potential (uV)')"
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "Week3",
    "section": "",
    "text": "Last week we established that the neural response to stimulus presentation produces a stereotyped pattern of electrical activity on the scalp, referred to as an event-related potential (ERP). This was determined by taking the times when a stimulus was presented and calculating the average EEG activity around that time. Since we know when the stimuli are presented, determining how the brain responds is fairly straight forward. What is not straight forward is going back the other direction: given the neural activity can we determine whether a stimulus was presented? This is the essential problem of BCI, and the one we will solve. And solving it for this case unlocks a host of new possibilities that. The approach developed this week can be elaborated to solve wide variety of brain activity decoding problems. It opens up the possibility to decode the identity of the presented stimulus, or a subject’s intended action. This is as close to mind-reading as one can get. Let’s get to work.\n\n\nThe overarching goal of this week is to introduce simple logistic regression models, where a single feature derived from neural data (ERP strength) is used to predict a single predictor (stimulus present or absent). To start we will discuss different ways to quantify the strength of an ERP. We will also weigh the advantages and disadvantages of different approaches to extract time periods from the recording that lack stimuli. Then logistic regression will be introduced and we will cover how to apply it using the scikit-learn python package. Lastly, we will discuss some of general issues that arise in machine learning algorithms like logistic regression.\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport sys\nsys.path.append('..')\nfrom source.loaders import EEG, remove_baseline_drift, remove_emg, remove_ac, detect_blinks\n\n\n\n\nERPs have complex waveforms that can last a couple hundred milliseconds. With a sample rate of 500 Hz, that means we will have around 100 samples of neural data to work with when trying to determine whether a stimulus was present or not. You could imagine being given a set of waveforms, some from trials with a stimulus and others without, and trying to determine which was which. One strategy is to calculate a feature of the waveform related to the presence of the ERP, and if that feature exceeds a certain value then the ERP is deemed present on that trial.\n\n\nAs a starting point, we will use the data set we worked with last week. An auditory cue was given about every 8 seconds, and there was a robust ERP elicited over occipital and parietal EEG sites. We will select those epochs when we know a cue was presented, and an equal number of epochs where we know a cue was not delivered. To start, we need to load the data.\nLast week we created an EEG class that interfaced with the EEG data, along with preprocessing functions to remove noise or detect eye blinks. We will reuse them here to quickly get us back to where we left off last week.\n\n# create file paths\nsubj = 'sub-AB58'\nevt_dir = ['data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_events.tsv'.format(subj)]\nevt_path = os.path.join(*evt_dir)\n\ndata_dir = ['data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_eeg.set'.format(subj)]\neeg_path = os.path.join(*data_dir)\n\nchan_dir = ['data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_channels.tsv'.format(subj)]\nchan_path = os.path.join(*chan_dir)\n\n# load data\neeg = EEG(eeg_path, chan_path)\nevents = pd.read_csv(evt_path, sep='\\t')\n\n# preprocess data\nremove_baseline_drift(eeg)\nremove_emg(eeg)\nremove_ac(eeg, ac_freq=50)\nblinks = detect_blinks(eeg)[0] # indexing the call to the function allows us to select the output we want\ncues = events['onset'][events['trial_type'] == 'cue'].values\n\n\n\n\nTo compare stimulus and non-stimulus periods, we need to select times when the stimulus did not occur. At first glance, this seems like a trivial task. Given the list of stimulus times, we can just create a new list by hand that does not include any of those times. However, we might unintentionally incorporate biases in our list creation. In addition, we want to avoid periods with blinks, because we exclude those periods from our ERP construction. This means we have to avoid two sets of conditions. But worst of all, we would have to create a new list for each recording session, which is tedious. We program to avoid tedium, so lets explore ways to automate this step.\nThe approach we devise to create no cue times should satisfy a few conditions:\n\nThe cue should not be present\nNo eye blinks should be present\nNo systematic temporal relationship with cue delivery\nNo-cue epochs should have similar properties to cue epochs, such as:\n\nOccur during the same period of the recording\nSimilar relative timing\n\n\nBut first, we will get the cue times that did not overlap with eye blinks.\n\n# Function that returns cues without blinks\nexcl_blink = 0.6 # seconds, exclusion period surrounding a blink\nrem_blinks = lambda c,b: np.array([ci for ci in c if not any(np.abs(ci-b) &lt; excl_blink)])\n\n# keep those cues that are not within the blink exclusion period\ncues_nob = rem_blinks(cues, blinks)\n\nThe same function should also be used with the non-stimulus times we will generate. We want the no-cue times we create to be as similar as possible to the times when cues were presented. If we are excluding cues that were near eye blinks, then we have to do the same for the no-cue periods. If we do not, then the cue and no-cue times will differ in whether an eye blink was present, which our algorithm could potentially pick up on to distinguish between them.\nTo ensure that our no-cue periods do not come close to the cue periods, we could set the no-cue times to be at a fixed time before each cue. If that time is greater than the post-window, and far enough from the previous cue time to ensure that it does not overlap with its ERP, this might work. This strategy would be questionable if the ITIs were at a fixed interval, because then there would be a systematic relationship between the cue times and no-cue times. But because they are random, that means the previous cue time will be random, plus or minus a second, with respect to the no-cue time.\n\n# fixed time before cue method\n# fixed time should be less than the shortest iti minus the window.\nfixed_time = 4 # half of the mean ITI\nnocues_fixed = cues-fixed_time\nnocues_fixed_nob = rem_blinks(nocues_fixed, blinks)\n\ndef plot_nocues(ax, nocues_orig, nocues_rem, cues):\n    \"\"\"Plot no-cue times, with cues and cut no-cue times overlaid\n    \n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes\n        Axes to plot on\n    nocues_orig : numpy.ndarray\n        Original no-cue times\n    nocues_rem : numpy.ndarray\n        No-cue times with blinks removed\n    cues : numpy.ndarray\n        Cue times\n    \"\"\"\n    ax.vlines(cues, colors='k', ymin=-1, ymax=2, alpha=0.2) # cue times\n    ax.eventplot(nocues_rem, colors='b') # no-cue times\n    nocues_cut = np.setdiff1d(nocues_orig, nocues_rem) # cut no-cue times\n    for nocue_cut in nocues_cut:\n        ax.text(nocue_cut, 1, 'X', fontsize=20, color='r', ha='center', va='center')\n    ax.set_ylim(0, 2)\n    ax.set_yticks([])\n    ax.set_xlabel('Time (s)')\n\n# plot permuted no-cue times\nfig, ax = plt.subplots(figsize=(10, 2))\nplot_nocues(ax, nocues_fixed, nocues_fixed_nob, cues)\nax.set_title('Fixed offset no-cue times')\n\nText(0.5, 1.0, 'Fixed offset no-cue times')\n\n\n\n\n\n\n\n\n\nWe created a custom plotting function to see how our event times for the cue and no-cue periods related to each other. In it vertical lines are the times when cues were presented, blue hatch marks are when valid random no-cue times occur, and red ’X’s are the no-cue times that were eliminated due to overlapping with a cue or blink. Using times at a fixed interval before cue delivery looks good. Our no-cue periods are consistently far away from the cues and have a similar distribution as the cues across the session. We will use these times to get our no-cue epochs.\n\n\n\n\n\n\nSome flawed approaches to selecting non-cued periods\n\n\n\nThere are a variety of strategies for choosing the times when no stimuli are present that should meet the criteria listed above. Here we will explore a couple that seem like good ideas, but upon closer examination exhibit flaws.\nSince these approaches will generate no-cue times that do not have a prespecified fixed offset from the cue times, we need to exclude those times near a cue. For this, we will create a similar function to the one that removed times close to blinks (rem_blinks), but use a tighter exclusion period because of the tight temporal relationship between cue delivery and the ERP.\n\n# create version outside cue period\nexcl_cue = 0.1 # seconds, exclusion period around cue\nrem_cues = lambda nc,c: np.array([nci for nci in nc if not any(np.abs(nci-c) &lt; excl_cue)])\n\n\n\nFor this approach, we will create a list of times using a random number generator. The random times we generate cannot have their window times extend past the beginning or end of the recording, nor overlap with blink period or cue periods. We will use a fixed random seed when generating these times to ensure consistency across runs.\n\n# set random seed, ensures consistent results across runs\nnp.random.seed(47)\n\n# random times between 0 and recording duration\nrec_dur = eeg.data.shape[1]/eeg.srate # (number of samples / sample rate) gives time in seconds\ncue_num = len(cues) # number of cues\nnocues_rand = np.random.uniform(1, rec_dur-1, cue_num)\nnocues_rand_nbc = rem_cues(rem_blinks(nocues_rand, blinks), cues)\n\nWe use the random.uniform random number generator in numpy to get as many times as we have cues (cue_num), and restricted to times from the first till the last second of the recording session. Those times that overlap with cues or blinks are then removed, leaving a list of no-cue times that we can use. A good way to establish that the criteria for our no cue periods are followed is to plot the no-cue times overlaid on top of the cue times.\n\nfig, ax = plt.subplots(figsize=(10, 2))\nplot_nocues(ax, nocues_rand, nocues_rand_nbc, cues)\nax.set_title('Random no-cue times')\n\nText(0.5, 1.0, 'Random no-cue times')\n\n\n\n\n\n\n\n\n\nIf we compare the cue and no-cue times, it is apparent that the fourth criterion for good no-cue times is violated. One problem is that we have several no-cue times that occur outside the time period when the cue times were delivered (notice the two at the beginning and four at the end.) The other problem is that no-cue times are distributed unevenly, unlike the cues which are evenly spread across the recording session.\n\n\n\nIf we want to have the no-cue times have a similar distribution across the session as the cue times, we could generate random times but ensure that they have the same spacing between cues. One way to do this is to get a list of times between cue trials (inter-trial intervals, ITIs), shuffle those intervals, and then create a new list of times by adding them together one at a time. This strategy works because the times between cues are randomly generated, varying between 7 and 9 seconds.\n\n# set random seed, ensures consistent results across runs\nnp.random.seed(45)\n\n# Calculate ITI times\n# Note, we repeat the first ITI time so that when we sum the ITIs\n# we get the correct number of trials\niti_times = np.insert(np.diff(cues), 0, cues[1]-cues[0])\n\n# randomly shuffle the iti_times\niti_times = np.random.permutation(iti_times)\n\n# get no-cue times by summing the iti_times.\n# cumsum adds each element of the array to the previous element, \n# returning an array of the same size\nnocues_perm = np.cumsum(iti_times)\n\n# remove no-cue times that overlapped with cue times and blinks\nnocues_perm_nbc = rem_cues(rem_blinks(nocues_perm, blinks), cues)\n\n# plot permuted no-cue times\nfig, ax = plt.subplots(figsize=(10, 2))\nplot_nocues(ax, nocues_perm, nocues_perm_nbc, cues)\nax.set_title('Permuted no-cue times')\n\nText(0.5, 1.0, 'Permuted no-cue times')\n\n\n\n\n\n\n\n\n\nThis looks much better than the purely random strategy. The no-cue times are during the same period when cues were presented, and have similar relative timing. However, they appear to sometimes overlap quite closely for runs of several trials with the cue times.\n\n\n\n\n\n\nLast week we covered extracting EEG epochs, so we will only lightly reprise this here. This time we need one set of epochs centered on the cue times, and another on the no-cue times. We can use the get_data method in our eeg object to extract the EEG data around these times. Each will be a 3-D numpy array, where time is on axis 0, channel is axis 1, and trial on axis 2. Instead of using all channels, we will focus only on a channel were the cue ERP was strong, O1.\n\n# time window edges\npre_win = 0.1 # time to sample before the cue\npost_win = 0.5 # time to sample after the end\nepoch_dur = pre_win+post_win # duration of an epoch\nsel_chan = 'O1'\n\n# get cue epochs\ncue_starts = cues_nob - pre_win\ncue_epochs, t_erp, _ = eeg.get_data(chans=sel_chan, start_t=cue_starts, dur_t=epoch_dur, scale='relative')\n\n# get no-cue epochs\nnocue_starts = nocues_fixed_nob - pre_win\nnocue_epochs, _, _ = eeg.get_data(chans=sel_chan, start_t=nocue_starts, dur_t=epoch_dur, scale='relative')\n\n# correct the relative time stamps to account for the pre_win period\nt_erp -= pre_win\n\nSince we only loaded one channel, our channel axis (axis 1) only has a length of 1. This ‘singleton’ dimension is useless for our purposes and so to make things easier we will remove it using the numpy squeeze function.\n\nprint('The original shape of the cue_starts array is: {}'.format(cue_epochs.shape))\ncue_epochs = np.squeeze(cue_epochs)\nprint('The squeezed shape of the cue_starts array is: {}'.format(cue_epochs.shape))\n\nnocue_epochs = np.squeeze(nocue_epochs)\n\nThe original shape of the cue_starts array is: (300, 1, 25)\nThe squeezed shape of the cue_starts array is: (300, 25)\n\n\nLet’s plot the ERPs for the cue and no-cue epochs to make sure the ERP is only present for the cue.\n\n# get the average ERP for each condition\ncue_erp = np.mean(cue_epochs, axis=1) \nnocue_erp = np.mean(nocue_epochs, axis=1)\n\n# plot the ERPs\nplt.figure(figsize=(10, 5))\nplt.plot(t_erp, cue_erp, label='Cue')\nplt.plot(t_erp, nocue_erp, label='No-cue')\nplt.xlabel('Time (s)')\nplt.ylabel('Voltage (uV)')\nplt.title('Average ERP for channel ' + sel_chan)\nplt.legend()\n\n\n\n\n\n\n\n\nSo far so good. We can see that the ERP is strong for epochs centered on the cue, and non-existent for times that avoid cue presentation.\n\n\n\nThe next step is to distill the complex ERP waveform into a single number that reflects its strength. There are numerous measures you can use for this, and we will explore a few. This step is known as feature engineering, wherein we try to distill multidimensional samples to a few values that capture the aspects of the sample that are relevant to our task. In this case, each sample is a collection of 300 voltage measurements, and the aspect we want to measure is the strength of the ERP. A good feature will take on values that distinguish between the cue and no cue epochs.\n\n\nSince we know the shape of the ERP, we know on average where it tends to peak in value. The voltage at that time should indicate whether an ERP is present. Looking at the average ERP traces immediately above, you can see that on trials with a cue the voltage will tend to be near 10 uV at the peak time, while on no-cue trials it will tend to be near 2 uV. To get the peak voltage on each trial, we first find the time when the ERP peak occurs, then sample the voltage at that time for each epoch.\n\n# get peak idx of cue erp\ncue_peak_idx = np.argmax(cue_erp)\n\n# return the voltages at the ERP peak time\ncue_erp_peaks = cue_epochs[cue_peak_idx, :]\nnocue_erp_peaks = nocue_epochs[cue_peak_idx, :]\n\nTo examine how well the peak voltage distinguishes between the Cue and No-cue conditions, we can plot their distribution. The less their respective distributions overlap, the better the feature is for distinguishing between the types of trials.\n\n# box plot of peak values for cue and nocue conditions with event plot of points on top\ndef plot_dist_erps(cue_vals, nocue_vals, ax=None):\n    \"\"\"Plot distribution of cue and no-cue ERP peak values\n\n    Parameters\n    ----------\n    cue_vals : numpy.ndarray\n        Array of cue ERP peak values\n    nocue_vals : numpy.ndarray\n        Array of no-cue ERP peak values\n    ax : matplotlib.axes.Axes\n        Axes to plot on\n    \"\"\"\n    \n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # violin plot of values, with cue_vals blue and nocue_vals orange\n    cue_vp = ax.violinplot(cue_vals, positions=[1], showmedians=True)\n    for pc in cue_vp['bodies']:\n        pc.set_facecolor('tab:blue')\n\n    nocue_vp = ax.violinplot(nocue_vals, positions=[2], showmedians=True)\n    for pc in nocue_vp['bodies']:\n        pc.set_facecolor('tab:orange')\n    \n    # plot the cue_vals and nocue_vals as points on top of the violin plot\n    ax.plot(np.ones(len(cue_vals)), cue_vals, '.', color='tab:blue')\n    ax.plot(2 * np.ones(len(nocue_vals)), nocue_vals, '.', color='tab:orange')\n    # set the xticks to be at 1 and 2, with labels Cue and No Cue\n    ax.set_xticks([1, 2])\n    ax.set_xticklabels(['Cue', 'No Cue'])\n\nfig, ax = plt.subplots(figsize=(2, 4))\nplot_dist_erps(cue_erp_peaks, nocue_erp_peaks, ax=ax)\nplt.ylabel('Peak Amplitude (uV)')\nplt.title('Peak Amplitude of Cue and No Cue Conditions')\n\nText(0.5, 1.0, 'Peak Amplitude of Cue and No Cue Conditions')\n\n\n\n\n\n\n\n\n\nThe violin plot above shows a smoothed distribution of the data points as a shaded region. The solid horizontal lines from top to bottom are the maximum, median, and minimum values. The distributions do overlap, with the median value for the No-cue condition falling within the distribution of values for the Cue. This overlap is likely because there is substantial spontaneous activity in the EEG signal that is added to our ERP peak estimate on each trial. When we got the average ERP, this spontaneous activity that was not time-locked to cue delivery canceled out because it was equally likely to be positive or negative. We can visualize this spontaneous component by plotting the EEG signal for each epoch with the ERP subtracted out. That looks like:\n\n# subtract the mean ERP waveform from each epoch\n# epochs have to be transposed, x.T, because of broadcasting rules in numpy\ncue_epochs_spon = cue_epochs.T-cue_erp\nnocue_epochs_spon = nocue_epochs.T-nocue_erp\n\n# plot EEG without ERP, and distribution of values where ERP peak would be\nfig, ax = plt.subplot_mosaic([['ul', 'ul', 'r'],['ll', 'll', 'r']], sharey=True)\nax['ul'].plot(t_erp, cue_epochs_spon.T, color='tab:blue', alpha=0.2)\nax['ul'].plot(t_erp, cue_erp, color='tab:blue')\nax['ul'].grid()\nax['ll'].plot(t_erp, nocue_epochs_spon.T, color='tab:orange', alpha=0.2)\nax['ll'].plot(t_erp, nocue_erp, color='tab:orange')\nax['ll'].grid()\nax['ll'].set(xlabel='Time (s)')\nplot_dist_erps(cue_epochs_spon[:,cue_peak_idx], nocue_epochs_spon[:,cue_peak_idx], ax=ax['r'])\nax['r'].set_yticklabels(range(-30, 40, 10))\nfig.subplots_adjust(hspace = 0)\nax['ul'].set(title='Waveforms')\nax['r'].set(title='EEG voltage at ERP peak')\nfig.suptitle('Spontaneous EEG', fontsize=16)\nfig.supylabel('Voltage (uV)')\n\n/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8892/2930109674.py:16: UserWarning:\n\nset_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n\n\n\nText(0.02, 0.5, 'Voltage (uV)')\n\n\n\n\n\n\n\n\n\nAfter removing the ERP, a a great deal of variation still remains, with an amplitude on par with the ERP itself. In fact, the shape of the distribution of EEG voltages at the ERP peak time is the same as the one for the ERPs above. This is a simple consequence of the ERP peak values reflecting the sum of the mean ERP peak and the spontaneous EEG at that time. When we subtract the mean ERP peak, we are just shifting the distribution to zero. Is there any way we can account for the spontaneous EEG activity on a single trial?\n\n\n\nExamining the spontaneous EEG waveforms, we see that the signal fluctuates between positive and negative values on the order of tens of milliseconds. Perhaps if we average the EEG signal over the 100 ms when the strongest ERP component is present, from 80 to 180 ms, the spontaneous activity will cancel itself out. The ERP, on the other hand, maintains positive values during that period, so it should persist despite the averaging. The logic here is similar to averaging the EEG epochs across trials, because the spontaneous voltage at each time point across epochs will randomly vary between positive and negative values. In this case, we are just applying that principle across time within a trial.\nWhen we restrict analysis or calculation to a specific time window, that period is referred to as the region of interest (ROI). Here, our ROI will be from 80 to 180 ms post cue (or no-cue), and we will take the mean value of the EEG voltage during that time.\n\n# get indices between 80 and 180 ms\nroi_idxs = np.where((t_erp &gt;= 0.08) & (t_erp &lt;= 0.18))[0]\n\n# get the mean EEG voltage during the strongest ERP component across trials for each condition\ncue_erp_means = np.mean(cue_epochs[roi_idxs, :], axis=0)\nnocue_erp_means = np.mean(nocue_epochs[roi_idxs, :], axis=0)\n\n# plot mean voltages\nfig, ax = plt.subplots(figsize=(2, 4))\nplot_dist_erps(cue_erp_means, nocue_erp_means, ax=ax)\nplt.ylabel('Mean ERP voltage (uV)')\nplt.title('Mean amplitude of Cue and No Cue Conditions')\n\nText(0.5, 1.0, 'Mean amplitude of Cue and No Cue Conditions')\n\n\n\n\n\n\n\n\n\nWell that did not work! Averaging over the period where the strongest ERP component was present diminished median peak ERP voltage for the Cue condition, making it less separable from the No-cue condition. This happens because the magnitude of the ERP varies across time, and we are averaging together periods where it is weaker. Indeed, if we calculate the mean of the ERP during our ROI:\n\n# get mean of ERP during the ROI\nerp_mean = np.mean(cue_erp[roi_idxs])\nprint(\"Mean of ERP during ROI: {:.2f}\".format(erp_mean))\n\n# get ERP voltage during its peak\nerp_peak = cue_erp[cue_peak_idx]\nprint(\"ERP voltage during peak: {:.2f}\".format(erp_peak))\n\nMean of ERP during ROI: 6.63\nERP voltage during peak: 11.21\n\n\nWe have gone from an ERP peak voltage of 11.2 to 6.6 uV, a decrease by almost half! While this might tempt us to go back to the original approach of just sampling the voltage at the ERP peak, we would leave behind one advantage of the averaging approach: it reduced the variability in the distribution of ERP values. This is evident when we compare the standard deviations of the distributions between the ROI mean and peak estimation approaches.\n\n# variance of ERP ROI means\ncue_erp_means_var = np.std(cue_erp_means)\nnocue_erp_means_var = np.std(nocue_erp_means)\n\n# variance of ERP peaks\ncue_erp_peaks_var = np.std(cue_erp_peaks)\nnocue_erp_peaks_var = np.std(nocue_erp_peaks)\n\n# print comparisons\nprint('Cue ERP ROI {:.2f} vs peaks {:.2f} uV'.format(cue_erp_means_var, cue_erp_peaks_var))\nprint('No-cue ERP ROI {:.2f} vs peaks {:.2f} uV'.format(nocue_erp_means_var, nocue_erp_peaks_var))\n\nCue ERP ROI 4.82 vs peaks 6.04 uV\nNo-cue ERP ROI 6.75 vs peaks 8.80 uV\n\n\nTaking the mean over the ROI reduced the variability in our ERP measures. This means their distributions are tighter. Assuming the means of those distributions do not change, this would decrease their overlap. Consequently, our ability to discriminate between cue and no-cue trials should improve. However, the means did change, we lost almost half of the ERP strength compared with just taking the peak.\nIs there a way we can benefit from averaging over multiple samples to get a measure of ERP strength, while not diminishing the strength of the ERP itself?\n\n\n\nOne way to average over multiple samples while not diminishing ERP strength is to weight each time point by the value of the ERP. In effect, this would measure the alignment between the voltages recorded during an epoch and the mean ERP. Here we determine the strength of the ERP by multiplying each data point by its corresponding voltage in the ERP. If we are at a sample where the ERP is strongly positive, then we want that point to strongly contribute the mean, while a point where the ERP is weak should contribute less. If the ERP is negative at a certain time point, we want to negate that sample so it will constructively add to the mean. By doing this, if the EEG contains a signal that follows the waveform of the ERP it will return a strong mean value, while an EEG signal that does not follow the ERP time course will return a weak mean.\nA mathematical operation that can be used to do this is called the dot product. A dot product multiplies together each corresponding element in two vectors, x and y, and then adds those together, returning a scalar z.\n \\begin{align}\n    \\notag z &= x \\cdot y \\\\\n    \\notag  &= \\sum_{i=1}^{n}x_{i}y_{i}\n\\end{align}\n\nLet’s explore how to calculate a dot product and its behavior.\n\n# how to code a dot product in base python\ndef dot(x, y):\n    \"\"\"\n    Calculate the dot product of two vectors x and y\n\n    Parameters\n    ----------\n    x : array-like\n        First vector\n\n    y : array-like\n        Second vector\n\n    Returns\n    -------\n    out : float\n        Dot product of x and y\n    \"\"\"\n\n    out = 0 # initialize output to 0\n    for x_i, y_i in zip(x, y): # loop over elements of x and y\n        out += x_i * y_i # add the product of the elements to out\n\n    return out\n\n\nv1 = [1, 2, 3]\nv2 = [1, 2, 3]\nprint('Dot product of two aligned vectors, {} and {} = {}'.format(v1,v2,dot(v1, v2)))\n\nv1 = [1, 2, 3]\nv2 = [2, 2, 2]\nprint('Dot product of two partly aligned vectors, {} and {} = {}'.format(v1,v2,dot(v1, v2)))\n\nv1 = [1, 2, 3]\nv2 = [-1, 2, -1]\nprint('Dot product of two misaligned vectors, {} and {} = {}'.format(v1,v2,dot(v1, v2)))\n\nDot product of two aligned vectors, [1, 2, 3] and [1, 2, 3] = 14\nDot product of two partly aligned vectors, [1, 2, 3] and [2, 2, 2] = 12\nDot product of two misaligned vectors, [1, 2, 3] and [-1, 2, -1] = 0\n\n\nInstead of rolling our own, we will use the numpy array dot function, dot, because it handles dot products on multidimensional arrays and runs faster.\nTo calculate the alignment between an EEG epoch and the mean ERP, we use the ERP waveform as our weighting vector. We will scale its values by its euclidean norm, so that the values returned by our dot product are in a comparable range to those from the peak and mean calculations we did previously. The euclidean norm is:\n ||x|| = \\sqrt{\\sum_{i=1}^{n}{x_{i}^{2}}} \nThis function is implemented in the linear algebra portion of the numpy package as linalg.norm.\nThus, the alignment between an EEG epoch and ERP is:\n Alignment = EEG \\cdot \\frac{ERP}{||ERP||} \nLet’s give it a try.\n\n# our function that measures the alignment between the EEG epoch and ERP\ndef erp_align(epochs, erp):\n    \"\"\"\n    Calculate the dot product of each epoch with the ERP\n\n    Parameters\n    ----------\n    epochs : numpy.ndarray\n        Array of EEG epochs, with epochs in the last dimension\n    erp : numpy.ndarray\n        Array of ERP waveform\n\n    Returns\n    -------\n    out : numpy.ndarray\n        Array of dot products, one for each epoch\n    \"\"\"\n\n    return np.dot(epochs.T, erp/np.linalg.norm(erp))\n\n\ncue_erp_dots = erp_align(cue_epochs, cue_erp)\nnocue_erp_dots = erp_align(nocue_epochs, cue_erp)\n\n# box plot of peak values for cue and nocue conditions with event plot of points on top\nfig, ax = plt.subplots(figsize=(2, 4))\nplot_dist_erps(cue_erp_dots, nocue_erp_dots, ax=ax)\nplt.ylabel('Dot product (uV)')\nplt.title('Dot product of Cue and No Cue Conditions')\n\nText(0.5, 1.0, 'Dot product of Cue and No Cue Conditions')\n\n\n\n\n\n\n\n\n\nThese two distributions seem much more separable than the previous ones. The median values for the Cue and No-cue conditions do not overlap with the distribution of the opposite condition, and their overall distributions overlap less as well. Replotting all three together highlights the improvement we have made in tailoring the feature we extract from the EEG for detecting the Cue-evoked ERP.\n\nfig, ax = plt.subplots(1,3,figsize=(6, 4))\nplot_dist_erps(cue_erp_peaks, nocue_erp_peaks, ax=ax[0])\nax[0].set_title('Peak amplitude')\nplot_dist_erps(cue_erp_means, nocue_erp_means, ax=ax[1])\nax[1].set_title('Mean amplitude')\nplot_dist_erps(cue_erp_dots, nocue_erp_dots, ax=ax[2])\nax[2].set_title('Dot product')\nfig.supylabel('uV')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur goal is to determine whether a trial had a cue stimulus or not. The dot product feature seems to distinguish between trial types, such that its value on a given trial indicates whether the cue was present. This is a decision process, where we assign a trial to either the Cue or No-cue category depending on the value of the dot product. A simple way to make this decision is to set a threshold value for the dot product, above which the trial is classified as a Cue trial. If the value is below the threshold, we label it a No-cue trial. This can be represented as a piecewise mathematical formula:\n f(x,t) =\n\\begin{cases}\n    0 & x\\leq t \\\\\n    1 & x\\gt t \\\\\n\\end{cases}\n\nHere x is the dot product of the epoch, and t is the threshold. When the function returns a 1 we refer to it as a Cue trial, while a 0 is labeled a No-cue trial. We have to choose a threshold, and to start we will use the minimum dot product of our Cue trials.\n\n# set threshold\nthresh = np.min(cue_erp_dots)\n\n# create vectors for ERP values and class labels\ncue_num = cue_erp_dots.size # number of cue trials\nnocue_num = nocue_erp_dots.size # number of nocue trials\nclass_act = np.repeat([True, False], [cue_num, nocue_num]) # actual class labels, 1 for cue, 0 for nocue\nvals = np.append(cue_erp_dots, nocue_erp_dots) # ERP values\nclass_min_pred = vals &gt; thresh # predicted class labels\n\n# print the true vs. predicted class labels\nprint('Actual class labels: ', class_act)\nprint('Predicted class labels: ', class_min_pred)\n\nActual class labels:  [ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nPredicted class labels:  [ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True False\n  True False False  True False  True False False False False False False\n  True  True False False False False False False  True False  True  True\n False False False False False False]\n\n\nIf you look closely, you can see that some of our predicted classes do not agree with the true ones. There are different types of errors a classifier can make, and numerous ways to measure their performance. We will start with two simple measures, error rate and accuracy. Error rate is the proportion of trials that were misclassified. This can be defined mathematically as:\n Error Rate = \\frac{1}{n}\\sum_{i=1}^{n}{I(y_{i}\\ne \\hat{y_{i}})} \nHere y is the actual class labels and \\hat{y} are the predicted class labels. The function I(x,y) returns a 1 when x \\ne y, and 0 when x=y. Summing across all trials and dividing by the total number of trials, n, gives the proportion of trials whose predicted class disagreed with its actual class.\nAnother helpful measure is accuracy, which is simply the proportion of trials where the predicted and actual class labels agreed. Expressed as:\n Accuracy = 1 - Error Rate \nHow does our simple classifier perform based on these metrics?\n\n# as an example, here we define error rate method with base python\ndef error_rate_base(y_act, y_pred):\n    er = 0\n    for i in range(len(y_act)):\n        if y_act[i] != y_pred[i]:\n            er += 1\n    return 100 * er / len(y_act)\n\n# define error rate method with numpy\ndef error_rate(y_act, y_pred):\n    return np.mean(y_act != y_pred) * 100\n\n# define accuracy method with numpy\ndef accuracy(y_act, y_pred):\n    return 100-error_rate(y_act, y_pred)\n\n\n# evaluate error rate for ERP peak\n# 1 for cue trials, 0 for non-cue trials\ner_min = error_rate(class_act, class_min_pred)\nac_min = accuracy(class_act, class_min_pred)\n\nprint('Error rate for ERP peaks: {:.1f}%'.format(er_min))\nprint('Accuracy for ERP peaks: {:.1f}%'.format(ac_min))\n\nError rate for ERP peaks: 14.8%\nAccuracy for ERP peaks: 85.2%\n\n\nNot too bad. Just setting our threshold to the minimum value from the Cue trials, we can correctly classify ~85% of trials. But, how do we know we have chosen the best threshold? Moreover, can we ascertain how confident our classifier is in the choice it puts out? To address these issues, we need to formalize the classification process. One approach is to use logistic regression, which takes a measurement (e.g. ERP dot product) and returns the probability that it was generated by class (e.g. cue stimulus). For this week, we will pose the problem and learn how to use the logistic regression functions in the scikit-learn package.\n\n\n\nA binary classifier takes a set of measurements, x, as inputs and returns the probability that they were generated by a specific class, \\hat{y}. (This is known as the discriminative view of classification. We will take on the generative view later in the semester when we tackle Naïve Bayes classifiers.) To get from x to \\hat{y}, we need a function that describes the probability of the class occurring over a range of ERP measurement values.\n\n\nProbabiltilies describe how likely events are to occur. They range from 0, for events that never happen, to 1, for events that are guaranteed to happen. When quantifying probabilities we do this for a class of events, with the total probability across all events adding up to 1 (which means that at any time one of them has to occur). For instance, in the case of flipping a coin, there is a 0.5 (1/2 or 50%) chance that the coin will come up Heads, and 0.5 that it will be Tails. These are the only possibilities (this is a Platonic coin, so it has no thickness and thus cannot land on its side). A coin flip is a good example of an unconditional probability, which is the same regardless of the circumstances. For this, we would write:\n\\begin{align}\np(H)&=0.5 \\\\\np(T)&=0.5 \\\\\n\\end{align}\n\nwhich says that the probability of the coin coming up heads, p(H), is 0.5, and the probability of coming up tails, p(T), is 0.5.\nBut probabilities can also depend on the situation, such as the probability that you will buy lunch at a certain time. It is more likely that you will purchase lunch at 11:30 AM than at 10:00 AM. This is a conditional probability. Conditional probabilities are expressed as P(Lunch|Time), which translates as the probability of going for Lunch, (Lunch), is conditional, |, on the time, Time. For a conditional probability we need to know the time to give the probability that we are going to lunch.\nIn the case of our ERP decoder, you can say that the probability of a trial having a cue is conditional on the strength of the ERP, p(Cue|ERP). For this, we need an equation that describes how the probability of being a Cue trial varies as a function of ERP strength.\n\n\n\nOne equation that is a useful way to express a conditional probability is the logistic function (also known as the sigmoid function). It has the form:\n \\sigma(x) = \\frac{1}{1+e^{-x}} \nLet’s code it up and visualize it:\n\n# create a logistic function\ndef logistic(x):\n    return 1 / (1 + np.exp(-x)) \n\n# plot the logistic function\nx = np.linspace(-10, 10, 100)\nplt.plot(x, logistic(x))\nplt.title('Logistic Function')\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThis function is bounded between 0 and 1, just like probabilities. However, by itself it is not useful. It also has a probability of 0.5 when x=0, which is not good for us because our decision point, called the location parameter, for a cue trial usually had our ERP measure at a positive value.\nTo shift the location, we can modify x by subtracting the location value from it.\n \\sigma(x) = \\frac{1}{1+e^{-(x-loc)}}\nHow does this look?\n\n# create logistic functio with adjustable location\ndef logistic(x,loc=0):\n    return 1/(1+np.exp(-(x-loc)))\n\n# plot logistic function with different locations\nloc_list = np.arange(-5,6,2.5)\nx = np.linspace(-10,10,100)\n\nfor ind, loc in enumerate(loc_list):\n    plt.plot(x,logistic(x,loc),label='$loc={}$'.format(loc), color=[ind/len(loc_list), 0, 0])\n\nplt.yticks(np.arange(0,1.1,0.25))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.title('Logistic function with different locations')\nplt.grid()\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n\n\n\n\n\n\n\n\nThis looks promising, we can shift the point at which the logistic function crosses 0.5. However, the range of x values over which the logistic function varies, its scale, is narrow compared to the range of values we get for our dot product measures of the ERP. This may not seem important now, since a simple classification threshold at 0.5 probability does not care about the spread in our logistic function. But, it does matter if we want our function to characterize p(Cue|ERP), which can indicate how confident the model is in its prediction of whether a cue was present on a trial. Adjusting the scale is straightforward with the addition of a new parameter.\n \\sigma(x)=\\frac{1}{1+e^{-\\frac{x-loc}{scale}}} \nBy dividing x-loc by scale, we can stretch or contract the logistic function with respect to the x-axis. As you increase scale, the values of x have to get larger to push the output closer to 0 or 1. If you decrease scale, when only a small change in x is needed to have the logistic function return 0 or 1. We can see this below:\n\n# logistic function with location and scale parameters\ndef logistic(x, loc=0, scale=1):\n    return 1 / (1 + np.exp(-(x - loc) / scale))\n\n# plot the logistic function\nx = np.linspace(-10, 10, 100)\nloc = 0\nscale_list = np.power(2.0, range(-2, 3, 1))\n\nfor ind, scale in enumerate(scale_list):\n    plt.plot(x, logistic(x, loc, scale), label='$scale$ = {}'.format(scale), color=[0, 0, ind / len(scale_list)])\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.grid()\nplt.title('Logistic function with different scales')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n\n\n\n\n\n\n\n\nGreat, we can change the spread of the logistic function now. With these two degrees of freedom, location and scale, we are able to create a function that captures the probability of a certain class (in our case, a cue trial) occurring. An important caveat to this function is that it is monotonic, meaning that the probability of being that class only increases (or decreases) as one increases the value of x. This works for most cases, but not if our class occurs for only a restricted range of x values (e.g. between 2 and 5). In later lectures we will cover other types of classifiers that circumvent this limitation.\n\n\n\n\nUsing the logistic function requires us to choose values for the location and scale parameters, which we could try doing by hand. That is not recommended. Instead, we will use the LogisticRegression class in the scikit-learn package. This class defines an object that can take our measurements and true classes, fit a logistic function to that data, and then deliver class predictions. To start using this, we will cover how scikit-learn implements its fitting functions in general, and then the specifics of the LogisticRegression class.\n\n\nScikit-learn is a python package started in 2007 and has grown to include a wide variety of machine learning algorithms. Most of these are implemented as estimators, which are classes that allow one to fit a model or function to some data and then make predictions from that model. The project has adopted a uniform standard for the creation of estimators, making it easier to incorporate new ones into the project or develop your own that will comprehensible to users already familiar with scikit-learn. For details you can check out the online documentation for developers.\nEach estimator has an __init__ method that creates the estimator object. When calling this method, you can pass settings parameters that determine how the estimator will fit to the data. Generally, these settings are supposed to be independent of the data being fit to. The estimator then has a fit function, which accepts a data matrix X and predicted classes y. Additional parameters can be set here that affect the fitting process in ways specific to the data. Once the fit function has been called, you can evaluate the performance of the fit using the score method, or predict new classes from new data using the predict method.\nWe will step through these using the LogisticRegression class to fit our predictor of of whether a cue was present on a trial given the ERP.\n\n\n\nTo use the LogisticRegression class, we need to create an instance of it as an object. Parameters specifying how to fit the function to your data, using set using the __init__ method, are also accessible with a set_params method after you created the estimator. When no parameters are given to initialize the object, it takes on default values. These can be inspected in the documentation or using the estimator’s get_params method.\n\n# create a vanila LogisticRegression object\nclf = LogisticRegression()\n\n# examine its default fitting parameters\nclf.get_params()\n\n{'C': 1.0,\n 'class_weight': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'l1_ratio': None,\n 'max_iter': 100,\n 'multi_class': 'deprecated',\n 'n_jobs': None,\n 'penalty': 'l2',\n 'random_state': None,\n 'solver': 'lbfgs',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}\n\n\nThe only parameter worth mentioning at this time is fit_intercept, which determines if we will include a location argument in our fitting. By default this is set to True, so we don’t have to worry about explicitly setting it.\n\n\n\nNow that we have our LogisticRegression object, we can call its fit method. It accepts arrays containing your independent, x, and dependent, y variables and optimizes the function to best fit that data. The first parameter is X, which is an array of measurements. It has 2 dimensions, with each row a different sample (e.g. trial), and each column a different feature (e.g. ERP peak voltage). Its shape is (n_samples, n_features), where ‘n’ stands for ‘number of …’. You can have multiple features per sample, which allows us to use more than one aspect of the measured brain activity on a trial to decode whether a stimulus was present. For now we will just use the one, our measure of ERP strength on a given trial.\nThe next parameter is y, the true class labels (e.g. Cue or NoCue) labels. Its shape is (n_samples,), where n_samples is the same as X. Notice that X is uppercase and y is lowercase. This is because a common convention is that a matrix (a 2-D array) is represented by an uppercase character, while vectors (1-D array) use lowercase characters.\n\n# format X\n# add dimension to X, because X needs to be a 2D array\nvals = vals[:, np.newaxis]\nprint('The shape of X is {}'.format(vals.shape))\n\n# class_act is already formatted correctly\nprint('The shape of y is {}'.format(class_act.shape))\n\n# fit the model\nclf.fit(vals, class_act)\n\nThe shape of X is (54, 1)\nThe shape of y is (54,)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\nThat’s it, we fit the model! A bit anticlimactic, since all we did was update the clf object with a fitted model. The next thing we need to do is evaluate the performance of the fitted model.\n\n\n\nThere are couple ways to judge the performance of our model. When we implemented our simple threshold decision model above, we used error rate and accuracy as our measures. The LogisticRegression class provides a method named score that calculates accuracy. It accepts X and y parameters similar to the fit method. Note: you do not have to provide the same X and y used for fit, but can use new data that the model was not trained on (in fact, this is a better practice as we will discuss later). It returns a number between 0 and 1, with 1 being perfect classification, and 0 being totally incorrect classification. This single number provides a good top line indication of whether the fit was successful. Poor accuracy will usually not be at 0, but at a level consistent with random guessing (typically 1 divided by the number of options). For binary decoding with equal numbers of each class, this would be 0.5 (1/2).\n\n# calculate the score for our model\nscore = clf.score(vals, class_act)\n\nprint('Accuracy for our model: {:.2f}%'.format(score*100))\n\nAccuracy for our model: 87.04%\n\n\nThe logistic regression decoder out performs our simple threshold classifier we set by eye, which was 85.2%. Not by much, but better nonetheless. Just as a sanity check, let’s see what happens if we scramble the actual class labels. In this case, the performance of the classifier should be random, since we have removed any correspondence between the ERP strength and whether the trial had a cue or not.\n\nclf_rand = LogisticRegression()\nclf_rand.fit(vals, np.random.permutation(class_act))\nscore_rand = clf_rand.score(vals, np.random.permutation(class_act))\n\nprint(\"Random prediction accuracy: %.2f%%\" % (score_rand * 100))\n\nRandom prediction accuracy: 53.70%\n\n\nIf we run this cell a few times we can see that the average accuracy is around 50%, which is what we would expect from random performance. However, random performance of a binary classifier does not need to be at 50%. If there are more of one of the classes over the other, then the decoder could perform better than 50%. Let’s remove a bunch of the No-cue trials and see if that can affect our chance performance.\n\n# Create new logistic regression classifier for unbalanced data set\nclf_ub = LogisticRegression()\n\n# remove some no-cue trials\nvals_ub = vals[:-20].copy() # remove last 20 trials, which are no-cue trials\nclass_act_ub = class_act[:-20].copy() # remove last 20 trials, which are no-cue trials\n\nclf_ub.fit(vals_ub, class_act_ub)\nclass_pred_ub = clf_ub.predict(vals_ub)\nscore_ub = clf_ub.score(vals_ub, class_act_ub)\nprint('Accuracy for unbalanced model: {:.2f}%'.format(score_ub*100))\n\n# randomly permute (shuffle) the class labels\nnp.random.seed(43)\nclass_rand_ub = np.random.permutation(class_act_ub)\nclf_ub.fit(vals_ub, class_rand_ub)\nscore_rand_ub = clf_ub.score(vals_ub, class_rand_ub)\nprint('Accuracy for unbalanced random model: {:.2f}%'.format(score_rand_ub*100))\n\nAccuracy for unbalanced model: 94.12%\nAccuracy for unbalanced random model: 73.53%\n\n\nNotice that the accuracy has gone up for our model, even for the random case! This is because the model optimized its performance simply by biasing its response to indicate Cue trials instead of No-cue trials. This works because the data set is unbalanced, with much fewer No-cue trials compared with Cue trials.\nIs there a way to account for this? The metrics section of scikit-learn provides a range of performance score functions that measure other aspects of classifier performance or control for issues in the data set. In this case, we are dealing with unbalanced data. To address that, we can use the function balanced_accuracy_score. It works by calculating the accuracy for each class separately, and then taking the average across classes. In this way, if one class is overexpressed and classified correctly more often due to bias, then its contribution to the accuracy score will be downgraded, while the less well represented class will have its score enhanced. Specifically, for the binary classification we are doing here, the is calculated using the following equation:\n Balanced Score = \\frac{1}{2} \\left( \\frac{TP}{TP+FN}+\\frac{TN}{TN+FP}  \\right) \nHere we will introduce a few new terms that are going to come up in discussing classifiers. Each trial has an actual condition or label associated with it, and a label predicted by the model. The terminology here comes from the signal detection literature, where one is trying to determine if a particular event occurred or did not on a given trial. Our case fits into this framework nicely: was a cue present or not. When a cue is present we refer to it as a Positive trial, while the No-cue trial is a Negative trial. When the classifier correctly labels a trial, we say that trial is True, and incorrectly labeling a trial makes it False. For instance, a true positive trial would have the cue and be labeled as a cue trial by the logistic regression model. We can list all possible trial types using this phrasing.\n\nTP - True Positive, event occurred and was predicted as occurring * Actual Cue / Predicted Cue\nTN - True Negative, event did not occur and was predicted as not occurring * Actual No-cue / Predicted No-cue\nFP - False Postive, event did not occur but was predicted as occurring * Actual No-cue / Predicted Cue\nFN - False Negative, event occurred but was predicted as not occurring * Actual Cue / Predicted No-cue\n\nLooking back at the equation for balanced scores, we can break it down into something a bit more interpretable:\n \\begin{align}\n    \\notag \\frac{TP}{TP+FN} &= \\frac{\\text{Number of correctly predicted cue labeled trials}}{\\text{Number of all actual cue labeled trials}} = \\text{Proportion correctly labeled cue trials} \\\\\n    \\notag \\frac{TN}{TN+FP} &= \\frac{\\text{Number of correctly predicted no-cue labeled trials}}{\\text{Number of all actual no-cue labeled trials}} = \\text{Proportion correctly labeled no-cue trials} \\\\\n    \\end{align}\n\nThis means the balanced score equation is just taking the average correct proportion across both classes, regardless of how many trials belonged to either class.\nIf you are working with unbalanced data, then using the balanced accuracy score is essential for interpretable accuracy scores. When doing that, you should use the built in one provided by scikit-learn, the function balanced_accuracy_score. An additional advantage of the scikit-learn version is that it supports classifiers with more than two outcomes, which we will make use of later in the semester.\n\n# calculate balanced accuracy scores using the function provided by scikit-learn\nscore_bal_ub = balanced_accuracy_score(class_act_ub, class_pred_ub)\nprint(\"Balanced accuracy score for our model: {:.2f}\".format(score_bal_ub*100))\n\nscore_bal_ub = balanced_accuracy_score(class_act_ub, class_rand_ub)\nprint(\"Balanced accuracy score for random model: {:.2f}\".format(score_bal_ub*100))\n\nBalanced accuracy score for our model: 88.89\nBalanced accuracy score for random model: 54.67\n\n\nThe accuracy has gone down now, with the random case close to 50%, as would be expected. Normally we will use balanced data sets so there should not be much of a difference between the score returned by the estimator object and the balanced accuracy metric.\n\n\n\n\n\n\nCoding our own balanced accuracy score function\n\n\n\nTo better understand the balanced accuracy score, we can try coding our own. This should make things a bit clearer.\n\n# balanced accuracy score in base python\ndef bal_acc_score(y_act, y_pred):\n\n    # initialize variables\n    tp = 0; tn = 0; fp = 0; fn = 0\n\n    # loop through each pair of actual and predicted labels\n    for act, pred in zip(y_act, y_pred):\n        if act == 1 and pred == 1: # true positive\n            tp += 1\n        elif act == 1 and pred == 0: # false negative\n            fn += 1\n        elif act == 0 and pred == 1: # false positive\n            fp += 1\n        elif act == 0 and pred == 0: # true negative\n            tn += 1\n    \n    # calculate proportion of correct predictions for each class\n    prop_corr_pos = tp / (tp + fn)\n    prop_corr_neg = tn / (tn + fp)\n\n    # return average of the two proportions\n    return (prop_corr_pos + prop_corr_neg) / 2\n\nLet’s make sure our homemade version agrees with the one in scikit-learn.\n\n# see how our scores have changed using the balanced accuracy score metric\nscore_bal_ub = bal_acc_score(class_act_ub, class_pred_ub)\nprint(\"Balanced accuracy score for our model: {:.2f}\".format(score_bal_ub*100))\n\nscore_bal_ub = bal_acc_score(class_act_ub, class_rand_ub)\nprint(\"Balanced accuracy score for random model: {:.2f}\".format(score_bal_ub*100))\n\nBalanced accuracy score for our model: 88.89\nBalanced accuracy score for random model: 54.67\n\n\nYay, they agree!\n\n\nThe score measures above take actual and predicted class labels, compare and combine them, and return a single number that reflects the performance of our classifier. This is good as a general summary of how well the classifier works, but it leaves out details about how the classifier achieves that performance. For instance, with our case, is our model really good at detecting cue trials, or at no-cue trials? Does performance vary depending on the type of trial?\nA way to easily visualize the performance of a classifier in all its gritty details is with a confusion matrix. In a confusion matrix, each row corresponds to an actual label, and each column the predicted label. Each cell in the matrix has the number of trials with that particular pairing of actual and predicted labels. For a binary classifier, this looks like:\n Confusion \\: Matrix = \\begin{pmatrix}\n                        TN &FP \\\\\n                        FN &TP\n                    \\end{pmatrix}\n\nAlong the diagonal, we have the number of trials that were correctly classified for each label type. Off-diagonal elements show the number of confused (i.e. mislabeled) instances, hence the name. The instances most prone to confusion helps us diagnose problems in our classifier. As a case study, let’s examine the confusion matrices for our classifiers trained on balanced and unbalanced data sets.\nThe metrics portion of the scikit-learn package provides functions for computing confusion matrices.\n\n# use balanced model to predict the class for each observation\nclf.fit(vals, class_act)\nclass_pred = clf.predict(vals)\n\n# PLOT THE CONFUSION MATRIX\n# initialize the figure\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# calculate confusion matrix\nconfmat_full = confusion_matrix(class_act, class_pred)\n\n# plot the confusion matrix using built-in function\nConfusionMatrixDisplay(confusion_matrix=confmat_full, \n                       display_labels=['No-cue', 'Cue']).plot(ax=ax[0], cmap=plt.cm.Blues)\n\n# redo the same for the unbalanced dataset\nconfmat_ub = confusion_matrix(class_act_ub, class_pred_ub)\nConfusionMatrixDisplay(confusion_matrix=confmat_ub, \n                       display_labels=['No-cue', 'Cue']).plot(ax=ax[1], cmap=plt.cm.Blues)\n\n# label the figure\nax[0].set_title('Balanced dataset')\nax[1].set_title('Unbalanced dataset')\nfig.suptitle('Confusion matrices', fontsize=16)\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe confusion matrix shows that both classifiers works well. High counts along the diagonal indicate that labels are correctly predicted. For the balanced case, the misclassifications tend to be equal for Cue and No-cue trials. On the other hand, when we decreased the number of No-cue trials misclassifications only occurred for the No-cue trials. Moreover, we had fewer misclassifications for the Cue trials, which were now overrepresented in the data. Thus, the off diagonal elements tell us how the model trained on data with too many Cue trials is biased perform better on that trial type.\nThat error suggests that the decision boundary of our logistic model is biased towards lower ERP strengths for the unbalanced data. In this way, it will tend to never miss a cue trial, and classify No-cue trials as cue trials. To determine if this is the case, we might want to visualize the logistic functions fitted to our balanced and unbalanced datasets.\nTo do this, we can use the predict_proba method. This takes samples, not necessarily the ones we fitted on, and returns the probability of either class for each sample. A good strategy here to is take the minimum and maximum values of the features in your dataset, and then create a grid of samples that span their range. This works really well when we have less than 3 features per sample, or in our case just 1 feature. If we plot the predicted probabilities across values of x, we can visualize the logistic function.\n\n# create range of feature values\nmin_erp_peak = np.min(vals)\nmax_erp_peak = np.max(vals)\nerp_peak_vals = np.linspace(min_erp_peak, max_erp_peak, 100)\nerp_peak_vals = erp_peak_vals[:, np.newaxis]\n\n# predict probabilities over range of values\npred_probs = clf.predict_proba(erp_peak_vals)\nprint('The shape of pred_probs is: {}'.format(pred_probs.shape))\n\n# plot predicted probability of each class\nplt.plot(erp_peak_vals, pred_probs[:,0], label='NoCue')\nplt.plot(erp_peak_vals, pred_probs[:,1], label='Cue')\nplt.xlabel('ERP Peak (uV)')\nplt.ylabel('Predicted Probability')\nplt.title('Predicted Probability vs. ERP Peak')\nplt.yticks(np.arange(0,1.1,0.25))\nplt.grid()\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n\nThe shape of pred_probs is: (100, 2)\n\n\n\n\n\n\n\n\n\nNote that predict_proba returns a matrix with the same number of rows as the samples we passed in erp_peak_vals, and two columns, one for each class (NoCue or Cue). Since we only have two classes, the probabilities of NoCue and Cue will add to 1 for each sample. This makes one of them redundant, so for future plotting we will drop the NoCue one.\nIt is helpful to plot our classes with the data points layered on top, which helps us better visualize the performance of the fit and whether it is falling prey to some issues that will be discussed later.\n\ndef plot_logistic(lf_obj, vals, class_act, ax=None, labels=['NoCue', 'Cue'], color='tab:blue'):\n    \"\"\"Plot the logistic function for a given logistic regression object\n    and a range of values.\n\n    Parameters\n    ----------\n    lf_obj : sklearn.linear_model.LogisticRegression\n        The logistic regression object.\n    vals : array-like\n        The X values used to train the logistic model.\n    classes : array-like\n        The y values used to train the logistic model.\n    ax : matplotlib.axes.Axes\n        The axes to plot on.\n    \"\"\"\n\n    if ax is None:\n        _, ax =  plt.subplots()\n    \n    min_val = np.min(vals)\n    max_val = np.max(vals)\n    val_grid = np.linspace(min_val, max_val, 100)\n    val_grid = val_grid[:, np.newaxis]\n\n    # predict probabilities over range of values\n    lf_obj.fit(vals, class_act) # refit the model, because for some reason passing it as an argument removes the fit\n    pred_probs = lf_obj.predict_proba(val_grid)\n    ax.plot(val_grid, pred_probs[:,1], label='Logistic model', color=color)\n    ax.scatter(vals, class_act, c=color, alpha=0.5, label='True data')\n    ax.set_yticks(np.arange(0,1.1,0.25))\n    ax.set_yticklabels([labels[0], '0.25', '0.5', '0.75', labels[1]])\n    ax.grid()\n\nfig, ax = plt.subplots(2,1, sharex=True, sharey=True)\nplot_logistic(clf, vals, class_act, ax=ax[0])\nplot_logistic(clf_ub, vals_ub, class_act_ub, ax=ax[1], color='tab:orange')\nfig.suptitle('Logistic regression')\nfig.supxlabel('ERP dot product (uV)')\nfig.supylabel('P(Cue|ERP)')\nax[0].set_title('Balanced data')\nax[1].set_title('Unbalanced data')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe balanced logistic function seems to reflect the p(Cue|ERP) quite well. Its probability increases strongly after the bulk of the NoCue ERP peaks, and is near 1 when only Cue ERP peak values remain. The one trained on the unbalanced is shifted to the left, towards lower ERP values, biasing its classification towards the overrepresented Cue trials.\nMoving beyond this qualitative way of describing our logistic fit, it would be better to do so quantitatively, using the location parameter we discussed before. However, here we will have to make things a little more complicated by changing how we parameterize the model. This initial pain is worth it, though, because later it will give us far more freedom in how we can use the logistic function.\n\n\n\nThe parameters of the logistic function tell us how it assigns a class probability to a measure. When we first introduced it, we used location and scale parameters. These are not used by the LogisticRegression class. Instead, it is formulated as:\n \\sigma(x) = \\frac{1}{1+e^{-(b+wx)}} \nHere b stands for the bias or intercept, which is similar, but not exactly the same as the location parameter. w is the slope of the dependence on x, and is similar, but not exactly the same as the scale parameter.\nHow do these new parameters affect the logistic function?\n\ndef logistic(x, b=0, w=1):\n    return 1 / (1 + np.exp(-(w * x + b)))\n\n# plot the logistic function as b is varied\nx = np.linspace(-10, 10, 100)\nb_list = np.arange(-5,6,2.5)\nfor idx, b in enumerate(b_list):\n    plt.plot(x, logistic(x, b=b), label=f'$b$={b}', color=[idx/len(b_list), 0, 0])\n#plot legend outside axes\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.yticks(np.arange(0,1.1,.25))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.title('Logistic Function as $b$ is varied')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nLooks like b has exactly the same effect as location. What if we vary w?\n\n# plot logistic function as w is varied\nw_list = np.power(2.0, np.arange(-2, 3))\n\nfor idx, w in enumerate(w_list):\n    plt.plot(x, logistic(x, b=0, w=w), label='$w$={}'.format(w), color=[0,0,idx/len(w_list)])\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.title('Logistic Function as $w$ is varied')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nIt may look the same, but look closely at the line colors. When we increased the scale parameter, the spread of the logistic function decreased. But in this case, increasing the w parameter decreases the spread. And it gets worse, look what happens when we vary w and b is not set to 0:\n\n# plot logistic function as w is varied and b is not zero\nnew_b = -1\nfor idx, w in enumerate(w_list):\n    plt.plot(x, logistic(x, b= new_b, w=w), label='$w$={}'.format(w), color=[0,idx/len(w_list),0])\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.title('Logistic Function as $w$ is varied and $b$ is fixed at {}'.format(new_b))\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWhen b is not at zero, then changing w affects the location of the logistic function. You can see this if you follow where each curve crosses the 0.5 line, which is the threshold for classifying a trial as receiving a Cue. We can use some simple algebra to resolve the relationship between location, w, and b.\nFirst, recognize that the loc parameter specifies when the logistic function is equal to 0.5. This happens when x = 0:\n \\begin{align}\n        \\notag 0.5&=\\frac{1}{1+e^{-x}} \\\\\n        \\notag 0.5&=\\frac{1}{1+e^{-0}} \\\\\n        \\notag 0.5&=\\frac{1}{1+1} \\\\\n        \\notag 0.5&=\\frac{1}{2}\n    \\end{align}\n\nHow does b and w relate to the x=0 location?  \\begin{align}\n        \\notag 0&=-(b+wx) \\\\\n        \\notag 0&=-b-wx \\\\\n        \\notag b&=-wx \\\\\n        \\notag -\\frac{b}{w}&=x\n    \\end{align}\n\nThe location parameter is important to us when interpreting the logistic function because it tells us when the classifier will label an ERP as arising from a Cue. It is the decision boundary. To get that, we need to pull out the b and w parameters from the fitted LogisticRegression object.\n\n# get b, also known as the intercept parameter\nb = clf.intercept_\nprint('The shape of intercept_ is: {}'.format(b.shape))\nprint('The intercept of our model is {:.2f}'.format(b[0]))\n\nThe shape of intercept_ is: (1,)\nThe intercept of our model is -4.37\n\n\nintercept_ is a class variable created once we call the fit method. It is a numpy array with a single value. To get w, access the coef_ variable.\n\n# get w from coef_\nw = clf.coef_\nprint('The shape of coef_ is: {}'.format(w.shape))\nprint('The value of coef_ is: {:.2f}'.format(w[0,0]))\n\nThe shape of coef_ is: (1, 1)\nThe value of coef_ is: 0.13\n\n\nNow that we have b and w, we can precisely position the decision boundary for our fitted logistic function.\n\n# calculate the decision boundary for balanced model\ndec_bound = -b[0]/w[0,0]\n\n# calculate the decision boundary for unbalanced model\nb_ub = clf_ub.intercept_\nw_ub = clf_ub.coef_\ndec_bound_ub = -b_ub[0]/w_ub[0,0]\n\n# print comparison of decision boundaries\nprint('Decision boundary for balanced model: {:.2f}'.format(dec_bound))\nprint('Decision boundary for unbalanced model: {:.2f}'.format(dec_bound_ub))\n\n# plot the logistic regression with decision boundaries marked\nfig, ax = plt.subplots(2,1, sharex=True, sharey=True)\nplot_logistic(clf, vals, class_act, ax=ax[0])\nplot_logistic(clf_ub, vals_ub, class_act_ub, ax=ax[1], color='tab:orange')\nfig.suptitle('Logistic regression')\nfig.supxlabel('ERP dot product (uV)')\nfig.supylabel('P(Cue|ERP)')\nax[0].set_title('Balanced data')\nax[1].set_title('Unbalanced data')\nax[0].axvline(dec_bound, color='r', linestyle='--')\nax[1].axvline(dec_bound_ub, color='r', linestyle='--')\nfig.tight_layout()\n\nDecision boundary for balanced model: 32.88\nDecision boundary for unbalanced model: 23.89\n\n\n\n\n\n\n\n\n\nUsing the logistic model’s internal parameters, we can explicitly calculate the decision boundary for classifying Cue trials. Doing this, we found that the model trained on balanced data has a boundary ~9 uV higher than the one trained on unbalanced data. This underscores the importance of considering the data ones trains the model on. Biases in the data will be reflected in the trained model, and in many cases impair predictions on new data it had not been trained on.\n\n\n\nOnce the model is fitted, you want to use it to fit new samples. Since we used all our data to fit the model, predicting on the same data would be circular. The model was optimized to perform best on the data set it was trained on, not the data it will receive following training. By chance, there may be tendencies in the training data that are not expressed in newly acquired trials. Since we do not have a time machine that allows us to go into the future and acquire the data the model will eventually have to classify (which, for that matter, would eliminate the need for a model anyway), we need a way to estimate how the model will perform on data it has not been trained on.\nThis is a well-recognized problem in evaluating the performance of statistical models, with a well-established solution - cross-validation. Instead of training the model on all our data, we train on a subset of the data, and test on the remainder that was held out. To do this, we need to divide our data into train and test sets, recalculate the mean ERP using the training set, fit the logistic model using the training set, and test the performance on the test set.\nScikit-learn provides a collection of functions for subdividing data and some of their estimators have versions with built-in cross-validation. We will make use of those, but first let’s code our own just to get a sense of how they work. To start, we will code a function to split our data into train and test sets.\nWhen specifying the size of the training and test datasets, we usually decide how many sets (or folds), k, we want to split the data set into. The model is trained on the most data possible, k-1 sets (\\frac{k-1}{k} % of data), and the last set is held out for testing (\\frac{1}{k} % of data). Deciding on a value for k requires balancing two needs. First, the model should be exposed to the most training data possible, so that it can be optimized on a representative sample. This inclines us to set k to a high value, so that a larger fraction of the data goes in to training. Second, we want to test the model on a representative sample as well, so that we get a more precise estimate of its performance. This inclines us to set k to a low value, which maximizes the data we test on. These conflicting demands need to be balanced based on the number of features the model is being trained on, how large the data set is, and the tendency of the model to overfit. All those issues will vary across situations, but a good rule of thumb is to set k=5. This means that 80% (4/5) of your data will be used for training, while the remaining 20% (1/5) is held in reserve for testing.\nWhen splitting our data, we also must decide how to do the actual splitting. The simplest approach is to split the data into sets based on their order. In that case, the first \\frac{4}{5}\\text{n\\_samples} are used for training, and the remaining \\frac{1}{5}\\text{n\\_samples} for testing. If you data is randomly ordered, so that different classes are equally represented at the beginning and end of the dataset, then this will typically work. On the other hand, if you data is like ours, where the first half of the samples are Cue trials, and the second half No-cue trials, then your train and test data sets will have different proportions of trial types and not be representative samples of the entire data set. To account for this, the samples you assign to the train or test data sets can be chosen randomly. On average, this should result in an equal proportion of samples for the different classes in the train and test sets. Let’s implement that:\n\n# Function to generate indices for train/test sets using k-fold random assignment\ndef train_test_kfold(n_samples, k=5):\n    # n_samples, number of samples in the dataset\n    # k, int, number of folds\n\n    fold_size = n_samples/k # number of samples in each fold\n    test_bool = np.arange(0, n_samples)&lt;=fold_size # boolean array of test indices\n    test_bool = np.random.permutation(test_bool) # randomize the test indices\n    test_idxs = np.where(test_bool==True)[0] # get the indices of the test set\n    train_idxs = np.where(test_bool==False)[0] # get the indices of the train set\n    return test_idxs, train_idxs\n\nOur function takes just two arguments, the number of samples, n_samples, and the number of folds, k, and returns randomly chosen indices for the test (test_idxs) and train (train_idxs) sets. Because we are assigning samples to the train and test groups at random, we should get equal proportions of Cue trials (or No-cue trials) in both sets. Before going further, we should make sure that is the case.\n\nnum_runs = 1000\nprop_cue_test = np.zeros(num_runs)\nprop_cue_train = np.zeros(num_runs)\nfor i in range(num_runs):\n    test_idxs, train_idxs = train_test_kfold(len(class_act))\n    prop_cue_test[i] = class_act[test_idxs].mean()\n    prop_cue_train[i] = class_act[train_idxs].mean()\n\nfig, ax = plt.subplots(2,1, sharex=True, sharey=True)\nax[0].hist(prop_cue_test, bins=np.linspace(0,1,20), alpha=0.5)\nax[1].hist(prop_cue_train, bins=np.linspace(0,1,20), alpha=0.5)\nax[0].set_title('Test set, mean proportion = {:.2f}'.format(prop_cue_test.mean()))\nax[1].set_title('Train set, mean proportion = {:.2f}'.format(prop_cue_train.mean()))\nfig.supylabel('Count')\nfig.supxlabel('Proportion of cues')\nfig.tight_layout()\nplt.show()\n\n# proportion of Cue trials across all trials\nprint('Proportion of Cue trials across all trials: {:.2f}'.format(class_act.mean()))\n\n\n\n\n\n\n\n\nProportion of Cue trials across all trials: 0.46\n\n\nOn average, random assignment of trials yields equal proportions of Cue trials to test and train sets. The variance is greater, though, for the smaller test set. Sometimes the test set will be dominated by Cue or No-cue trials, so for those cases will not offer a balanced assessment of classifier performance. To avoid this, we want to stratify assignment of trials to the test and train sets by the trial type. To do this, we will generate train and test sets for each class of trials separately, and then combine them.\n\n# Function to generate indices for train/test sets using stratified k-fold random assignment\ndef train_test_kstrat(class_act, k=5):\n    # class_act, array-like, shape (n_samples,)\n    # k, int, number of folds\n\n    # gets unique classes\n    uniq_classes = np.unique(class_act) \n\n    # initialize empty array for train indices\n    train_idxs = np.array([]) \n\n    # for each class, get indices of samples in class_ and add k-1/k of them to train_idxs\n    for class_ in uniq_classes: # variable named class_ to avoid confusion with 'class' keyword\n        # get indices of samples in class_\n        class_idxs = np.where(class_act==class_)[0] \n\n        # number of samples in class_\n        n_class = len(class_idxs) \n\n        # check if there are enough samples in class_ for k-fold cross validation\n        if n_class &lt; k:\n            raise ValueError(\"k-fold cross validation requires at least k samples in each class\")\n        \n        n_train = int(n_class*(k-1)/k) # number of from class_ in train set\n\n        # add train indices to train_idxs\n        train_idxs = np.append(train_idxs, np.random.choice(class_idxs, n_train, replace=False)) \n    \n    # indices not selected for the test group are assigned to the train group\n    test_idxs = np.setdiff1d(np.arange(0, len(class_act)), train_idxs) \n\n    return test_idxs.astype(int), train_idxs.astype(int) # return indices as integers\n\nThe stratified train/test splitting function assigns a random subset of samples from each class into a train group. The size of the subset is dictated by the k and number of samples of that class, ensuring that the proportion of samples used in the train and test set is the same for each class. This should result in the same proportions every time we call this splitting function.\n\n# get the distribution of Cue trials across multiple runs of the stratified train-test split\nnum_runs = 1000\nprop_cue_test = np.zeros(num_runs)\nprop_cue_train = np.zeros(num_runs)\nfor i in range(num_runs):\n    test_idxs, train_idxs = train_test_kstrat(class_act)\n    prop_cue_test[i] = class_act[test_idxs].mean()\n    prop_cue_train[i] = class_act[train_idxs].mean()\n\nfig, ax = plt.subplots(2,1, sharex=True, sharey=True)\nax[0].hist(prop_cue_test, bins=np.linspace(0,1,20), alpha=0.5)\nax[1].hist(prop_cue_train, bins=np.linspace(0,1,20), alpha=0.5)\nax[0].set_title('Test set, mean proportion = {:.2f}'.format(prop_cue_test.mean()))\nax[1].set_title('Train set, mean proportion = {:.2f}'.format(prop_cue_train.mean()))\nfig.supylabel('Count')\nfig.supxlabel('Proportion of cues')\nfig.tight_layout()\nplt.show()\n\n# proportion of Cue trials across all trials\nprint('Proportion of Cue trials across all trials: {:.2f}'.format(class_act.mean()))\n\n\n\n\n\n\n\n\nProportion of Cue trials across all trials: 0.46\n\n\nStatifying the assignment of samples based on their class has eliminated variability in their proportions. One drawback, though, is that the mean proportion is subtly different between the train and test sets, with 0.45 and 0.47 respectively. This bias happens because we cannot evenly split the samples from the Cue class into train and test sets, and since the same number of Cue samples are drawn each time, this difference persists across runs. By minimizing the variability in the proportions, we have introduced a bias in the proportions. There is a well-known bias-variance trade-off in statistics that speaks to this.\nReturning to the prediction question, we want to determine how our logistic regression model performs on data it was not trained on. Since the selection of trials in the test and train sets is random, a single estimate of performance would not tell us how the decoder performs in general. To estimate the performance, we will run it a thousand times. In addition, we will evaluate the performance on both the test and train data sets.\n\n# Fit and evalulate logistic regression model to ERP data using cross-validation\ndef logreg_traintest(epochs, class_act, k):\n    test_idxs, train_idxs = train_test_kstrat(class_act, k=5)\n\n    # set union of class_act and train_idxs\n    train_cue_idxs = np.intersect1d(np.where(class_act)[0], train_idxs)\n\n    # calculate ERP for training set\n    cue_erp = np.mean(epochs[:, train_cue_idxs], axis=1)\n\n    # calculate ERP alignment for each trial\n    val_all = erp_align(epochs, cue_erp)\n\n    # create train and test sets\n    val_test = val_all[test_idxs, np.newaxis]\n    class_act_test = class_act[test_idxs]\n    val_train = val_all[train_idxs, np.newaxis]\n    class_act_train = class_act[train_idxs]\n\n    # fit logistic regression model\n    clf_sub = LogisticRegression()\n    clf_sub.fit(val_train, class_act_train)\n\n    # get performance for train and test sets\n    score_test = clf_sub.score(val_test, class_act_test)\n    score_train = clf_sub.score(val_train, class_act_train)\n    return score_train*100, score_test*100\n\n# Run logistic regression model on ERP data\nnum_runs = 1000\nall_epochs = np.concatenate((cue_epochs, nocue_epochs), axis=1)\nscores_train = np.zeros(num_runs)\nscores_test = np.zeros(num_runs)\nfor i in range(num_runs):\n    scores_train[i], scores_test[i] = logreg_traintest(all_epochs, class_act, 5)\n\n# plot violin plot of scores compared across train and test sets\nfig, ax = plt.subplots(figsize=(2,5))\nax.violinplot([scores_train, scores_test], showmedians=True)\nax.set_xticks([1, 2])\nax.set_xticklabels(['Train', 'Test'])\nax.set_ylabel('Accuracy (%)')\nax.grid(axis='y')\nax.set_title('Cross-validated logistic regression performance')\n\n# print median performance for train and test sets\nprint('Median train accuracy: {:.2f} %'.format(np.median(scores_train)))\nprint('Median test accuracy: {:.2f} %'.format(np.median(scores_test)))\n\nMedian train accuracy: 86.05 %\nMedian test accuracy: 81.82 %\n\n\n\n\n\n\n\n\n\nPerformance on the train set is close to the performance achieved when we trained on the model on the entire data set. When the model was tested on the test set, performance was more variable. It is skewed to lower performance, sometimes much worse than the train set. We can compare the performance for each pair of train and test sets to determine if there was a systematic difference in performance.\n\n# difference in scores by train and test sets\nscore_diff = scores_test - scores_train\n\n# calculate median difference\nmedian_diff = np.median(scores_test - scores_train)\nprint(\"Median difference: {:0.2f} %\".format(median_diff))\n\n# plot distribution of score differences\nplt.hist(score_diff,20, label=\"Distribution\")\nplt.xlabel(\"Test score - train score\")\nplt.ylabel(\"Count\")\nplt.title(\"Histogram of score differences\")\nplt.axvline(median_diff, color='r', linestyle='--', linewidth=2, label=\"Median\")\nplt.legend()\nplt.show()\n\nMedian difference: -6.45 %\n\n\n\n\n\n\n\n\n\nThe median performance dropped ~7% on the test set. However, the extremes of the differences are wide, sometimes performing worse by 50% or better by 20%. It is common for performance to decrease on the test set, and the small drop we see here on average is not bad.\nBefore ending this section, we will go over how scikit-learn implements data splitting. It features a variety of splitting schemes, whose documentation can be found here.\nTo segregate data into train and test sets, you use a splitter object. Splitter objects are implemented similar to estimator objects. You first initialize them with parameters that are agnostic to the dataset, and then pass your data to the object’s split method, which returns indices for the train and test sets. Each time you call split, a new subset of indices is generated. To do this, it is implemented as type of iterator object (e.g. python’s range function) known as a generator object. Usually iterators are python objects that have an __iter__ method, but in the case of the splitter objects they use yield in their split method to return a new set of indices each time they are called.\n\n# initialize the KFold object\nksplit = KFold(n_splits=5, shuffle=True, random_state=47)\n\nThe first parameter for KFold is n_splits, which specifies how many folds, k, to use. shuffle sets whether the indices selected for the sets should be randomly drawn from across the data set. By default it is set to False, which will select indices in the order they occur in the dataset. Lastly, when randomly shuffling the indices we can specify the random seed to use using the random_state parameter so that the same random subset can be selected each time the function is called.\nNow you might be tempted to get the indices for the train and test sets by directly calling the split method, such as:\n\nksplit.split(vals)\n\n&lt;generator object _BaseKFold.split at 0x15fa68d60&gt;\n\n\nBut, this does not return what we want. Instead, it delivers a generator object that is not the indices we want. To get the indices, we have to call the split method as part of a loop, which engages its behavior as an iterator.\n\n# print the indices for each test set\nfor run_num, (train_idxs, test_idxs) in enumerate(ksplit.split(vals)):\n    print(\"Test indices on run {}: {}\".format(run_num+1, test_idxs))\n\nTest indices on run 1: [12 14 24 32 35 36 40 41 43 44 46]\nTest indices on run 2: [10 11 13 18 19 21 27 29 30 33 37]\nTest indices on run 3: [ 3  4  5 15 17 20 22 25 38 39 52]\nTest indices on run 4: [ 0  1  2  9 26 28 31 34 42 47 49]\nTest indices on run 5: [ 6  7  8 16 23 45 48 50 51 53]\n\n\nRepeated calls to the split method in the for loop returns a new set of indices each time. Notice that each set is distinct, no overlap between sets. This is because one often runs cross-validation multiple times and to ensure that independent data sets are used when measuring performance. This restriction means that the total number of runs for generating a cross-validation set is the number of splits, k. If you want just a single run to be returned, you can use the python function next.\n\ntrain_idxs, test_idxs = next(ksplit.split(vals))\nprint(\"Train indices: {}\".format(train_idxs))\nprint(\"Test indices: {}\".format(test_idxs))\n\nTrain indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 15 16 17 18 19 20 21 22 23 25 26\n 27 28 29 30 31 33 34 37 38 39 42 45 47 48 49 50 51 52 53]\nTest indices: [12 14 24 32 35 36 40 41 43 44 46]\n\n\nNormally when you call next with an iterator it will return the a different output each time it is called. This is not the case for the splitter objects, so if you want to get a new set of indices (and do not mind them overlapping with the previous set), then you have to reinitialize the splitter object each time.\nIf you want the splits to be balanced in their proportion of trial types, you can use a stratified approach to generate the train and test sets. This is done with the StratifiedKFold object. It is used similarly to KFold, except that when calling split you pass both the X and y parameters. y is used for stratifying the selection of indices by the classes. Here is how to use it:\n\n# initialize the KFold object\nkstratsplit = StratifiedKFold(n_splits=5, shuffle=True, random_state=47)\n\n# print the indices for each test set\nfor run_num, (train_idxs, test_idxs) in enumerate(kstratsplit.split(vals, class_act)):\n    print(\"Test indices on run {}: {}\".format(run_num+1, test_idxs))\n\nTest indices on run 1: [ 4  5  6 11 13 42 43 44 48 49 53]\nTest indices on run 2: [ 0  2 21 23 24 28 30 33 36 39 50]\nTest indices on run 3: [ 7  8  9 10 14 27 31 32 45 46 51]\nTest indices on run 4: [12 15 17 18 19 25 26 29 34 35 52]\nTest indices on run 5: [ 1  3 16 20 22 37 38 40 41 47]\n\n\n\n\n\n\nNow that we have gone over how to detect an ERP, we should pull all our code together into a single class that implements each stage of the fitting pipeline. We will use the builtin scikit-learn functions to do this.\n\nclass ERP_Decode():\n\n    def __init__(self, k=5, rand_seed=47, **kwargs):\n        self._k = k # number of folds for cross-validation\n        self._rand_seed = rand_seed # random seed for reproducibility\n        self._test_acc = None \n        self._train_acc = None\n        self.erp = None # ERP is the average of the ERP labeled epochs\n        self._logreg = LogisticRegression(**kwargs) # **kwargs allows us to pass in arguments to the LogisticRegression class\n        self._stratkfold = StratifiedKFold(n_splits=self._k, shuffle=True, random_state=self._rand_seed)\n    \n    def set_rand_seed(self, rand_seed):\n        # set random seed to generate new random folds\n        self._rand_seed = rand_seed\n        self._stratkfold = StratifiedKFold(n_splits=self._k, shuffle=True, random_state=self._rand_seed)\n    \n    def _erp_calc(self, epochs):\n        # calculate the ERP\n        return np.mean(epochs, axis=0)\n    \n    def _erp_align(self, epochs, erp):\n        # align the ERP to the origin\n        return np.dot(epochs, erp.T/np.linalg.norm(erp))\n    \n    def fit(self, epochs, labels):\n        # fit the model\n\n        # get the training and testing indices for first fold\n        train_idxs, test_idxs = next(self._stratkfold.split(epochs, labels))\n        \n        # get the training and testing data for first fold\n        y_train = labels[train_idxs]\n        X_train = epochs[train_idxs]\n        train_cue_idxs = np.intersect1d(np.where(y_train)[0], train_idxs)\n        self.erp = self._erp_calc(X_train[train_cue_idxs])\n        X_train = self._erp_align(X_train, self.erp)[:,np.newaxis]\n        X_test = self._erp_align(epochs[test_idxs], self.erp)[:,np.newaxis]\n        y_test = labels[test_idxs]\n        \n        # fit the model\n        self._logreg.fit(X_train, y_train)\n        self._train_acc = self._logreg.score(X_train, y_train)*100\n        self._test_acc = self._logreg.score(X_test, y_test)*100\n        \n        # return the training and testing accuracies\n        return self._train_acc, self._test_acc\n    \n    def decision_boundary(self):\n        # get the decision boundary\n        return -self._logreg.intercept_[0]/self._logreg.coef_[0,0]\n\n    def model_coef(self):\n        # get the model coefficients\n        return self._logreg.coef_[0,0]\n    \n    def model_intercept(self):\n        # get the model intercept\n        return self._logreg.intercept_[0]\n    \n    def predict(self, epochs):\n        # predict the labels of new data using the trained model\n        return self._logreg.predict(self.erp_align_(epochs, self.erp))\n\n\ntrain_ac = np.zeros(1000)\ntest_ac = np.zeros(1000)\nfor rep in range(1000):\n    erp_dec = ERP_Decode(rand_seed=rep)\n    train_ac[rep], test_ac[rep] = erp_dec.fit(all_epochs.T, class_act)\n\nprint(erp_dec.decision_boundary())\nfig, ax = plt.subplots(figsize=(2, 5))\nax.violinplot([train_ac, test_ac])\nax.set_xticks([1, 2])\nax.set_xticklabels(['Train', 'Test'])\nax.set_ylabel('Accuracy')\nax.set_ylim([20, 105])\nax.grid(axis='y')\nax.set_title('ERP Decoding')\n\n35.90827949750561\n\n\nText(0.5, 1.0, 'ERP Decoding')\n\n\n\n\n\n\n\n\n\nThis replicates the results we got earlier with the splitting functions we wrote ourselves. Performance on the training data typically exceeds that on the test data. Let’s save the parameters from the last model fitting. We will use them next week when we evaluate our hand-coded functions for fitting logistic regression models.\n\nw = erp_dec.model_coef()\nb = erp_dec.model_intercept()\nerp = erp_dec._erp_calc(all_epochs[:,class_act].T)\nX = erp_dec._erp_align(all_epochs.T, erp)\ny = class_act\n# had to convert X and y formats to save in JSON file\nlogreg_data = {'w': w, 'b': b, 'X': list(X), 'y': list(y.astype(float))}\nwith open(os.path.join('data', 'logregdata.json'), 'w') as f:\n    json.dump(logreg_data, f)\n\nReferences\nG. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics, Chapter 4, https://doi.org/10.1007/978-3-031-38747-0_4"
  },
  {
    "objectID": "Week3.html#what-we-will-cover",
    "href": "Week3.html#what-we-will-cover",
    "title": "Week3",
    "section": "",
    "text": "The overarching goal of this week is to introduce simple logistic regression models, where a single feature derived from neural data (ERP strength) is used to predict a single predictor (stimulus present or absent). To start we will discuss different ways to quantify the strength of an ERP. We will also weigh the advantages and disadvantages of different approaches to extract time periods from the recording that lack stimuli. Then logistic regression will be introduced and we will cover how to apply it using the scikit-learn python package. Lastly, we will discuss some of general issues that arise in machine learning algorithms like logistic regression.\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport sys\nsys.path.append('..')\nfrom source.loaders import EEG, remove_baseline_drift, remove_emg, remove_ac, detect_blinks"
  },
  {
    "objectID": "Week3.html#quantification-of-erp-strength",
    "href": "Week3.html#quantification-of-erp-strength",
    "title": "Week3",
    "section": "",
    "text": "ERPs have complex waveforms that can last a couple hundred milliseconds. With a sample rate of 500 Hz, that means we will have around 100 samples of neural data to work with when trying to determine whether a stimulus was present or not. You could imagine being given a set of waveforms, some from trials with a stimulus and others without, and trying to determine which was which. One strategy is to calculate a feature of the waveform related to the presence of the ERP, and if that feature exceeds a certain value then the ERP is deemed present on that trial.\n\n\nAs a starting point, we will use the data set we worked with last week. An auditory cue was given about every 8 seconds, and there was a robust ERP elicited over occipital and parietal EEG sites. We will select those epochs when we know a cue was presented, and an equal number of epochs where we know a cue was not delivered. To start, we need to load the data.\nLast week we created an EEG class that interfaced with the EEG data, along with preprocessing functions to remove noise or detect eye blinks. We will reuse them here to quickly get us back to where we left off last week.\n\n# create file paths\nsubj = 'sub-AB58'\nevt_dir = ['data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_events.tsv'.format(subj)]\nevt_path = os.path.join(*evt_dir)\n\ndata_dir = ['data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_eeg.set'.format(subj)]\neeg_path = os.path.join(*data_dir)\n\nchan_dir = ['data', 'eeg', 'ds003690', subj, 'eeg', '{}_task-passive_run-1_channels.tsv'.format(subj)]\nchan_path = os.path.join(*chan_dir)\n\n# load data\neeg = EEG(eeg_path, chan_path)\nevents = pd.read_csv(evt_path, sep='\\t')\n\n# preprocess data\nremove_baseline_drift(eeg)\nremove_emg(eeg)\nremove_ac(eeg, ac_freq=50)\nblinks = detect_blinks(eeg)[0] # indexing the call to the function allows us to select the output we want\ncues = events['onset'][events['trial_type'] == 'cue'].values\n\n\n\n\nTo compare stimulus and non-stimulus periods, we need to select times when the stimulus did not occur. At first glance, this seems like a trivial task. Given the list of stimulus times, we can just create a new list by hand that does not include any of those times. However, we might unintentionally incorporate biases in our list creation. In addition, we want to avoid periods with blinks, because we exclude those periods from our ERP construction. This means we have to avoid two sets of conditions. But worst of all, we would have to create a new list for each recording session, which is tedious. We program to avoid tedium, so lets explore ways to automate this step.\nThe approach we devise to create no cue times should satisfy a few conditions:\n\nThe cue should not be present\nNo eye blinks should be present\nNo systematic temporal relationship with cue delivery\nNo-cue epochs should have similar properties to cue epochs, such as:\n\nOccur during the same period of the recording\nSimilar relative timing\n\n\nBut first, we will get the cue times that did not overlap with eye blinks.\n\n# Function that returns cues without blinks\nexcl_blink = 0.6 # seconds, exclusion period surrounding a blink\nrem_blinks = lambda c,b: np.array([ci for ci in c if not any(np.abs(ci-b) &lt; excl_blink)])\n\n# keep those cues that are not within the blink exclusion period\ncues_nob = rem_blinks(cues, blinks)\n\nThe same function should also be used with the non-stimulus times we will generate. We want the no-cue times we create to be as similar as possible to the times when cues were presented. If we are excluding cues that were near eye blinks, then we have to do the same for the no-cue periods. If we do not, then the cue and no-cue times will differ in whether an eye blink was present, which our algorithm could potentially pick up on to distinguish between them.\nTo ensure that our no-cue periods do not come close to the cue periods, we could set the no-cue times to be at a fixed time before each cue. If that time is greater than the post-window, and far enough from the previous cue time to ensure that it does not overlap with its ERP, this might work. This strategy would be questionable if the ITIs were at a fixed interval, because then there would be a systematic relationship between the cue times and no-cue times. But because they are random, that means the previous cue time will be random, plus or minus a second, with respect to the no-cue time.\n\n# fixed time before cue method\n# fixed time should be less than the shortest iti minus the window.\nfixed_time = 4 # half of the mean ITI\nnocues_fixed = cues-fixed_time\nnocues_fixed_nob = rem_blinks(nocues_fixed, blinks)\n\ndef plot_nocues(ax, nocues_orig, nocues_rem, cues):\n    \"\"\"Plot no-cue times, with cues and cut no-cue times overlaid\n    \n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes\n        Axes to plot on\n    nocues_orig : numpy.ndarray\n        Original no-cue times\n    nocues_rem : numpy.ndarray\n        No-cue times with blinks removed\n    cues : numpy.ndarray\n        Cue times\n    \"\"\"\n    ax.vlines(cues, colors='k', ymin=-1, ymax=2, alpha=0.2) # cue times\n    ax.eventplot(nocues_rem, colors='b') # no-cue times\n    nocues_cut = np.setdiff1d(nocues_orig, nocues_rem) # cut no-cue times\n    for nocue_cut in nocues_cut:\n        ax.text(nocue_cut, 1, 'X', fontsize=20, color='r', ha='center', va='center')\n    ax.set_ylim(0, 2)\n    ax.set_yticks([])\n    ax.set_xlabel('Time (s)')\n\n# plot permuted no-cue times\nfig, ax = plt.subplots(figsize=(10, 2))\nplot_nocues(ax, nocues_fixed, nocues_fixed_nob, cues)\nax.set_title('Fixed offset no-cue times')\n\nText(0.5, 1.0, 'Fixed offset no-cue times')\n\n\n\n\n\n\n\n\n\nWe created a custom plotting function to see how our event times for the cue and no-cue periods related to each other. In it vertical lines are the times when cues were presented, blue hatch marks are when valid random no-cue times occur, and red ’X’s are the no-cue times that were eliminated due to overlapping with a cue or blink. Using times at a fixed interval before cue delivery looks good. Our no-cue periods are consistently far away from the cues and have a similar distribution as the cues across the session. We will use these times to get our no-cue epochs.\n\n\n\n\n\n\nSome flawed approaches to selecting non-cued periods\n\n\n\nThere are a variety of strategies for choosing the times when no stimuli are present that should meet the criteria listed above. Here we will explore a couple that seem like good ideas, but upon closer examination exhibit flaws.\nSince these approaches will generate no-cue times that do not have a prespecified fixed offset from the cue times, we need to exclude those times near a cue. For this, we will create a similar function to the one that removed times close to blinks (rem_blinks), but use a tighter exclusion period because of the tight temporal relationship between cue delivery and the ERP.\n\n# create version outside cue period\nexcl_cue = 0.1 # seconds, exclusion period around cue\nrem_cues = lambda nc,c: np.array([nci for nci in nc if not any(np.abs(nci-c) &lt; excl_cue)])\n\n\n\nFor this approach, we will create a list of times using a random number generator. The random times we generate cannot have their window times extend past the beginning or end of the recording, nor overlap with blink period or cue periods. We will use a fixed random seed when generating these times to ensure consistency across runs.\n\n# set random seed, ensures consistent results across runs\nnp.random.seed(47)\n\n# random times between 0 and recording duration\nrec_dur = eeg.data.shape[1]/eeg.srate # (number of samples / sample rate) gives time in seconds\ncue_num = len(cues) # number of cues\nnocues_rand = np.random.uniform(1, rec_dur-1, cue_num)\nnocues_rand_nbc = rem_cues(rem_blinks(nocues_rand, blinks), cues)\n\nWe use the random.uniform random number generator in numpy to get as many times as we have cues (cue_num), and restricted to times from the first till the last second of the recording session. Those times that overlap with cues or blinks are then removed, leaving a list of no-cue times that we can use. A good way to establish that the criteria for our no cue periods are followed is to plot the no-cue times overlaid on top of the cue times.\n\nfig, ax = plt.subplots(figsize=(10, 2))\nplot_nocues(ax, nocues_rand, nocues_rand_nbc, cues)\nax.set_title('Random no-cue times')\n\nText(0.5, 1.0, 'Random no-cue times')\n\n\n\n\n\n\n\n\n\nIf we compare the cue and no-cue times, it is apparent that the fourth criterion for good no-cue times is violated. One problem is that we have several no-cue times that occur outside the time period when the cue times were delivered (notice the two at the beginning and four at the end.) The other problem is that no-cue times are distributed unevenly, unlike the cues which are evenly spread across the recording session.\n\n\n\nIf we want to have the no-cue times have a similar distribution across the session as the cue times, we could generate random times but ensure that they have the same spacing between cues. One way to do this is to get a list of times between cue trials (inter-trial intervals, ITIs), shuffle those intervals, and then create a new list of times by adding them together one at a time. This strategy works because the times between cues are randomly generated, varying between 7 and 9 seconds.\n\n# set random seed, ensures consistent results across runs\nnp.random.seed(45)\n\n# Calculate ITI times\n# Note, we repeat the first ITI time so that when we sum the ITIs\n# we get the correct number of trials\niti_times = np.insert(np.diff(cues), 0, cues[1]-cues[0])\n\n# randomly shuffle the iti_times\niti_times = np.random.permutation(iti_times)\n\n# get no-cue times by summing the iti_times.\n# cumsum adds each element of the array to the previous element, \n# returning an array of the same size\nnocues_perm = np.cumsum(iti_times)\n\n# remove no-cue times that overlapped with cue times and blinks\nnocues_perm_nbc = rem_cues(rem_blinks(nocues_perm, blinks), cues)\n\n# plot permuted no-cue times\nfig, ax = plt.subplots(figsize=(10, 2))\nplot_nocues(ax, nocues_perm, nocues_perm_nbc, cues)\nax.set_title('Permuted no-cue times')\n\nText(0.5, 1.0, 'Permuted no-cue times')\n\n\n\n\n\n\n\n\n\nThis looks much better than the purely random strategy. The no-cue times are during the same period when cues were presented, and have similar relative timing. However, they appear to sometimes overlap quite closely for runs of several trials with the cue times.\n\n\n\n\n\n\nLast week we covered extracting EEG epochs, so we will only lightly reprise this here. This time we need one set of epochs centered on the cue times, and another on the no-cue times. We can use the get_data method in our eeg object to extract the EEG data around these times. Each will be a 3-D numpy array, where time is on axis 0, channel is axis 1, and trial on axis 2. Instead of using all channels, we will focus only on a channel were the cue ERP was strong, O1.\n\n# time window edges\npre_win = 0.1 # time to sample before the cue\npost_win = 0.5 # time to sample after the end\nepoch_dur = pre_win+post_win # duration of an epoch\nsel_chan = 'O1'\n\n# get cue epochs\ncue_starts = cues_nob - pre_win\ncue_epochs, t_erp, _ = eeg.get_data(chans=sel_chan, start_t=cue_starts, dur_t=epoch_dur, scale='relative')\n\n# get no-cue epochs\nnocue_starts = nocues_fixed_nob - pre_win\nnocue_epochs, _, _ = eeg.get_data(chans=sel_chan, start_t=nocue_starts, dur_t=epoch_dur, scale='relative')\n\n# correct the relative time stamps to account for the pre_win period\nt_erp -= pre_win\n\nSince we only loaded one channel, our channel axis (axis 1) only has a length of 1. This ‘singleton’ dimension is useless for our purposes and so to make things easier we will remove it using the numpy squeeze function.\n\nprint('The original shape of the cue_starts array is: {}'.format(cue_epochs.shape))\ncue_epochs = np.squeeze(cue_epochs)\nprint('The squeezed shape of the cue_starts array is: {}'.format(cue_epochs.shape))\n\nnocue_epochs = np.squeeze(nocue_epochs)\n\nThe original shape of the cue_starts array is: (300, 1, 25)\nThe squeezed shape of the cue_starts array is: (300, 25)\n\n\nLet’s plot the ERPs for the cue and no-cue epochs to make sure the ERP is only present for the cue.\n\n# get the average ERP for each condition\ncue_erp = np.mean(cue_epochs, axis=1) \nnocue_erp = np.mean(nocue_epochs, axis=1)\n\n# plot the ERPs\nplt.figure(figsize=(10, 5))\nplt.plot(t_erp, cue_erp, label='Cue')\nplt.plot(t_erp, nocue_erp, label='No-cue')\nplt.xlabel('Time (s)')\nplt.ylabel('Voltage (uV)')\nplt.title('Average ERP for channel ' + sel_chan)\nplt.legend()\n\n\n\n\n\n\n\n\nSo far so good. We can see that the ERP is strong for epochs centered on the cue, and non-existent for times that avoid cue presentation.\n\n\n\nThe next step is to distill the complex ERP waveform into a single number that reflects its strength. There are numerous measures you can use for this, and we will explore a few. This step is known as feature engineering, wherein we try to distill multidimensional samples to a few values that capture the aspects of the sample that are relevant to our task. In this case, each sample is a collection of 300 voltage measurements, and the aspect we want to measure is the strength of the ERP. A good feature will take on values that distinguish between the cue and no cue epochs.\n\n\nSince we know the shape of the ERP, we know on average where it tends to peak in value. The voltage at that time should indicate whether an ERP is present. Looking at the average ERP traces immediately above, you can see that on trials with a cue the voltage will tend to be near 10 uV at the peak time, while on no-cue trials it will tend to be near 2 uV. To get the peak voltage on each trial, we first find the time when the ERP peak occurs, then sample the voltage at that time for each epoch.\n\n# get peak idx of cue erp\ncue_peak_idx = np.argmax(cue_erp)\n\n# return the voltages at the ERP peak time\ncue_erp_peaks = cue_epochs[cue_peak_idx, :]\nnocue_erp_peaks = nocue_epochs[cue_peak_idx, :]\n\nTo examine how well the peak voltage distinguishes between the Cue and No-cue conditions, we can plot their distribution. The less their respective distributions overlap, the better the feature is for distinguishing between the types of trials.\n\n# box plot of peak values for cue and nocue conditions with event plot of points on top\ndef plot_dist_erps(cue_vals, nocue_vals, ax=None):\n    \"\"\"Plot distribution of cue and no-cue ERP peak values\n\n    Parameters\n    ----------\n    cue_vals : numpy.ndarray\n        Array of cue ERP peak values\n    nocue_vals : numpy.ndarray\n        Array of no-cue ERP peak values\n    ax : matplotlib.axes.Axes\n        Axes to plot on\n    \"\"\"\n    \n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # violin plot of values, with cue_vals blue and nocue_vals orange\n    cue_vp = ax.violinplot(cue_vals, positions=[1], showmedians=True)\n    for pc in cue_vp['bodies']:\n        pc.set_facecolor('tab:blue')\n\n    nocue_vp = ax.violinplot(nocue_vals, positions=[2], showmedians=True)\n    for pc in nocue_vp['bodies']:\n        pc.set_facecolor('tab:orange')\n    \n    # plot the cue_vals and nocue_vals as points on top of the violin plot\n    ax.plot(np.ones(len(cue_vals)), cue_vals, '.', color='tab:blue')\n    ax.plot(2 * np.ones(len(nocue_vals)), nocue_vals, '.', color='tab:orange')\n    # set the xticks to be at 1 and 2, with labels Cue and No Cue\n    ax.set_xticks([1, 2])\n    ax.set_xticklabels(['Cue', 'No Cue'])\n\nfig, ax = plt.subplots(figsize=(2, 4))\nplot_dist_erps(cue_erp_peaks, nocue_erp_peaks, ax=ax)\nplt.ylabel('Peak Amplitude (uV)')\nplt.title('Peak Amplitude of Cue and No Cue Conditions')\n\nText(0.5, 1.0, 'Peak Amplitude of Cue and No Cue Conditions')\n\n\n\n\n\n\n\n\n\nThe violin plot above shows a smoothed distribution of the data points as a shaded region. The solid horizontal lines from top to bottom are the maximum, median, and minimum values. The distributions do overlap, with the median value for the No-cue condition falling within the distribution of values for the Cue. This overlap is likely because there is substantial spontaneous activity in the EEG signal that is added to our ERP peak estimate on each trial. When we got the average ERP, this spontaneous activity that was not time-locked to cue delivery canceled out because it was equally likely to be positive or negative. We can visualize this spontaneous component by plotting the EEG signal for each epoch with the ERP subtracted out. That looks like:\n\n# subtract the mean ERP waveform from each epoch\n# epochs have to be transposed, x.T, because of broadcasting rules in numpy\ncue_epochs_spon = cue_epochs.T-cue_erp\nnocue_epochs_spon = nocue_epochs.T-nocue_erp\n\n# plot EEG without ERP, and distribution of values where ERP peak would be\nfig, ax = plt.subplot_mosaic([['ul', 'ul', 'r'],['ll', 'll', 'r']], sharey=True)\nax['ul'].plot(t_erp, cue_epochs_spon.T, color='tab:blue', alpha=0.2)\nax['ul'].plot(t_erp, cue_erp, color='tab:blue')\nax['ul'].grid()\nax['ll'].plot(t_erp, nocue_epochs_spon.T, color='tab:orange', alpha=0.2)\nax['ll'].plot(t_erp, nocue_erp, color='tab:orange')\nax['ll'].grid()\nax['ll'].set(xlabel='Time (s)')\nplot_dist_erps(cue_epochs_spon[:,cue_peak_idx], nocue_epochs_spon[:,cue_peak_idx], ax=ax['r'])\nax['r'].set_yticklabels(range(-30, 40, 10))\nfig.subplots_adjust(hspace = 0)\nax['ul'].set(title='Waveforms')\nax['r'].set(title='EEG voltage at ERP peak')\nfig.suptitle('Spontaneous EEG', fontsize=16)\nfig.supylabel('Voltage (uV)')\n\n/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8892/2930109674.py:16: UserWarning:\n\nset_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n\n\n\nText(0.02, 0.5, 'Voltage (uV)')\n\n\n\n\n\n\n\n\n\nAfter removing the ERP, a a great deal of variation still remains, with an amplitude on par with the ERP itself. In fact, the shape of the distribution of EEG voltages at the ERP peak time is the same as the one for the ERPs above. This is a simple consequence of the ERP peak values reflecting the sum of the mean ERP peak and the spontaneous EEG at that time. When we subtract the mean ERP peak, we are just shifting the distribution to zero. Is there any way we can account for the spontaneous EEG activity on a single trial?\n\n\n\nExamining the spontaneous EEG waveforms, we see that the signal fluctuates between positive and negative values on the order of tens of milliseconds. Perhaps if we average the EEG signal over the 100 ms when the strongest ERP component is present, from 80 to 180 ms, the spontaneous activity will cancel itself out. The ERP, on the other hand, maintains positive values during that period, so it should persist despite the averaging. The logic here is similar to averaging the EEG epochs across trials, because the spontaneous voltage at each time point across epochs will randomly vary between positive and negative values. In this case, we are just applying that principle across time within a trial.\nWhen we restrict analysis or calculation to a specific time window, that period is referred to as the region of interest (ROI). Here, our ROI will be from 80 to 180 ms post cue (or no-cue), and we will take the mean value of the EEG voltage during that time.\n\n# get indices between 80 and 180 ms\nroi_idxs = np.where((t_erp &gt;= 0.08) & (t_erp &lt;= 0.18))[0]\n\n# get the mean EEG voltage during the strongest ERP component across trials for each condition\ncue_erp_means = np.mean(cue_epochs[roi_idxs, :], axis=0)\nnocue_erp_means = np.mean(nocue_epochs[roi_idxs, :], axis=0)\n\n# plot mean voltages\nfig, ax = plt.subplots(figsize=(2, 4))\nplot_dist_erps(cue_erp_means, nocue_erp_means, ax=ax)\nplt.ylabel('Mean ERP voltage (uV)')\nplt.title('Mean amplitude of Cue and No Cue Conditions')\n\nText(0.5, 1.0, 'Mean amplitude of Cue and No Cue Conditions')\n\n\n\n\n\n\n\n\n\nWell that did not work! Averaging over the period where the strongest ERP component was present diminished median peak ERP voltage for the Cue condition, making it less separable from the No-cue condition. This happens because the magnitude of the ERP varies across time, and we are averaging together periods where it is weaker. Indeed, if we calculate the mean of the ERP during our ROI:\n\n# get mean of ERP during the ROI\nerp_mean = np.mean(cue_erp[roi_idxs])\nprint(\"Mean of ERP during ROI: {:.2f}\".format(erp_mean))\n\n# get ERP voltage during its peak\nerp_peak = cue_erp[cue_peak_idx]\nprint(\"ERP voltage during peak: {:.2f}\".format(erp_peak))\n\nMean of ERP during ROI: 6.63\nERP voltage during peak: 11.21\n\n\nWe have gone from an ERP peak voltage of 11.2 to 6.6 uV, a decrease by almost half! While this might tempt us to go back to the original approach of just sampling the voltage at the ERP peak, we would leave behind one advantage of the averaging approach: it reduced the variability in the distribution of ERP values. This is evident when we compare the standard deviations of the distributions between the ROI mean and peak estimation approaches.\n\n# variance of ERP ROI means\ncue_erp_means_var = np.std(cue_erp_means)\nnocue_erp_means_var = np.std(nocue_erp_means)\n\n# variance of ERP peaks\ncue_erp_peaks_var = np.std(cue_erp_peaks)\nnocue_erp_peaks_var = np.std(nocue_erp_peaks)\n\n# print comparisons\nprint('Cue ERP ROI {:.2f} vs peaks {:.2f} uV'.format(cue_erp_means_var, cue_erp_peaks_var))\nprint('No-cue ERP ROI {:.2f} vs peaks {:.2f} uV'.format(nocue_erp_means_var, nocue_erp_peaks_var))\n\nCue ERP ROI 4.82 vs peaks 6.04 uV\nNo-cue ERP ROI 6.75 vs peaks 8.80 uV\n\n\nTaking the mean over the ROI reduced the variability in our ERP measures. This means their distributions are tighter. Assuming the means of those distributions do not change, this would decrease their overlap. Consequently, our ability to discriminate between cue and no-cue trials should improve. However, the means did change, we lost almost half of the ERP strength compared with just taking the peak.\nIs there a way we can benefit from averaging over multiple samples to get a measure of ERP strength, while not diminishing the strength of the ERP itself?\n\n\n\nOne way to average over multiple samples while not diminishing ERP strength is to weight each time point by the value of the ERP. In effect, this would measure the alignment between the voltages recorded during an epoch and the mean ERP. Here we determine the strength of the ERP by multiplying each data point by its corresponding voltage in the ERP. If we are at a sample where the ERP is strongly positive, then we want that point to strongly contribute the mean, while a point where the ERP is weak should contribute less. If the ERP is negative at a certain time point, we want to negate that sample so it will constructively add to the mean. By doing this, if the EEG contains a signal that follows the waveform of the ERP it will return a strong mean value, while an EEG signal that does not follow the ERP time course will return a weak mean.\nA mathematical operation that can be used to do this is called the dot product. A dot product multiplies together each corresponding element in two vectors, x and y, and then adds those together, returning a scalar z.\n \\begin{align}\n    \\notag z &= x \\cdot y \\\\\n    \\notag  &= \\sum_{i=1}^{n}x_{i}y_{i}\n\\end{align}\n\nLet’s explore how to calculate a dot product and its behavior.\n\n# how to code a dot product in base python\ndef dot(x, y):\n    \"\"\"\n    Calculate the dot product of two vectors x and y\n\n    Parameters\n    ----------\n    x : array-like\n        First vector\n\n    y : array-like\n        Second vector\n\n    Returns\n    -------\n    out : float\n        Dot product of x and y\n    \"\"\"\n\n    out = 0 # initialize output to 0\n    for x_i, y_i in zip(x, y): # loop over elements of x and y\n        out += x_i * y_i # add the product of the elements to out\n\n    return out\n\n\nv1 = [1, 2, 3]\nv2 = [1, 2, 3]\nprint('Dot product of two aligned vectors, {} and {} = {}'.format(v1,v2,dot(v1, v2)))\n\nv1 = [1, 2, 3]\nv2 = [2, 2, 2]\nprint('Dot product of two partly aligned vectors, {} and {} = {}'.format(v1,v2,dot(v1, v2)))\n\nv1 = [1, 2, 3]\nv2 = [-1, 2, -1]\nprint('Dot product of two misaligned vectors, {} and {} = {}'.format(v1,v2,dot(v1, v2)))\n\nDot product of two aligned vectors, [1, 2, 3] and [1, 2, 3] = 14\nDot product of two partly aligned vectors, [1, 2, 3] and [2, 2, 2] = 12\nDot product of two misaligned vectors, [1, 2, 3] and [-1, 2, -1] = 0\n\n\nInstead of rolling our own, we will use the numpy array dot function, dot, because it handles dot products on multidimensional arrays and runs faster.\nTo calculate the alignment between an EEG epoch and the mean ERP, we use the ERP waveform as our weighting vector. We will scale its values by its euclidean norm, so that the values returned by our dot product are in a comparable range to those from the peak and mean calculations we did previously. The euclidean norm is:\n ||x|| = \\sqrt{\\sum_{i=1}^{n}{x_{i}^{2}}} \nThis function is implemented in the linear algebra portion of the numpy package as linalg.norm.\nThus, the alignment between an EEG epoch and ERP is:\n Alignment = EEG \\cdot \\frac{ERP}{||ERP||} \nLet’s give it a try.\n\n# our function that measures the alignment between the EEG epoch and ERP\ndef erp_align(epochs, erp):\n    \"\"\"\n    Calculate the dot product of each epoch with the ERP\n\n    Parameters\n    ----------\n    epochs : numpy.ndarray\n        Array of EEG epochs, with epochs in the last dimension\n    erp : numpy.ndarray\n        Array of ERP waveform\n\n    Returns\n    -------\n    out : numpy.ndarray\n        Array of dot products, one for each epoch\n    \"\"\"\n\n    return np.dot(epochs.T, erp/np.linalg.norm(erp))\n\n\ncue_erp_dots = erp_align(cue_epochs, cue_erp)\nnocue_erp_dots = erp_align(nocue_epochs, cue_erp)\n\n# box plot of peak values for cue and nocue conditions with event plot of points on top\nfig, ax = plt.subplots(figsize=(2, 4))\nplot_dist_erps(cue_erp_dots, nocue_erp_dots, ax=ax)\nplt.ylabel('Dot product (uV)')\nplt.title('Dot product of Cue and No Cue Conditions')\n\nText(0.5, 1.0, 'Dot product of Cue and No Cue Conditions')\n\n\n\n\n\n\n\n\n\nThese two distributions seem much more separable than the previous ones. The median values for the Cue and No-cue conditions do not overlap with the distribution of the opposite condition, and their overall distributions overlap less as well. Replotting all three together highlights the improvement we have made in tailoring the feature we extract from the EEG for detecting the Cue-evoked ERP.\n\nfig, ax = plt.subplots(1,3,figsize=(6, 4))\nplot_dist_erps(cue_erp_peaks, nocue_erp_peaks, ax=ax[0])\nax[0].set_title('Peak amplitude')\nplot_dist_erps(cue_erp_means, nocue_erp_means, ax=ax[1])\nax[1].set_title('Mean amplitude')\nplot_dist_erps(cue_erp_dots, nocue_erp_dots, ax=ax[2])\nax[2].set_title('Dot product')\nfig.supylabel('uV')\nfig.tight_layout()"
  },
  {
    "objectID": "Week3.html#a-simple-decision-model",
    "href": "Week3.html#a-simple-decision-model",
    "title": "Week3",
    "section": "",
    "text": "Our goal is to determine whether a trial had a cue stimulus or not. The dot product feature seems to distinguish between trial types, such that its value on a given trial indicates whether the cue was present. This is a decision process, where we assign a trial to either the Cue or No-cue category depending on the value of the dot product. A simple way to make this decision is to set a threshold value for the dot product, above which the trial is classified as a Cue trial. If the value is below the threshold, we label it a No-cue trial. This can be represented as a piecewise mathematical formula:\n f(x,t) =\n\\begin{cases}\n    0 & x\\leq t \\\\\n    1 & x\\gt t \\\\\n\\end{cases}\n\nHere x is the dot product of the epoch, and t is the threshold. When the function returns a 1 we refer to it as a Cue trial, while a 0 is labeled a No-cue trial. We have to choose a threshold, and to start we will use the minimum dot product of our Cue trials.\n\n# set threshold\nthresh = np.min(cue_erp_dots)\n\n# create vectors for ERP values and class labels\ncue_num = cue_erp_dots.size # number of cue trials\nnocue_num = nocue_erp_dots.size # number of nocue trials\nclass_act = np.repeat([True, False], [cue_num, nocue_num]) # actual class labels, 1 for cue, 0 for nocue\nvals = np.append(cue_erp_dots, nocue_erp_dots) # ERP values\nclass_min_pred = vals &gt; thresh # predicted class labels\n\n# print the true vs. predicted class labels\nprint('Actual class labels: ', class_act)\nprint('Predicted class labels: ', class_min_pred)\n\nActual class labels:  [ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nPredicted class labels:  [ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True False\n  True False False  True False  True False False False False False False\n  True  True False False False False False False  True False  True  True\n False False False False False False]\n\n\nIf you look closely, you can see that some of our predicted classes do not agree with the true ones. There are different types of errors a classifier can make, and numerous ways to measure their performance. We will start with two simple measures, error rate and accuracy. Error rate is the proportion of trials that were misclassified. This can be defined mathematically as:\n Error Rate = \\frac{1}{n}\\sum_{i=1}^{n}{I(y_{i}\\ne \\hat{y_{i}})} \nHere y is the actual class labels and \\hat{y} are the predicted class labels. The function I(x,y) returns a 1 when x \\ne y, and 0 when x=y. Summing across all trials and dividing by the total number of trials, n, gives the proportion of trials whose predicted class disagreed with its actual class.\nAnother helpful measure is accuracy, which is simply the proportion of trials where the predicted and actual class labels agreed. Expressed as:\n Accuracy = 1 - Error Rate \nHow does our simple classifier perform based on these metrics?\n\n# as an example, here we define error rate method with base python\ndef error_rate_base(y_act, y_pred):\n    er = 0\n    for i in range(len(y_act)):\n        if y_act[i] != y_pred[i]:\n            er += 1\n    return 100 * er / len(y_act)\n\n# define error rate method with numpy\ndef error_rate(y_act, y_pred):\n    return np.mean(y_act != y_pred) * 100\n\n# define accuracy method with numpy\ndef accuracy(y_act, y_pred):\n    return 100-error_rate(y_act, y_pred)\n\n\n# evaluate error rate for ERP peak\n# 1 for cue trials, 0 for non-cue trials\ner_min = error_rate(class_act, class_min_pred)\nac_min = accuracy(class_act, class_min_pred)\n\nprint('Error rate for ERP peaks: {:.1f}%'.format(er_min))\nprint('Accuracy for ERP peaks: {:.1f}%'.format(ac_min))\n\nError rate for ERP peaks: 14.8%\nAccuracy for ERP peaks: 85.2%\n\n\nNot too bad. Just setting our threshold to the minimum value from the Cue trials, we can correctly classify ~85% of trials. But, how do we know we have chosen the best threshold? Moreover, can we ascertain how confident our classifier is in the choice it puts out? To address these issues, we need to formalize the classification process. One approach is to use logistic regression, which takes a measurement (e.g. ERP dot product) and returns the probability that it was generated by class (e.g. cue stimulus). For this week, we will pose the problem and learn how to use the logistic regression functions in the scikit-learn package."
  },
  {
    "objectID": "Week3.html#binary-classification",
    "href": "Week3.html#binary-classification",
    "title": "Week3",
    "section": "",
    "text": "A binary classifier takes a set of measurements, x, as inputs and returns the probability that they were generated by a specific class, \\hat{y}. (This is known as the discriminative view of classification. We will take on the generative view later in the semester when we tackle Naïve Bayes classifiers.) To get from x to \\hat{y}, we need a function that describes the probability of the class occurring over a range of ERP measurement values.\n\n\nProbabiltilies describe how likely events are to occur. They range from 0, for events that never happen, to 1, for events that are guaranteed to happen. When quantifying probabilities we do this for a class of events, with the total probability across all events adding up to 1 (which means that at any time one of them has to occur). For instance, in the case of flipping a coin, there is a 0.5 (1/2 or 50%) chance that the coin will come up Heads, and 0.5 that it will be Tails. These are the only possibilities (this is a Platonic coin, so it has no thickness and thus cannot land on its side). A coin flip is a good example of an unconditional probability, which is the same regardless of the circumstances. For this, we would write:\n\\begin{align}\np(H)&=0.5 \\\\\np(T)&=0.5 \\\\\n\\end{align}\n\nwhich says that the probability of the coin coming up heads, p(H), is 0.5, and the probability of coming up tails, p(T), is 0.5.\nBut probabilities can also depend on the situation, such as the probability that you will buy lunch at a certain time. It is more likely that you will purchase lunch at 11:30 AM than at 10:00 AM. This is a conditional probability. Conditional probabilities are expressed as P(Lunch|Time), which translates as the probability of going for Lunch, (Lunch), is conditional, |, on the time, Time. For a conditional probability we need to know the time to give the probability that we are going to lunch.\nIn the case of our ERP decoder, you can say that the probability of a trial having a cue is conditional on the strength of the ERP, p(Cue|ERP). For this, we need an equation that describes how the probability of being a Cue trial varies as a function of ERP strength.\n\n\n\nOne equation that is a useful way to express a conditional probability is the logistic function (also known as the sigmoid function). It has the form:\n \\sigma(x) = \\frac{1}{1+e^{-x}} \nLet’s code it up and visualize it:\n\n# create a logistic function\ndef logistic(x):\n    return 1 / (1 + np.exp(-x)) \n\n# plot the logistic function\nx = np.linspace(-10, 10, 100)\nplt.plot(x, logistic(x))\nplt.title('Logistic Function')\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThis function is bounded between 0 and 1, just like probabilities. However, by itself it is not useful. It also has a probability of 0.5 when x=0, which is not good for us because our decision point, called the location parameter, for a cue trial usually had our ERP measure at a positive value.\nTo shift the location, we can modify x by subtracting the location value from it.\n \\sigma(x) = \\frac{1}{1+e^{-(x-loc)}}\nHow does this look?\n\n# create logistic functio with adjustable location\ndef logistic(x,loc=0):\n    return 1/(1+np.exp(-(x-loc)))\n\n# plot logistic function with different locations\nloc_list = np.arange(-5,6,2.5)\nx = np.linspace(-10,10,100)\n\nfor ind, loc in enumerate(loc_list):\n    plt.plot(x,logistic(x,loc),label='$loc={}$'.format(loc), color=[ind/len(loc_list), 0, 0])\n\nplt.yticks(np.arange(0,1.1,0.25))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.title('Logistic function with different locations')\nplt.grid()\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n\n\n\n\n\n\n\n\nThis looks promising, we can shift the point at which the logistic function crosses 0.5. However, the range of x values over which the logistic function varies, its scale, is narrow compared to the range of values we get for our dot product measures of the ERP. This may not seem important now, since a simple classification threshold at 0.5 probability does not care about the spread in our logistic function. But, it does matter if we want our function to characterize p(Cue|ERP), which can indicate how confident the model is in its prediction of whether a cue was present on a trial. Adjusting the scale is straightforward with the addition of a new parameter.\n \\sigma(x)=\\frac{1}{1+e^{-\\frac{x-loc}{scale}}} \nBy dividing x-loc by scale, we can stretch or contract the logistic function with respect to the x-axis. As you increase scale, the values of x have to get larger to push the output closer to 0 or 1. If you decrease scale, when only a small change in x is needed to have the logistic function return 0 or 1. We can see this below:\n\n# logistic function with location and scale parameters\ndef logistic(x, loc=0, scale=1):\n    return 1 / (1 + np.exp(-(x - loc) / scale))\n\n# plot the logistic function\nx = np.linspace(-10, 10, 100)\nloc = 0\nscale_list = np.power(2.0, range(-2, 3, 1))\n\nfor ind, scale in enumerate(scale_list):\n    plt.plot(x, logistic(x, loc, scale), label='$scale$ = {}'.format(scale), color=[0, 0, ind / len(scale_list)])\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.grid()\nplt.title('Logistic function with different scales')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n\n\n\n\n\n\n\n\nGreat, we can change the spread of the logistic function now. With these two degrees of freedom, location and scale, we are able to create a function that captures the probability of a certain class (in our case, a cue trial) occurring. An important caveat to this function is that it is monotonic, meaning that the probability of being that class only increases (or decreases) as one increases the value of x. This works for most cases, but not if our class occurs for only a restricted range of x values (e.g. between 2 and 5). In later lectures we will cover other types of classifiers that circumvent this limitation."
  },
  {
    "objectID": "Week3.html#fitting-the-logistic-function",
    "href": "Week3.html#fitting-the-logistic-function",
    "title": "Week3",
    "section": "",
    "text": "Using the logistic function requires us to choose values for the location and scale parameters, which we could try doing by hand. That is not recommended. Instead, we will use the LogisticRegression class in the scikit-learn package. This class defines an object that can take our measurements and true classes, fit a logistic function to that data, and then deliver class predictions. To start using this, we will cover how scikit-learn implements its fitting functions in general, and then the specifics of the LogisticRegression class.\n\n\nScikit-learn is a python package started in 2007 and has grown to include a wide variety of machine learning algorithms. Most of these are implemented as estimators, which are classes that allow one to fit a model or function to some data and then make predictions from that model. The project has adopted a uniform standard for the creation of estimators, making it easier to incorporate new ones into the project or develop your own that will comprehensible to users already familiar with scikit-learn. For details you can check out the online documentation for developers.\nEach estimator has an __init__ method that creates the estimator object. When calling this method, you can pass settings parameters that determine how the estimator will fit to the data. Generally, these settings are supposed to be independent of the data being fit to. The estimator then has a fit function, which accepts a data matrix X and predicted classes y. Additional parameters can be set here that affect the fitting process in ways specific to the data. Once the fit function has been called, you can evaluate the performance of the fit using the score method, or predict new classes from new data using the predict method.\nWe will step through these using the LogisticRegression class to fit our predictor of of whether a cue was present on a trial given the ERP.\n\n\n\nTo use the LogisticRegression class, we need to create an instance of it as an object. Parameters specifying how to fit the function to your data, using set using the __init__ method, are also accessible with a set_params method after you created the estimator. When no parameters are given to initialize the object, it takes on default values. These can be inspected in the documentation or using the estimator’s get_params method.\n\n# create a vanila LogisticRegression object\nclf = LogisticRegression()\n\n# examine its default fitting parameters\nclf.get_params()\n\n{'C': 1.0,\n 'class_weight': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'l1_ratio': None,\n 'max_iter': 100,\n 'multi_class': 'deprecated',\n 'n_jobs': None,\n 'penalty': 'l2',\n 'random_state': None,\n 'solver': 'lbfgs',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}\n\n\nThe only parameter worth mentioning at this time is fit_intercept, which determines if we will include a location argument in our fitting. By default this is set to True, so we don’t have to worry about explicitly setting it.\n\n\n\nNow that we have our LogisticRegression object, we can call its fit method. It accepts arrays containing your independent, x, and dependent, y variables and optimizes the function to best fit that data. The first parameter is X, which is an array of measurements. It has 2 dimensions, with each row a different sample (e.g. trial), and each column a different feature (e.g. ERP peak voltage). Its shape is (n_samples, n_features), where ‘n’ stands for ‘number of …’. You can have multiple features per sample, which allows us to use more than one aspect of the measured brain activity on a trial to decode whether a stimulus was present. For now we will just use the one, our measure of ERP strength on a given trial.\nThe next parameter is y, the true class labels (e.g. Cue or NoCue) labels. Its shape is (n_samples,), where n_samples is the same as X. Notice that X is uppercase and y is lowercase. This is because a common convention is that a matrix (a 2-D array) is represented by an uppercase character, while vectors (1-D array) use lowercase characters.\n\n# format X\n# add dimension to X, because X needs to be a 2D array\nvals = vals[:, np.newaxis]\nprint('The shape of X is {}'.format(vals.shape))\n\n# class_act is already formatted correctly\nprint('The shape of y is {}'.format(class_act.shape))\n\n# fit the model\nclf.fit(vals, class_act)\n\nThe shape of X is (54, 1)\nThe shape of y is (54,)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\nThat’s it, we fit the model! A bit anticlimactic, since all we did was update the clf object with a fitted model. The next thing we need to do is evaluate the performance of the fitted model.\n\n\n\nThere are couple ways to judge the performance of our model. When we implemented our simple threshold decision model above, we used error rate and accuracy as our measures. The LogisticRegression class provides a method named score that calculates accuracy. It accepts X and y parameters similar to the fit method. Note: you do not have to provide the same X and y used for fit, but can use new data that the model was not trained on (in fact, this is a better practice as we will discuss later). It returns a number between 0 and 1, with 1 being perfect classification, and 0 being totally incorrect classification. This single number provides a good top line indication of whether the fit was successful. Poor accuracy will usually not be at 0, but at a level consistent with random guessing (typically 1 divided by the number of options). For binary decoding with equal numbers of each class, this would be 0.5 (1/2).\n\n# calculate the score for our model\nscore = clf.score(vals, class_act)\n\nprint('Accuracy for our model: {:.2f}%'.format(score*100))\n\nAccuracy for our model: 87.04%\n\n\nThe logistic regression decoder out performs our simple threshold classifier we set by eye, which was 85.2%. Not by much, but better nonetheless. Just as a sanity check, let’s see what happens if we scramble the actual class labels. In this case, the performance of the classifier should be random, since we have removed any correspondence between the ERP strength and whether the trial had a cue or not.\n\nclf_rand = LogisticRegression()\nclf_rand.fit(vals, np.random.permutation(class_act))\nscore_rand = clf_rand.score(vals, np.random.permutation(class_act))\n\nprint(\"Random prediction accuracy: %.2f%%\" % (score_rand * 100))\n\nRandom prediction accuracy: 53.70%\n\n\nIf we run this cell a few times we can see that the average accuracy is around 50%, which is what we would expect from random performance. However, random performance of a binary classifier does not need to be at 50%. If there are more of one of the classes over the other, then the decoder could perform better than 50%. Let’s remove a bunch of the No-cue trials and see if that can affect our chance performance.\n\n# Create new logistic regression classifier for unbalanced data set\nclf_ub = LogisticRegression()\n\n# remove some no-cue trials\nvals_ub = vals[:-20].copy() # remove last 20 trials, which are no-cue trials\nclass_act_ub = class_act[:-20].copy() # remove last 20 trials, which are no-cue trials\n\nclf_ub.fit(vals_ub, class_act_ub)\nclass_pred_ub = clf_ub.predict(vals_ub)\nscore_ub = clf_ub.score(vals_ub, class_act_ub)\nprint('Accuracy for unbalanced model: {:.2f}%'.format(score_ub*100))\n\n# randomly permute (shuffle) the class labels\nnp.random.seed(43)\nclass_rand_ub = np.random.permutation(class_act_ub)\nclf_ub.fit(vals_ub, class_rand_ub)\nscore_rand_ub = clf_ub.score(vals_ub, class_rand_ub)\nprint('Accuracy for unbalanced random model: {:.2f}%'.format(score_rand_ub*100))\n\nAccuracy for unbalanced model: 94.12%\nAccuracy for unbalanced random model: 73.53%\n\n\nNotice that the accuracy has gone up for our model, even for the random case! This is because the model optimized its performance simply by biasing its response to indicate Cue trials instead of No-cue trials. This works because the data set is unbalanced, with much fewer No-cue trials compared with Cue trials.\nIs there a way to account for this? The metrics section of scikit-learn provides a range of performance score functions that measure other aspects of classifier performance or control for issues in the data set. In this case, we are dealing with unbalanced data. To address that, we can use the function balanced_accuracy_score. It works by calculating the accuracy for each class separately, and then taking the average across classes. In this way, if one class is overexpressed and classified correctly more often due to bias, then its contribution to the accuracy score will be downgraded, while the less well represented class will have its score enhanced. Specifically, for the binary classification we are doing here, the is calculated using the following equation:\n Balanced Score = \\frac{1}{2} \\left( \\frac{TP}{TP+FN}+\\frac{TN}{TN+FP}  \\right) \nHere we will introduce a few new terms that are going to come up in discussing classifiers. Each trial has an actual condition or label associated with it, and a label predicted by the model. The terminology here comes from the signal detection literature, where one is trying to determine if a particular event occurred or did not on a given trial. Our case fits into this framework nicely: was a cue present or not. When a cue is present we refer to it as a Positive trial, while the No-cue trial is a Negative trial. When the classifier correctly labels a trial, we say that trial is True, and incorrectly labeling a trial makes it False. For instance, a true positive trial would have the cue and be labeled as a cue trial by the logistic regression model. We can list all possible trial types using this phrasing.\n\nTP - True Positive, event occurred and was predicted as occurring * Actual Cue / Predicted Cue\nTN - True Negative, event did not occur and was predicted as not occurring * Actual No-cue / Predicted No-cue\nFP - False Postive, event did not occur but was predicted as occurring * Actual No-cue / Predicted Cue\nFN - False Negative, event occurred but was predicted as not occurring * Actual Cue / Predicted No-cue\n\nLooking back at the equation for balanced scores, we can break it down into something a bit more interpretable:\n \\begin{align}\n    \\notag \\frac{TP}{TP+FN} &= \\frac{\\text{Number of correctly predicted cue labeled trials}}{\\text{Number of all actual cue labeled trials}} = \\text{Proportion correctly labeled cue trials} \\\\\n    \\notag \\frac{TN}{TN+FP} &= \\frac{\\text{Number of correctly predicted no-cue labeled trials}}{\\text{Number of all actual no-cue labeled trials}} = \\text{Proportion correctly labeled no-cue trials} \\\\\n    \\end{align}\n\nThis means the balanced score equation is just taking the average correct proportion across both classes, regardless of how many trials belonged to either class.\nIf you are working with unbalanced data, then using the balanced accuracy score is essential for interpretable accuracy scores. When doing that, you should use the built in one provided by scikit-learn, the function balanced_accuracy_score. An additional advantage of the scikit-learn version is that it supports classifiers with more than two outcomes, which we will make use of later in the semester.\n\n# calculate balanced accuracy scores using the function provided by scikit-learn\nscore_bal_ub = balanced_accuracy_score(class_act_ub, class_pred_ub)\nprint(\"Balanced accuracy score for our model: {:.2f}\".format(score_bal_ub*100))\n\nscore_bal_ub = balanced_accuracy_score(class_act_ub, class_rand_ub)\nprint(\"Balanced accuracy score for random model: {:.2f}\".format(score_bal_ub*100))\n\nBalanced accuracy score for our model: 88.89\nBalanced accuracy score for random model: 54.67\n\n\nThe accuracy has gone down now, with the random case close to 50%, as would be expected. Normally we will use balanced data sets so there should not be much of a difference between the score returned by the estimator object and the balanced accuracy metric.\n\n\n\n\n\n\nCoding our own balanced accuracy score function\n\n\n\nTo better understand the balanced accuracy score, we can try coding our own. This should make things a bit clearer.\n\n# balanced accuracy score in base python\ndef bal_acc_score(y_act, y_pred):\n\n    # initialize variables\n    tp = 0; tn = 0; fp = 0; fn = 0\n\n    # loop through each pair of actual and predicted labels\n    for act, pred in zip(y_act, y_pred):\n        if act == 1 and pred == 1: # true positive\n            tp += 1\n        elif act == 1 and pred == 0: # false negative\n            fn += 1\n        elif act == 0 and pred == 1: # false positive\n            fp += 1\n        elif act == 0 and pred == 0: # true negative\n            tn += 1\n    \n    # calculate proportion of correct predictions for each class\n    prop_corr_pos = tp / (tp + fn)\n    prop_corr_neg = tn / (tn + fp)\n\n    # return average of the two proportions\n    return (prop_corr_pos + prop_corr_neg) / 2\n\nLet’s make sure our homemade version agrees with the one in scikit-learn.\n\n# see how our scores have changed using the balanced accuracy score metric\nscore_bal_ub = bal_acc_score(class_act_ub, class_pred_ub)\nprint(\"Balanced accuracy score for our model: {:.2f}\".format(score_bal_ub*100))\n\nscore_bal_ub = bal_acc_score(class_act_ub, class_rand_ub)\nprint(\"Balanced accuracy score for random model: {:.2f}\".format(score_bal_ub*100))\n\nBalanced accuracy score for our model: 88.89\nBalanced accuracy score for random model: 54.67\n\n\nYay, they agree!\n\n\nThe score measures above take actual and predicted class labels, compare and combine them, and return a single number that reflects the performance of our classifier. This is good as a general summary of how well the classifier works, but it leaves out details about how the classifier achieves that performance. For instance, with our case, is our model really good at detecting cue trials, or at no-cue trials? Does performance vary depending on the type of trial?\nA way to easily visualize the performance of a classifier in all its gritty details is with a confusion matrix. In a confusion matrix, each row corresponds to an actual label, and each column the predicted label. Each cell in the matrix has the number of trials with that particular pairing of actual and predicted labels. For a binary classifier, this looks like:\n Confusion \\: Matrix = \\begin{pmatrix}\n                        TN &FP \\\\\n                        FN &TP\n                    \\end{pmatrix}\n\nAlong the diagonal, we have the number of trials that were correctly classified for each label type. Off-diagonal elements show the number of confused (i.e. mislabeled) instances, hence the name. The instances most prone to confusion helps us diagnose problems in our classifier. As a case study, let’s examine the confusion matrices for our classifiers trained on balanced and unbalanced data sets.\nThe metrics portion of the scikit-learn package provides functions for computing confusion matrices.\n\n# use balanced model to predict the class for each observation\nclf.fit(vals, class_act)\nclass_pred = clf.predict(vals)\n\n# PLOT THE CONFUSION MATRIX\n# initialize the figure\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# calculate confusion matrix\nconfmat_full = confusion_matrix(class_act, class_pred)\n\n# plot the confusion matrix using built-in function\nConfusionMatrixDisplay(confusion_matrix=confmat_full, \n                       display_labels=['No-cue', 'Cue']).plot(ax=ax[0], cmap=plt.cm.Blues)\n\n# redo the same for the unbalanced dataset\nconfmat_ub = confusion_matrix(class_act_ub, class_pred_ub)\nConfusionMatrixDisplay(confusion_matrix=confmat_ub, \n                       display_labels=['No-cue', 'Cue']).plot(ax=ax[1], cmap=plt.cm.Blues)\n\n# label the figure\nax[0].set_title('Balanced dataset')\nax[1].set_title('Unbalanced dataset')\nfig.suptitle('Confusion matrices', fontsize=16)\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe confusion matrix shows that both classifiers works well. High counts along the diagonal indicate that labels are correctly predicted. For the balanced case, the misclassifications tend to be equal for Cue and No-cue trials. On the other hand, when we decreased the number of No-cue trials misclassifications only occurred for the No-cue trials. Moreover, we had fewer misclassifications for the Cue trials, which were now overrepresented in the data. Thus, the off diagonal elements tell us how the model trained on data with too many Cue trials is biased perform better on that trial type.\nThat error suggests that the decision boundary of our logistic model is biased towards lower ERP strengths for the unbalanced data. In this way, it will tend to never miss a cue trial, and classify No-cue trials as cue trials. To determine if this is the case, we might want to visualize the logistic functions fitted to our balanced and unbalanced datasets.\nTo do this, we can use the predict_proba method. This takes samples, not necessarily the ones we fitted on, and returns the probability of either class for each sample. A good strategy here to is take the minimum and maximum values of the features in your dataset, and then create a grid of samples that span their range. This works really well when we have less than 3 features per sample, or in our case just 1 feature. If we plot the predicted probabilities across values of x, we can visualize the logistic function.\n\n# create range of feature values\nmin_erp_peak = np.min(vals)\nmax_erp_peak = np.max(vals)\nerp_peak_vals = np.linspace(min_erp_peak, max_erp_peak, 100)\nerp_peak_vals = erp_peak_vals[:, np.newaxis]\n\n# predict probabilities over range of values\npred_probs = clf.predict_proba(erp_peak_vals)\nprint('The shape of pred_probs is: {}'.format(pred_probs.shape))\n\n# plot predicted probability of each class\nplt.plot(erp_peak_vals, pred_probs[:,0], label='NoCue')\nplt.plot(erp_peak_vals, pred_probs[:,1], label='Cue')\nplt.xlabel('ERP Peak (uV)')\nplt.ylabel('Predicted Probability')\nplt.title('Predicted Probability vs. ERP Peak')\nplt.yticks(np.arange(0,1.1,0.25))\nplt.grid()\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n\nThe shape of pred_probs is: (100, 2)\n\n\n\n\n\n\n\n\n\nNote that predict_proba returns a matrix with the same number of rows as the samples we passed in erp_peak_vals, and two columns, one for each class (NoCue or Cue). Since we only have two classes, the probabilities of NoCue and Cue will add to 1 for each sample. This makes one of them redundant, so for future plotting we will drop the NoCue one.\nIt is helpful to plot our classes with the data points layered on top, which helps us better visualize the performance of the fit and whether it is falling prey to some issues that will be discussed later.\n\ndef plot_logistic(lf_obj, vals, class_act, ax=None, labels=['NoCue', 'Cue'], color='tab:blue'):\n    \"\"\"Plot the logistic function for a given logistic regression object\n    and a range of values.\n\n    Parameters\n    ----------\n    lf_obj : sklearn.linear_model.LogisticRegression\n        The logistic regression object.\n    vals : array-like\n        The X values used to train the logistic model.\n    classes : array-like\n        The y values used to train the logistic model.\n    ax : matplotlib.axes.Axes\n        The axes to plot on.\n    \"\"\"\n\n    if ax is None:\n        _, ax =  plt.subplots()\n    \n    min_val = np.min(vals)\n    max_val = np.max(vals)\n    val_grid = np.linspace(min_val, max_val, 100)\n    val_grid = val_grid[:, np.newaxis]\n\n    # predict probabilities over range of values\n    lf_obj.fit(vals, class_act) # refit the model, because for some reason passing it as an argument removes the fit\n    pred_probs = lf_obj.predict_proba(val_grid)\n    ax.plot(val_grid, pred_probs[:,1], label='Logistic model', color=color)\n    ax.scatter(vals, class_act, c=color, alpha=0.5, label='True data')\n    ax.set_yticks(np.arange(0,1.1,0.25))\n    ax.set_yticklabels([labels[0], '0.25', '0.5', '0.75', labels[1]])\n    ax.grid()\n\nfig, ax = plt.subplots(2,1, sharex=True, sharey=True)\nplot_logistic(clf, vals, class_act, ax=ax[0])\nplot_logistic(clf_ub, vals_ub, class_act_ub, ax=ax[1], color='tab:orange')\nfig.suptitle('Logistic regression')\nfig.supxlabel('ERP dot product (uV)')\nfig.supylabel('P(Cue|ERP)')\nax[0].set_title('Balanced data')\nax[1].set_title('Unbalanced data')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe balanced logistic function seems to reflect the p(Cue|ERP) quite well. Its probability increases strongly after the bulk of the NoCue ERP peaks, and is near 1 when only Cue ERP peak values remain. The one trained on the unbalanced is shifted to the left, towards lower ERP values, biasing its classification towards the overrepresented Cue trials.\nMoving beyond this qualitative way of describing our logistic fit, it would be better to do so quantitatively, using the location parameter we discussed before. However, here we will have to make things a little more complicated by changing how we parameterize the model. This initial pain is worth it, though, because later it will give us far more freedom in how we can use the logistic function.\n\n\n\nThe parameters of the logistic function tell us how it assigns a class probability to a measure. When we first introduced it, we used location and scale parameters. These are not used by the LogisticRegression class. Instead, it is formulated as:\n \\sigma(x) = \\frac{1}{1+e^{-(b+wx)}} \nHere b stands for the bias or intercept, which is similar, but not exactly the same as the location parameter. w is the slope of the dependence on x, and is similar, but not exactly the same as the scale parameter.\nHow do these new parameters affect the logistic function?\n\ndef logistic(x, b=0, w=1):\n    return 1 / (1 + np.exp(-(w * x + b)))\n\n# plot the logistic function as b is varied\nx = np.linspace(-10, 10, 100)\nb_list = np.arange(-5,6,2.5)\nfor idx, b in enumerate(b_list):\n    plt.plot(x, logistic(x, b=b), label=f'$b$={b}', color=[idx/len(b_list), 0, 0])\n#plot legend outside axes\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.yticks(np.arange(0,1.1,.25))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.title('Logistic Function as $b$ is varied')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nLooks like b has exactly the same effect as location. What if we vary w?\n\n# plot logistic function as w is varied\nw_list = np.power(2.0, np.arange(-2, 3))\n\nfor idx, w in enumerate(w_list):\n    plt.plot(x, logistic(x, b=0, w=w), label='$w$={}'.format(w), color=[0,0,idx/len(w_list)])\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.title('Logistic Function as $w$ is varied')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nIt may look the same, but look closely at the line colors. When we increased the scale parameter, the spread of the logistic function decreased. But in this case, increasing the w parameter decreases the spread. And it gets worse, look what happens when we vary w and b is not set to 0:\n\n# plot logistic function as w is varied and b is not zero\nnew_b = -1\nfor idx, w in enumerate(w_list):\n    plt.plot(x, logistic(x, b= new_b, w=w), label='$w$={}'.format(w), color=[0,idx/len(w_list),0])\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.xlabel('$x$')\nplt.ylabel('$\\sigma(x)$')\nplt.title('Logistic Function as $w$ is varied and $b$ is fixed at {}'.format(new_b))\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWhen b is not at zero, then changing w affects the location of the logistic function. You can see this if you follow where each curve crosses the 0.5 line, which is the threshold for classifying a trial as receiving a Cue. We can use some simple algebra to resolve the relationship between location, w, and b.\nFirst, recognize that the loc parameter specifies when the logistic function is equal to 0.5. This happens when x = 0:\n \\begin{align}\n        \\notag 0.5&=\\frac{1}{1+e^{-x}} \\\\\n        \\notag 0.5&=\\frac{1}{1+e^{-0}} \\\\\n        \\notag 0.5&=\\frac{1}{1+1} \\\\\n        \\notag 0.5&=\\frac{1}{2}\n    \\end{align}\n\nHow does b and w relate to the x=0 location?  \\begin{align}\n        \\notag 0&=-(b+wx) \\\\\n        \\notag 0&=-b-wx \\\\\n        \\notag b&=-wx \\\\\n        \\notag -\\frac{b}{w}&=x\n    \\end{align}\n\nThe location parameter is important to us when interpreting the logistic function because it tells us when the classifier will label an ERP as arising from a Cue. It is the decision boundary. To get that, we need to pull out the b and w parameters from the fitted LogisticRegression object.\n\n# get b, also known as the intercept parameter\nb = clf.intercept_\nprint('The shape of intercept_ is: {}'.format(b.shape))\nprint('The intercept of our model is {:.2f}'.format(b[0]))\n\nThe shape of intercept_ is: (1,)\nThe intercept of our model is -4.37\n\n\nintercept_ is a class variable created once we call the fit method. It is a numpy array with a single value. To get w, access the coef_ variable.\n\n# get w from coef_\nw = clf.coef_\nprint('The shape of coef_ is: {}'.format(w.shape))\nprint('The value of coef_ is: {:.2f}'.format(w[0,0]))\n\nThe shape of coef_ is: (1, 1)\nThe value of coef_ is: 0.13\n\n\nNow that we have b and w, we can precisely position the decision boundary for our fitted logistic function.\n\n# calculate the decision boundary for balanced model\ndec_bound = -b[0]/w[0,0]\n\n# calculate the decision boundary for unbalanced model\nb_ub = clf_ub.intercept_\nw_ub = clf_ub.coef_\ndec_bound_ub = -b_ub[0]/w_ub[0,0]\n\n# print comparison of decision boundaries\nprint('Decision boundary for balanced model: {:.2f}'.format(dec_bound))\nprint('Decision boundary for unbalanced model: {:.2f}'.format(dec_bound_ub))\n\n# plot the logistic regression with decision boundaries marked\nfig, ax = plt.subplots(2,1, sharex=True, sharey=True)\nplot_logistic(clf, vals, class_act, ax=ax[0])\nplot_logistic(clf_ub, vals_ub, class_act_ub, ax=ax[1], color='tab:orange')\nfig.suptitle('Logistic regression')\nfig.supxlabel('ERP dot product (uV)')\nfig.supylabel('P(Cue|ERP)')\nax[0].set_title('Balanced data')\nax[1].set_title('Unbalanced data')\nax[0].axvline(dec_bound, color='r', linestyle='--')\nax[1].axvline(dec_bound_ub, color='r', linestyle='--')\nfig.tight_layout()\n\nDecision boundary for balanced model: 32.88\nDecision boundary for unbalanced model: 23.89\n\n\n\n\n\n\n\n\n\nUsing the logistic model’s internal parameters, we can explicitly calculate the decision boundary for classifying Cue trials. Doing this, we found that the model trained on balanced data has a boundary ~9 uV higher than the one trained on unbalanced data. This underscores the importance of considering the data ones trains the model on. Biases in the data will be reflected in the trained model, and in many cases impair predictions on new data it had not been trained on.\n\n\n\nOnce the model is fitted, you want to use it to fit new samples. Since we used all our data to fit the model, predicting on the same data would be circular. The model was optimized to perform best on the data set it was trained on, not the data it will receive following training. By chance, there may be tendencies in the training data that are not expressed in newly acquired trials. Since we do not have a time machine that allows us to go into the future and acquire the data the model will eventually have to classify (which, for that matter, would eliminate the need for a model anyway), we need a way to estimate how the model will perform on data it has not been trained on.\nThis is a well-recognized problem in evaluating the performance of statistical models, with a well-established solution - cross-validation. Instead of training the model on all our data, we train on a subset of the data, and test on the remainder that was held out. To do this, we need to divide our data into train and test sets, recalculate the mean ERP using the training set, fit the logistic model using the training set, and test the performance on the test set.\nScikit-learn provides a collection of functions for subdividing data and some of their estimators have versions with built-in cross-validation. We will make use of those, but first let’s code our own just to get a sense of how they work. To start, we will code a function to split our data into train and test sets.\nWhen specifying the size of the training and test datasets, we usually decide how many sets (or folds), k, we want to split the data set into. The model is trained on the most data possible, k-1 sets (\\frac{k-1}{k} % of data), and the last set is held out for testing (\\frac{1}{k} % of data). Deciding on a value for k requires balancing two needs. First, the model should be exposed to the most training data possible, so that it can be optimized on a representative sample. This inclines us to set k to a high value, so that a larger fraction of the data goes in to training. Second, we want to test the model on a representative sample as well, so that we get a more precise estimate of its performance. This inclines us to set k to a low value, which maximizes the data we test on. These conflicting demands need to be balanced based on the number of features the model is being trained on, how large the data set is, and the tendency of the model to overfit. All those issues will vary across situations, but a good rule of thumb is to set k=5. This means that 80% (4/5) of your data will be used for training, while the remaining 20% (1/5) is held in reserve for testing.\nWhen splitting our data, we also must decide how to do the actual splitting. The simplest approach is to split the data into sets based on their order. In that case, the first \\frac{4}{5}\\text{n\\_samples} are used for training, and the remaining \\frac{1}{5}\\text{n\\_samples} for testing. If you data is randomly ordered, so that different classes are equally represented at the beginning and end of the dataset, then this will typically work. On the other hand, if you data is like ours, where the first half of the samples are Cue trials, and the second half No-cue trials, then your train and test data sets will have different proportions of trial types and not be representative samples of the entire data set. To account for this, the samples you assign to the train or test data sets can be chosen randomly. On average, this should result in an equal proportion of samples for the different classes in the train and test sets. Let’s implement that:\n\n# Function to generate indices for train/test sets using k-fold random assignment\ndef train_test_kfold(n_samples, k=5):\n    # n_samples, number of samples in the dataset\n    # k, int, number of folds\n\n    fold_size = n_samples/k # number of samples in each fold\n    test_bool = np.arange(0, n_samples)&lt;=fold_size # boolean array of test indices\n    test_bool = np.random.permutation(test_bool) # randomize the test indices\n    test_idxs = np.where(test_bool==True)[0] # get the indices of the test set\n    train_idxs = np.where(test_bool==False)[0] # get the indices of the train set\n    return test_idxs, train_idxs\n\nOur function takes just two arguments, the number of samples, n_samples, and the number of folds, k, and returns randomly chosen indices for the test (test_idxs) and train (train_idxs) sets. Because we are assigning samples to the train and test groups at random, we should get equal proportions of Cue trials (or No-cue trials) in both sets. Before going further, we should make sure that is the case.\n\nnum_runs = 1000\nprop_cue_test = np.zeros(num_runs)\nprop_cue_train = np.zeros(num_runs)\nfor i in range(num_runs):\n    test_idxs, train_idxs = train_test_kfold(len(class_act))\n    prop_cue_test[i] = class_act[test_idxs].mean()\n    prop_cue_train[i] = class_act[train_idxs].mean()\n\nfig, ax = plt.subplots(2,1, sharex=True, sharey=True)\nax[0].hist(prop_cue_test, bins=np.linspace(0,1,20), alpha=0.5)\nax[1].hist(prop_cue_train, bins=np.linspace(0,1,20), alpha=0.5)\nax[0].set_title('Test set, mean proportion = {:.2f}'.format(prop_cue_test.mean()))\nax[1].set_title('Train set, mean proportion = {:.2f}'.format(prop_cue_train.mean()))\nfig.supylabel('Count')\nfig.supxlabel('Proportion of cues')\nfig.tight_layout()\nplt.show()\n\n# proportion of Cue trials across all trials\nprint('Proportion of Cue trials across all trials: {:.2f}'.format(class_act.mean()))\n\n\n\n\n\n\n\n\nProportion of Cue trials across all trials: 0.46\n\n\nOn average, random assignment of trials yields equal proportions of Cue trials to test and train sets. The variance is greater, though, for the smaller test set. Sometimes the test set will be dominated by Cue or No-cue trials, so for those cases will not offer a balanced assessment of classifier performance. To avoid this, we want to stratify assignment of trials to the test and train sets by the trial type. To do this, we will generate train and test sets for each class of trials separately, and then combine them.\n\n# Function to generate indices for train/test sets using stratified k-fold random assignment\ndef train_test_kstrat(class_act, k=5):\n    # class_act, array-like, shape (n_samples,)\n    # k, int, number of folds\n\n    # gets unique classes\n    uniq_classes = np.unique(class_act) \n\n    # initialize empty array for train indices\n    train_idxs = np.array([]) \n\n    # for each class, get indices of samples in class_ and add k-1/k of them to train_idxs\n    for class_ in uniq_classes: # variable named class_ to avoid confusion with 'class' keyword\n        # get indices of samples in class_\n        class_idxs = np.where(class_act==class_)[0] \n\n        # number of samples in class_\n        n_class = len(class_idxs) \n\n        # check if there are enough samples in class_ for k-fold cross validation\n        if n_class &lt; k:\n            raise ValueError(\"k-fold cross validation requires at least k samples in each class\")\n        \n        n_train = int(n_class*(k-1)/k) # number of from class_ in train set\n\n        # add train indices to train_idxs\n        train_idxs = np.append(train_idxs, np.random.choice(class_idxs, n_train, replace=False)) \n    \n    # indices not selected for the test group are assigned to the train group\n    test_idxs = np.setdiff1d(np.arange(0, len(class_act)), train_idxs) \n\n    return test_idxs.astype(int), train_idxs.astype(int) # return indices as integers\n\nThe stratified train/test splitting function assigns a random subset of samples from each class into a train group. The size of the subset is dictated by the k and number of samples of that class, ensuring that the proportion of samples used in the train and test set is the same for each class. This should result in the same proportions every time we call this splitting function.\n\n# get the distribution of Cue trials across multiple runs of the stratified train-test split\nnum_runs = 1000\nprop_cue_test = np.zeros(num_runs)\nprop_cue_train = np.zeros(num_runs)\nfor i in range(num_runs):\n    test_idxs, train_idxs = train_test_kstrat(class_act)\n    prop_cue_test[i] = class_act[test_idxs].mean()\n    prop_cue_train[i] = class_act[train_idxs].mean()\n\nfig, ax = plt.subplots(2,1, sharex=True, sharey=True)\nax[0].hist(prop_cue_test, bins=np.linspace(0,1,20), alpha=0.5)\nax[1].hist(prop_cue_train, bins=np.linspace(0,1,20), alpha=0.5)\nax[0].set_title('Test set, mean proportion = {:.2f}'.format(prop_cue_test.mean()))\nax[1].set_title('Train set, mean proportion = {:.2f}'.format(prop_cue_train.mean()))\nfig.supylabel('Count')\nfig.supxlabel('Proportion of cues')\nfig.tight_layout()\nplt.show()\n\n# proportion of Cue trials across all trials\nprint('Proportion of Cue trials across all trials: {:.2f}'.format(class_act.mean()))\n\n\n\n\n\n\n\n\nProportion of Cue trials across all trials: 0.46\n\n\nStatifying the assignment of samples based on their class has eliminated variability in their proportions. One drawback, though, is that the mean proportion is subtly different between the train and test sets, with 0.45 and 0.47 respectively. This bias happens because we cannot evenly split the samples from the Cue class into train and test sets, and since the same number of Cue samples are drawn each time, this difference persists across runs. By minimizing the variability in the proportions, we have introduced a bias in the proportions. There is a well-known bias-variance trade-off in statistics that speaks to this.\nReturning to the prediction question, we want to determine how our logistic regression model performs on data it was not trained on. Since the selection of trials in the test and train sets is random, a single estimate of performance would not tell us how the decoder performs in general. To estimate the performance, we will run it a thousand times. In addition, we will evaluate the performance on both the test and train data sets.\n\n# Fit and evalulate logistic regression model to ERP data using cross-validation\ndef logreg_traintest(epochs, class_act, k):\n    test_idxs, train_idxs = train_test_kstrat(class_act, k=5)\n\n    # set union of class_act and train_idxs\n    train_cue_idxs = np.intersect1d(np.where(class_act)[0], train_idxs)\n\n    # calculate ERP for training set\n    cue_erp = np.mean(epochs[:, train_cue_idxs], axis=1)\n\n    # calculate ERP alignment for each trial\n    val_all = erp_align(epochs, cue_erp)\n\n    # create train and test sets\n    val_test = val_all[test_idxs, np.newaxis]\n    class_act_test = class_act[test_idxs]\n    val_train = val_all[train_idxs, np.newaxis]\n    class_act_train = class_act[train_idxs]\n\n    # fit logistic regression model\n    clf_sub = LogisticRegression()\n    clf_sub.fit(val_train, class_act_train)\n\n    # get performance for train and test sets\n    score_test = clf_sub.score(val_test, class_act_test)\n    score_train = clf_sub.score(val_train, class_act_train)\n    return score_train*100, score_test*100\n\n# Run logistic regression model on ERP data\nnum_runs = 1000\nall_epochs = np.concatenate((cue_epochs, nocue_epochs), axis=1)\nscores_train = np.zeros(num_runs)\nscores_test = np.zeros(num_runs)\nfor i in range(num_runs):\n    scores_train[i], scores_test[i] = logreg_traintest(all_epochs, class_act, 5)\n\n# plot violin plot of scores compared across train and test sets\nfig, ax = plt.subplots(figsize=(2,5))\nax.violinplot([scores_train, scores_test], showmedians=True)\nax.set_xticks([1, 2])\nax.set_xticklabels(['Train', 'Test'])\nax.set_ylabel('Accuracy (%)')\nax.grid(axis='y')\nax.set_title('Cross-validated logistic regression performance')\n\n# print median performance for train and test sets\nprint('Median train accuracy: {:.2f} %'.format(np.median(scores_train)))\nprint('Median test accuracy: {:.2f} %'.format(np.median(scores_test)))\n\nMedian train accuracy: 86.05 %\nMedian test accuracy: 81.82 %\n\n\n\n\n\n\n\n\n\nPerformance on the train set is close to the performance achieved when we trained on the model on the entire data set. When the model was tested on the test set, performance was more variable. It is skewed to lower performance, sometimes much worse than the train set. We can compare the performance for each pair of train and test sets to determine if there was a systematic difference in performance.\n\n# difference in scores by train and test sets\nscore_diff = scores_test - scores_train\n\n# calculate median difference\nmedian_diff = np.median(scores_test - scores_train)\nprint(\"Median difference: {:0.2f} %\".format(median_diff))\n\n# plot distribution of score differences\nplt.hist(score_diff,20, label=\"Distribution\")\nplt.xlabel(\"Test score - train score\")\nplt.ylabel(\"Count\")\nplt.title(\"Histogram of score differences\")\nplt.axvline(median_diff, color='r', linestyle='--', linewidth=2, label=\"Median\")\nplt.legend()\nplt.show()\n\nMedian difference: -6.45 %\n\n\n\n\n\n\n\n\n\nThe median performance dropped ~7% on the test set. However, the extremes of the differences are wide, sometimes performing worse by 50% or better by 20%. It is common for performance to decrease on the test set, and the small drop we see here on average is not bad.\nBefore ending this section, we will go over how scikit-learn implements data splitting. It features a variety of splitting schemes, whose documentation can be found here.\nTo segregate data into train and test sets, you use a splitter object. Splitter objects are implemented similar to estimator objects. You first initialize them with parameters that are agnostic to the dataset, and then pass your data to the object’s split method, which returns indices for the train and test sets. Each time you call split, a new subset of indices is generated. To do this, it is implemented as type of iterator object (e.g. python’s range function) known as a generator object. Usually iterators are python objects that have an __iter__ method, but in the case of the splitter objects they use yield in their split method to return a new set of indices each time they are called.\n\n# initialize the KFold object\nksplit = KFold(n_splits=5, shuffle=True, random_state=47)\n\nThe first parameter for KFold is n_splits, which specifies how many folds, k, to use. shuffle sets whether the indices selected for the sets should be randomly drawn from across the data set. By default it is set to False, which will select indices in the order they occur in the dataset. Lastly, when randomly shuffling the indices we can specify the random seed to use using the random_state parameter so that the same random subset can be selected each time the function is called.\nNow you might be tempted to get the indices for the train and test sets by directly calling the split method, such as:\n\nksplit.split(vals)\n\n&lt;generator object _BaseKFold.split at 0x15fa68d60&gt;\n\n\nBut, this does not return what we want. Instead, it delivers a generator object that is not the indices we want. To get the indices, we have to call the split method as part of a loop, which engages its behavior as an iterator.\n\n# print the indices for each test set\nfor run_num, (train_idxs, test_idxs) in enumerate(ksplit.split(vals)):\n    print(\"Test indices on run {}: {}\".format(run_num+1, test_idxs))\n\nTest indices on run 1: [12 14 24 32 35 36 40 41 43 44 46]\nTest indices on run 2: [10 11 13 18 19 21 27 29 30 33 37]\nTest indices on run 3: [ 3  4  5 15 17 20 22 25 38 39 52]\nTest indices on run 4: [ 0  1  2  9 26 28 31 34 42 47 49]\nTest indices on run 5: [ 6  7  8 16 23 45 48 50 51 53]\n\n\nRepeated calls to the split method in the for loop returns a new set of indices each time. Notice that each set is distinct, no overlap between sets. This is because one often runs cross-validation multiple times and to ensure that independent data sets are used when measuring performance. This restriction means that the total number of runs for generating a cross-validation set is the number of splits, k. If you want just a single run to be returned, you can use the python function next.\n\ntrain_idxs, test_idxs = next(ksplit.split(vals))\nprint(\"Train indices: {}\".format(train_idxs))\nprint(\"Test indices: {}\".format(test_idxs))\n\nTrain indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 15 16 17 18 19 20 21 22 23 25 26\n 27 28 29 30 31 33 34 37 38 39 42 45 47 48 49 50 51 52 53]\nTest indices: [12 14 24 32 35 36 40 41 43 44 46]\n\n\nNormally when you call next with an iterator it will return the a different output each time it is called. This is not the case for the splitter objects, so if you want to get a new set of indices (and do not mind them overlapping with the previous set), then you have to reinitialize the splitter object each time.\nIf you want the splits to be balanced in their proportion of trial types, you can use a stratified approach to generate the train and test sets. This is done with the StratifiedKFold object. It is used similarly to KFold, except that when calling split you pass both the X and y parameters. y is used for stratifying the selection of indices by the classes. Here is how to use it:\n\n# initialize the KFold object\nkstratsplit = StratifiedKFold(n_splits=5, shuffle=True, random_state=47)\n\n# print the indices for each test set\nfor run_num, (train_idxs, test_idxs) in enumerate(kstratsplit.split(vals, class_act)):\n    print(\"Test indices on run {}: {}\".format(run_num+1, test_idxs))\n\nTest indices on run 1: [ 4  5  6 11 13 42 43 44 48 49 53]\nTest indices on run 2: [ 0  2 21 23 24 28 30 33 36 39 50]\nTest indices on run 3: [ 7  8  9 10 14 27 31 32 45 46 51]\nTest indices on run 4: [12 15 17 18 19 25 26 29 34 35 52]\nTest indices on run 5: [ 1  3 16 20 22 37 38 40 41 47]"
  },
  {
    "objectID": "Week3.html#putting-it-all-together",
    "href": "Week3.html#putting-it-all-together",
    "title": "Week3",
    "section": "",
    "text": "Now that we have gone over how to detect an ERP, we should pull all our code together into a single class that implements each stage of the fitting pipeline. We will use the builtin scikit-learn functions to do this.\n\nclass ERP_Decode():\n\n    def __init__(self, k=5, rand_seed=47, **kwargs):\n        self._k = k # number of folds for cross-validation\n        self._rand_seed = rand_seed # random seed for reproducibility\n        self._test_acc = None \n        self._train_acc = None\n        self.erp = None # ERP is the average of the ERP labeled epochs\n        self._logreg = LogisticRegression(**kwargs) # **kwargs allows us to pass in arguments to the LogisticRegression class\n        self._stratkfold = StratifiedKFold(n_splits=self._k, shuffle=True, random_state=self._rand_seed)\n    \n    def set_rand_seed(self, rand_seed):\n        # set random seed to generate new random folds\n        self._rand_seed = rand_seed\n        self._stratkfold = StratifiedKFold(n_splits=self._k, shuffle=True, random_state=self._rand_seed)\n    \n    def _erp_calc(self, epochs):\n        # calculate the ERP\n        return np.mean(epochs, axis=0)\n    \n    def _erp_align(self, epochs, erp):\n        # align the ERP to the origin\n        return np.dot(epochs, erp.T/np.linalg.norm(erp))\n    \n    def fit(self, epochs, labels):\n        # fit the model\n\n        # get the training and testing indices for first fold\n        train_idxs, test_idxs = next(self._stratkfold.split(epochs, labels))\n        \n        # get the training and testing data for first fold\n        y_train = labels[train_idxs]\n        X_train = epochs[train_idxs]\n        train_cue_idxs = np.intersect1d(np.where(y_train)[0], train_idxs)\n        self.erp = self._erp_calc(X_train[train_cue_idxs])\n        X_train = self._erp_align(X_train, self.erp)[:,np.newaxis]\n        X_test = self._erp_align(epochs[test_idxs], self.erp)[:,np.newaxis]\n        y_test = labels[test_idxs]\n        \n        # fit the model\n        self._logreg.fit(X_train, y_train)\n        self._train_acc = self._logreg.score(X_train, y_train)*100\n        self._test_acc = self._logreg.score(X_test, y_test)*100\n        \n        # return the training and testing accuracies\n        return self._train_acc, self._test_acc\n    \n    def decision_boundary(self):\n        # get the decision boundary\n        return -self._logreg.intercept_[0]/self._logreg.coef_[0,0]\n\n    def model_coef(self):\n        # get the model coefficients\n        return self._logreg.coef_[0,0]\n    \n    def model_intercept(self):\n        # get the model intercept\n        return self._logreg.intercept_[0]\n    \n    def predict(self, epochs):\n        # predict the labels of new data using the trained model\n        return self._logreg.predict(self.erp_align_(epochs, self.erp))\n\n\ntrain_ac = np.zeros(1000)\ntest_ac = np.zeros(1000)\nfor rep in range(1000):\n    erp_dec = ERP_Decode(rand_seed=rep)\n    train_ac[rep], test_ac[rep] = erp_dec.fit(all_epochs.T, class_act)\n\nprint(erp_dec.decision_boundary())\nfig, ax = plt.subplots(figsize=(2, 5))\nax.violinplot([train_ac, test_ac])\nax.set_xticks([1, 2])\nax.set_xticklabels(['Train', 'Test'])\nax.set_ylabel('Accuracy')\nax.set_ylim([20, 105])\nax.grid(axis='y')\nax.set_title('ERP Decoding')\n\n35.90827949750561\n\n\nText(0.5, 1.0, 'ERP Decoding')\n\n\n\n\n\n\n\n\n\nThis replicates the results we got earlier with the splitting functions we wrote ourselves. Performance on the training data typically exceeds that on the test data. Let’s save the parameters from the last model fitting. We will use them next week when we evaluate our hand-coded functions for fitting logistic regression models.\n\nw = erp_dec.model_coef()\nb = erp_dec.model_intercept()\nerp = erp_dec._erp_calc(all_epochs[:,class_act].T)\nX = erp_dec._erp_align(all_epochs.T, erp)\ny = class_act\n# had to convert X and y formats to save in JSON file\nlogreg_data = {'w': w, 'b': b, 'X': list(X), 'y': list(y.astype(float))}\nwith open(os.path.join('data', 'logregdata.json'), 'w') as f:\n    json.dump(logreg_data, f)\n\nReferences\nG. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics, Chapter 4, https://doi.org/10.1007/978-3-031-38747-0_4"
  }
]