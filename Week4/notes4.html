<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Logistic Regression from Scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="notes4_files/libs/clipboard/clipboard.min.js"></script>
<script src="notes4_files/libs/quarto-html/quarto.js"></script>
<script src="notes4_files/libs/quarto-html/popper.min.js"></script>
<script src="notes4_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="notes4_files/libs/quarto-html/anchor.min.js"></script>
<link href="notes4_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="notes4_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="notes4_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="notes4_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="notes4_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-problem" id="toc-the-problem" class="nav-link active" data-scroll-target="#the-problem">The problem</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss function</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient descent</a></li>
  <li><a href="#gradient-descent-for-the-logistic-function" id="toc-gradient-descent-for-the-logistic-function" class="nav-link" data-scroll-target="#gradient-descent-for-the-logistic-function">Gradient descent for the logistic function</a>
  <ul class="collapse">
  <li><a href="#determining-the-gradient-in-the-loss-landscape" id="toc-determining-the-gradient-in-the-loss-landscape" class="nav-link" data-scroll-target="#determining-the-gradient-in-the-loss-landscape">Determining the gradient in the loss landscape</a></li>
  </ul></li>
  <li><a href="#creating-a-logistic-regression-class-from-scratch-with-numpy" id="toc-creating-a-logistic-regression-class-from-scratch-with-numpy" class="nav-link" data-scroll-target="#creating-a-logistic-regression-class-from-scratch-with-numpy">Creating a logistic regression class from scratch with Numpy</a></li>
  <li><a href="#building-the-logistic-regression-model-with-pytorch" id="toc-building-the-logistic-regression-model-with-pytorch" class="nav-link" data-scroll-target="#building-the-logistic-regression-model-with-pytorch">Building the logistic regression model with PyTorch</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Logistic Regression from Scratch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Last week we used logistic regression to build our first brain signal decoder. Recall that the logistic model takes a measurement and determines the probability that it is associated with a binary outcome. In our case, we found that the degree of match between the EEG waveform and the ERP to a stimulus predicted whether a stimulus had been presented. We fit the logistic model using the scikit-learn package, but this masks much of what is going on ‘under-the-hood’ when it comes to fitting. The best way to learn about this is to code it ourselves, and in the process we will get a few side benefits.</p>
<ol type="1">
<li>It will introduce us to more theoretical aspects of <em>optimization</em>, such as convex functions and gradient descent. These topics are broadly applicable across the machine learning field.</li>
<li>The logistic model itself can be thought of as a single computational unit in a larger neural network, so a deep understanding of how it operates will help demystify more complex neural network models that you cover in other courses.</li>
<li>This knowledge will set us up for adding additional complexity to our logistic decoder.</li>
</ol>
<p>In subsequent lectures we will cover how to use logistic decoders with multiple inputs, and pool multiple decoders together to decode multiple categories of events.</p>
<div id="6459e164" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> colors</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>sys.path.append(<span class="st">'..'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> source.loaders <span class="im">import</span> EEG, remove_baseline_drift, remove_emg, remove_ac, detect_blinks</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="the-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-problem">The problem</h2>
<p>We have to find the parameters of a logistic function that best describe the relationship between neural features (ERP magnitude) and the trial class (Cue vs.&nbsp;No Cue). The space of possible parameters is infinite, and we want an approach to find them that is general enough to work with any data set we throw at it. One way is to randomly guess the parameters, and to keep guessing until we get a good fit. First we will load our data and the parameters from the model we fitted last week using the scikit-learn <code>LogisticRegression</code> estimator.</p>
<div id="2208547e" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load fitted parameters</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> <span class="st">'../Week3/data/'</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>logreg_data <span class="op">=</span> json.load(<span class="bu">open</span>(os.path.join(data_dir, <span class="st">'logregdata.json'</span>), <span class="st">'r'</span>))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># get the model parameters fitted by sklearn</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>w_sk <span class="op">=</span> logreg_data[<span class="st">'w'</span>] <span class="co"># weight/slope</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>b_sk <span class="op">=</span> logreg_data[<span class="st">'b'</span>] <span class="co"># bias/intercept</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array(logreg_data[<span class="st">'X'</span>]) <span class="co"># ERP strengths across trials</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array(logreg_data[<span class="st">'y'</span>]).astype(<span class="bu">bool</span>) <span class="co"># Trial classifications</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Sklearn model parameters:'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'w ='</span>, w_sk)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'b ='</span>, b_sk)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sklearn model parameters:
w = 0.051879134175060924
b = -1.8628904500466827</code></pre>
</div>
</div>
<p>Next we will plot some logistic curves for random guesses of the ‘w’ and ‘b’ parameters and see how they compare with the ones calculated by the <code>LogisticRegression</code> estimator.</p>
<div id="7027c423" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># some functions to help us visualize the performance of our random guesses</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic(x, w, b):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(w<span class="op">*</span>x <span class="op">+</span> b)))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_logistic(X_vals, y_vals, w, b, ax<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X_vals, y_vals, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(np.<span class="bu">min</span>(X_vals), np.<span class="bu">max</span>(X_vals), <span class="dv">100</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, logistic(x, w, b), <span class="st">'r'</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    ax.grid()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here are some random guesses:</p>
<div id="f293974a" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">4</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">6</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plot_logistic(X, y, <span class="dv">1</span>, <span class="dv">0</span>, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=1, b=0'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plot_logistic(X, y, <span class="fl">0.01</span>, <span class="dv">1</span>, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=0.01, b=1'</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plot_logistic(X, y, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">10</span>, ax<span class="op">=</span>ax[<span class="dv">2</span>])</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=-1, b=-100'</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>plot_logistic(X, y, w_sk, b_sk, ax<span class="op">=</span>ax[<span class="dv">3</span>])</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">'Scikit-learn fit:</span><span class="ch">\n</span><span class="st"> w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_sk, b_sk),color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>fig.supxlabel(<span class="st">'X'</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'y'</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-5-output-1.png" width="381" height="576" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Those random guesses don’t look too good! While it may be tempting to keep trying different combinations of <code>w</code> and <code>b</code> to get a better fit, that would be a poor use of our time. Moreover, we would have to go through the same process for each new data set.</p>
<p>Instead, perhaps we could start with a random choice, and then iteratively update our parameters to try and get a better fit. At each update, we want to move the parameters so the fit gets better. This process is called <em>optimization</em>. For this to work, we need two things: a measure of the error in our fit (referred to as a <em>loss function</em>), and a method for minimizing the loss function (we will use <em>gradient descent</em>). Let’s discuss each of these in turn.</p>
</section>
<section id="loss-function" class="level2">
<h2 class="anchored" data-anchor-id="loss-function">Loss function</h2>
<p>The loss function reflects the difference between the actual class, <span class="math inline">y</span>, and the predicted probability of that class, <span class="math inline">\hat{y}</span>. The lower value it has, the better the fit. To start building this function, let’s consider the probability of <span class="math inline">y</span> given our prediction, <span class="math inline">\hat{y}</span>, expressed as <span class="math inline">p(y|\hat{y})</span>. Since <span class="math inline">y</span> is a binary variable (i.e.&nbsp;Cue or NoCue), we can express the probability as a Bernoulli distribution:</p>
<p><span class="math display"> p(y|\hat{y}) = \hat{y}^y(1-\hat{y})^{1-y} \tag{1}</span></p>
<p>Note that if <span class="math inline">y=1</span>, we just return <span class="math inline">\hat{y}</span>, because:</p>
<p><span class="math display"> \begin{align}
    \notag p(1|\hat{y}) &amp;= \hat{y}^1(1-\hat{y})^{1-1}  \\
    \notag &amp;= \hat{y}^1(1-\hat{y})^{0} \\
    \notag &amp;= \hat{y}^1\times 1 \\
    \notag &amp;= \hat{y}
    \end{align}
</span></p>
<p>and if <span class="math inline">y=0</span>, then <span class="math inline">1-\hat{y}</span> is returned: <span class="math display"> \begin{align}
    \notag p(0|\hat{y}) &amp;= \hat{y}^0(1-\hat{y})^{1-0}  \\
    \notag &amp;= 1\times(1-\hat{y})^{1} \\
    \notag &amp;= 1-\hat{y}
    \end{align}
</span></p>
<p>Alternatively, it might make more sense to you if we express equation 1 as a piecewise function. <span class="math display"> p(y|\hat{y})=\begin{cases}
    \hat{y} &amp; y=1 \\
    1-\hat{y} &amp; y=0
    \end{cases}
</span></p>
<p>Each possibility is represented as a different case enclosed by the large bracket. Here y=1 when a cue occurs, and y=0 when no cue is present. If a cue occurred, then we return the probability of the cue, <span class="math inline">\hat{y}</span>. If no cue occurred, then we return the probability that no cue occurred, <span class="math inline">1-\hat{y}</span>.</p>
<p>How does equation 1 behave depending on <span class="math inline">y</span> and <span class="math inline">\hat{y}</span>?</p>
<div id="01ebdbfc" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># method for calculating Bernoulli distribution</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bernoulli_dist(y, y_hat):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.power(y_hat, y) <span class="op">*</span> np.power(<span class="dv">1</span><span class="op">-</span>y_hat,<span class="dv">1</span><span class="op">-</span>y)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># array of potential y_hat values, probabilities range from 0 to 1</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the Bernoulli distribution for y=1 and y=0</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>ax.plot(y_hat, bernoulli_dist(<span class="dv">1</span>, y_hat), label<span class="op">=</span><span class="st">'y=1'</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>ax.plot(y_hat, bernoulli_dist(<span class="dv">0</span>, y_hat), label<span class="op">=</span><span class="st">'y=0'</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'$\hat</span><span class="sc">{y}</span><span class="vs">$'</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$p(y|\hat</span><span class="sc">{y}</span><span class="vs">)$'</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Bernoulli Distribution'</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-6-output-1.png" width="594" height="454" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Using the Bernoulli distribution, if we fit a model successfully so that it correctly predicts <span class="math inline">y</span>, then trials with a cue (<span class="math inline">y=1</span>) will return high values of <span class="math inline">\hat{y}</span>. Correctly predicting no cue (<span class="math inline">y=0</span>), will give high values as well since <span class="math inline">1-\hat{y}</span> is returned. Performing this calculation for each pair of predicted and actual values, we can sum those together and evaluate the overall performance of our model. Put another way, a model with good performance will on average give larger values for <span class="math inline">p(y|\hat{y})</span>. We can use this to compare the random model fits above.</p>
<p>To visualize this, we can add arrows to our logistic regression graph that represent the output of the Bernoulli probability function for each sample.</p>
<div id="ac85eba9" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># method to plot the logistic model, </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># corresponding Bernoulli distribution values for each data point, </span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and the mean of the Bernoulli distribution values</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_logistic_bern(X_vals, y_vals, w, b, ax<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plot_logistic(X_vals, y_vals, w, b, ax<span class="op">=</span>ax)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    sign_y <span class="op">=</span> np.sign(y_vals <span class="op">-</span> <span class="fl">0.5</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    bern_vals <span class="op">=</span> bernoulli_dist(y_vals, logistic(X_vals.squeeze(), w, b))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    ax.quiver(X_vals, <span class="dv">1</span><span class="op">-</span>y_vals, np.zeros(y_vals.size), </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>              sign_y<span class="op">*</span>bern_vals, angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax, np.mean(bern_vals)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">4</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">6</span>))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>_, bern_sum <span class="op">=</span> plot_logistic_bern(X, y, <span class="dv">1</span>, <span class="dv">0</span>, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=1, b=0, Bernoulli mean=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(bern_sum))</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>_, bern_sum <span class="op">=</span> plot_logistic_bern(X, y, <span class="fl">0.01</span>, <span class="dv">1</span>, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=0.01, b=1, Bernoulli mean=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(bern_sum))</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>_, bern_sum <span class="op">=</span> plot_logistic_bern(X, y, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">10</span>, ax<span class="op">=</span>ax[<span class="dv">2</span>])</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=-1, b=-10, Bernoulli mean=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(bern_sum))</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>_, bern_sum <span class="op">=</span> plot_logistic_bern(X, y, w_sk, b_sk, ax<span class="op">=</span>ax[<span class="dv">3</span>])</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">'Scikit-learn fit:</span><span class="ch">\n</span><span class="st"> w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">, Bernoulli mean=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_sk, b_sk, bern_sum), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>fig.supxlabel(<span class="st">'X'</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'y'</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-7-output-1.png" width="395" height="576" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Each data point has a black arrow pointing at it whose length is the Bernoulli distribution value for that particular observation. Longer arrows indicate a better match between the predicted and actual class. The top example has a high mean probability. This is because it has more of its Cue trials on the high probability side, and NoCue trials on the low probability side. The second example is similar, but the logistic function is so broad that the expected probabilities, <span class="math inline">\hat{y}</span>, are never close to 0 or 1. Worst is the third example, whose sign is flipped with peak probability predicted for low x-values that is only summing probabilities for NoCue trials. Lastly, the fitted parameters derived from the scikit-learn logistic regression object have the highest, and best, mean probability.</p>
<p>The behavior of the Bernoulli probability function makes it a good basis for constructing our loss function. To make it an actual loss function, we will make two changes. First, we will take its logarithm. This has several benefits. For now, we will just note that it changes the shape of the probability curves to give stronger weight to mismatches between the predicted and actual classes. Put another way, wrong answers are penalized more than correct ones. Second, we negate it so that lower values correspond to better fits. This is because the process we use for optimization is <em>gradient</em> <strong><em>descent</em></strong>, which tries to minimize the loss function. Semantically this makes sense, a smaller loss value should correspond to a better fit. So how does our loss function look?</p>
<p><span class="math display"> \begin{align}
    \notag -\log(p(y|\hat{y})) &amp;= -\log(\hat{y}^y(1-\hat{y})^{1-y}) \\
    \notag &amp;= -(y\log(\hat{y}) + (1-y)\log(1-\hat{y})) \\
    \end{align}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Rules for working with logarithms
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you have not dealt with logarithms in a while, below are some helpful rules to keep in mind.</p>
<ol type="1">
<li><span class="math inline">\log(xy) = \log(x) + \log(y)</span></li>
<li><span class="math inline">\log\left(\frac{x}{y}\right) = \log(x) - \log(y)</span></li>
<li><span class="math inline">\log(x^n) = n\log(x)</span></li>
<li><span class="math inline">\log(1) = 0</span></li>
<li><span class="math inline">\log(0) = \text{undefined}</span></li>
</ol>
</div>
</div>
<p>This loss function is referred to as the cross-entropy loss and is used in many machine learning applications that perform binary classification besides just logistic regression. Let’s convert this equation to code and visualize its behavior depending on whether the actual class (<span class="math inline">\hat{y}</span>) is supposed to be 0 or 1</p>
<div id="d6474dd1" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cross entropy loss function</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy(y, y_hat):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(y<span class="op">*</span>np.log(y_hat) <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span>np.log(<span class="dv">1</span><span class="op">-</span>y_hat))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot comparison of Bernoulli and cross-entropy loss</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># note we start at 0.01 to avoid log(0), which is undefined</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">4</span>))</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the cross-entropy loss when y=0</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(x, cross_entropy(<span class="dv">0</span>, x), color<span class="op">=</span><span class="st">'tab:blue'</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'y=0'</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].grid()</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'$\hat</span><span class="sc">{y}</span><span class="vs">$'</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].annotate(<span class="st">'Correct</span><span class="ch">\n</span><span class="st">prediction'</span>, xy<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), xytext<span class="op">=</span>(<span class="fl">0.21</span>, <span class="fl">1.1</span>), </span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>                arrowprops<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'gray'</span>, width<span class="op">=</span><span class="dv">1</span>, headwidth<span class="op">=</span><span class="dv">8</span>))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].annotate(<span class="st">'Incorrect</span><span class="ch">\n</span><span class="st">prediction'</span>, xy<span class="op">=</span>(<span class="fl">0.99</span>, cross_entropy(<span class="dv">0</span>,<span class="fl">0.99</span>)), xytext<span class="op">=</span>(<span class="fl">0.61</span>, <span class="fl">3.1</span>), </span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>                arrowprops<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'gray'</span>, width<span class="op">=</span><span class="dv">1</span>, headwidth<span class="op">=</span><span class="dv">8</span>))</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the cross-entropy loss when y=1</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, cross_entropy(<span class="dv">1</span>, x), color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'y=1'</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].grid()</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$\hat</span><span class="sc">{y}</span><span class="vs">$'</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].annotate(<span class="st">'Incorrect</span><span class="ch">\n</span><span class="st">prediction'</span>, xy<span class="op">=</span>(<span class="fl">0.01</span>, cross_entropy(<span class="dv">0</span>,<span class="fl">0.99</span>)), xytext<span class="op">=</span>(<span class="fl">0.21</span>, <span class="fl">3.1</span>), </span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>                arrowprops<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'gray'</span>, width<span class="op">=</span><span class="dv">1</span>, headwidth<span class="op">=</span><span class="dv">8</span>))</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].annotate(<span class="st">'Correct</span><span class="ch">\n</span><span class="st">prediction'</span>, xy<span class="op">=</span>(<span class="fl">0.99</span>, <span class="dv">0</span>), xytext<span class="op">=</span>(<span class="fl">0.61</span>, <span class="fl">1.1</span>), </span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>                arrowprops<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'gray'</span>, width<span class="op">=</span><span class="dv">1</span>, headwidth<span class="op">=</span><span class="dv">8</span>))</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Cross-entropy loss'</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8088/1054478998.py:3: RuntimeWarning:

divide by zero encountered in log

/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8088/1054478998.py:3: RuntimeWarning:

invalid value encountered in multiply
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>Text(0.5, 1.05, 'Cross-entropy loss')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-8-output-3.png" width="757" height="407" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here we can see that the cross-entropy loss function has the desired properties of a loss function. It is larger the more incorrect our probability is (e.g.&nbsp;<span class="math inline">y=0</span> has largest loss when <span class="math inline">\hat{y}=1</span>). As the predicted probability approaches the true probability, the loss approaches 0, reaching it when our prediction is correct. This is the case for both types of outcomes, 0 or 1, or in our specific case Cue or NoCue.</p>
<p>The cross-entropy loss function as written above only applies to a single sample. However, when fitting the logistic regression we have to minimize the loss across multiple samples. We can do this by taking the mean of the cross-entropy loss across samples like so:</p>
<p><span class="math display"> loss = -\frac{1}{N}\sum_{i=0}^{N}y_{i}\log(\hat{y}_{i}) + (1-y_{i})\log(1-\hat{y}_{i}) \tag{2}</span></p>
<p>Here <span class="math inline">i</span> refers to a sample and we have <span class="math inline">N</span> total samples in our data set. Translating this into code we get:</p>
<div id="39426781" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Version of the cross entropy loss function that explicitly shows the mean calculation</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy_loss_verbose(y, y_hat):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate the cross entropy loss</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    y : array</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">        True labels</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    y_hat : array</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Predicted probabilities</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">    loss : float</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">        The mean cross entropy loss</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> y.size</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> <span class="op">-</span>(y[i]<span class="op">*</span>np.log(y_hat[i])<span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y[i])<span class="op">*</span>np.log(<span class="dv">1</span><span class="op">-</span>y_hat[i]))</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss<span class="op">/</span>N</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Version of the cross entropy loss function that uses numpy mean</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy_loss(y, y_hat):</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate the cross entropy loss</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="co">    y : array</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="co">        True labels</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="co">    y_hat : array</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co">        Predicted probabilities</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="co">    loss : float</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="co">        The mean cross entropy loss</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(<span class="op">-</span>(y<span class="op">*</span>np.log(y_hat)<span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span>np.log(<span class="dv">1</span><span class="op">-</span>y_hat)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we can calculate the loss, we can see what value it takes for each of our guesses for the logistic regression parameters, and for the one returned by scikit-learn.</p>
<div id="feaa6b7f" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the cross-entropy loss for each set of model parameters</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">4</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">6</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plot_logistic_bern(X, y, <span class="dv">1</span>, <span class="dv">0</span>, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(y, logistic(X.squeeze(), <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=1, b=0, loss=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(loss))</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>plot_logistic_bern(X, y, <span class="fl">0.01</span>, <span class="dv">1</span>, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(y, logistic(X.squeeze(), <span class="fl">0.01</span>, <span class="dv">1</span>))</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=0.01, b=1, loss=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(loss))</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>plot_logistic_bern(X, y, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">10</span>, ax<span class="op">=</span>ax[<span class="dv">2</span>])</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(y, logistic(X.squeeze(), <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">10</span>))</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=-1, b=-10, loss=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(loss))</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>plot_logistic_bern(X, y, w_sk, b_sk, ax<span class="op">=</span>ax[<span class="dv">3</span>])</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(y, logistic(X.squeeze(), w_sk, b_sk))</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">'Scikit-learn fit:</span><span class="ch">\n</span><span class="st"> w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">, loss=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_sk, b_sk, loss), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>fig.supxlabel(<span class="st">'X'</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'y'</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8088/3423839434.py:44: RuntimeWarning:

divide by zero encountered in log

/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8088/3423839434.py:44: RuntimeWarning:

invalid value encountered in multiply
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-10-output-2.png" width="381" height="576" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Uh oh, we can already see some problems here. The first and third guesses give a loss of ‘nan’ and ‘inf’, respectively. These likely reflect instances where a 0 is passed to <code>np.log</code>, which returns -infinity. To address this problem, we can set all instances of <code>y_hat</code> equal to 0 to the smallest possible floating point value, and <code>'y_hat</code> equal to 1 to its closest value.</p>
<div id="23a81b8e" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fixed version of the cross entropy loss function that uses numpy mean</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy_loss(y, y_hat):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate the cross entropy loss</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    y : array</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">        True labels</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">    y_hat : array</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Predicted probabilities</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co">    loss : float</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co">        The mean cross entropy loss</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># replace 0s with machine epsilon, the smallest possible floating point number</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    y_hat[y_hat<span class="op">==</span><span class="dv">0</span>] <span class="op">=</span> np.finfo(<span class="bu">float</span>).eps </span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># replace 1s with 1 minus machine epsilon</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    y_hat[y_hat<span class="op">==</span><span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>np.finfo(<span class="bu">float</span>).eps </span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the loss</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.mean(<span class="op">-</span>(y<span class="op">*</span>np.log(y_hat)<span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span>np.log(<span class="dv">1</span><span class="op">-</span>y_hat)))</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="34153baa" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the fixed version of the cross-entropy loss for each set of parameters</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">4</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">6</span>))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plot_logistic_bern(X, y, <span class="dv">1</span>, <span class="dv">0</span>, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(y, logistic(X.squeeze(), <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=1, b=0, loss=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(loss))</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plot_logistic_bern(X, y, <span class="fl">0.01</span>, <span class="dv">1</span>, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(y, logistic(X.squeeze(), <span class="fl">0.01</span>, <span class="dv">1</span>))</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=0.01, b=1, loss=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(loss))</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>plot_logistic_bern(X, y, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">10</span>, ax<span class="op">=</span>ax[<span class="dv">2</span>])</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(y, logistic(X.squeeze(), <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">10</span>))</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'Randomly selected:</span><span class="ch">\n</span><span class="st"> w=-1, b=-10, loss=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(loss))</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>plot_logistic_bern(X, y, w_sk, b_sk, ax<span class="op">=</span>ax[<span class="dv">3</span>])</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(y, logistic(X.squeeze(), w_sk, b_sk))</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].set_title(<span class="st">'Scikit-learn fit:</span><span class="ch">\n</span><span class="st"> w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">, loss=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_sk, b_sk, loss), color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>fig.supxlabel(<span class="st">'X'</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'y'</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-12-output-1.png" width="381" height="576" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Much better! It is not surprising that the cases with the largest loss were the ones previously undefined. They were most likely to have predicted probabilities returned that were closest to 0 or 1 since most data points fell on portions of the logistic equation close to its asymptotes.</p>
<p>To summarize, the cross-entropy loss captures the degree to which the predicted probability of an event agrees with whether that event did or did not occur. Larger values indicate worse predictions, while smaller values suggest better predictions. We calculate this across all samples in our data and return their mean, giving an assessment of the overall performance of our logistic decoder.</p>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h2>
<p>Now that we have a function that captures the performance of our logistic model, we want to find the combination of <span class="math inline">w</span> and <span class="math inline">b</span> parameters that minimize the cross-entropy loss. Scikit-learn offers several different approaches, but all of these are essentially engaging in some form of gradient descent. Indeed, virtually all neural networks are trained at least in part via some form of gradient descent. Thus, what we cover here will apply broadly.</p>
<p>To start off, let’s consider how gradient descent works with a toy example. Imagine we have a loss function of the form: <span class="math display">loss(x) = x^2</span> This is a quadratic function, where <span class="math inline">x</span> is the parameter that we seek to change to minimize the value of our loss, <span class="math inline">x^2</span>. We can imagine that <span class="math inline">loss(x)</span> is a surface, so to start exploring that surface we choose a random value of <span class="math inline">x</span> that will be our starting position.</p>
<div id="33202415" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># toy loss function</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> toy_loss(x):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># method to plot the landscape of the toy loss function</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss(ax<span class="op">=</span><span class="va">None</span>, loss<span class="op">=</span>toy_loss):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss(x)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, loss, color<span class="op">=</span><span class="st">'tab:blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'loss'</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># method to plot the path of gradient descent on the toy loss function</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss_gd(ax<span class="op">=</span><span class="va">None</span>, loss<span class="op">=</span>toy_loss, x_pts<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x_pts <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot points using linked arrows, using matplotlib with the start and end points labeled</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        ax.quiver(x_pts[:<span class="op">-</span><span class="dv">1</span>], loss(x_pts[:<span class="op">-</span><span class="dv">1</span>]), np.diff(x_pts), np.diff(loss(x_pts)), </span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>                  angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'tab:orange'</span>, width<span class="op">=</span><span class="fl">0.005</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        ax.scatter(x_pts, loss(x_pts), color<span class="op">=</span><span class="st">'tab:orange'</span>, label<span class="op">=</span><span class="st">'GD path'</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        ax.text(x_pts[<span class="dv">0</span>], loss(x_pts[<span class="dv">0</span>]), <span class="st">'start'</span>, ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'top'</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x_pts.size <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>            ax.text(x_pts[<span class="op">-</span><span class="dv">1</span>], loss(x_pts[<span class="op">-</span><span class="dv">1</span>]), <span class="st">'end'</span>, ha<span class="op">=</span><span class="st">'left'</span>, va<span class="op">=</span><span class="st">'bottom'</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        ax.legend()</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize random seed numpy</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">47</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="co"># choose our starting point at random</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>x_hist <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Starting point:'</span>, x_hist[<span class="dv">0</span>])</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_loss()</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>plot_loss_gd(ax<span class="op">=</span>ax, x_pts<span class="op">=</span>x_hist)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Toy loss function'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting point: -3.8651152810635048</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>Text(0.5, 1.0, 'Toy loss function')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-13-output-3.png" width="585" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here the blue line is the loss landscape, a typical quadratic function with a minimum at <span class="math inline">x=0</span>. We have chosen a random start point that is denoted by an orange dot. Our starting point is at <span class="math inline">x=-3.86</span>, which is not at the minimum.</p>
<p>To get closer to the minimum, we can measure the slope of the loss landscape at our starting point, and then move our dot in the direction where that slope is descending. To get that slope, we need the derivative of our loss function, which is <span class="math inline">2x</span> (solved using the derivative power rule <span class="math inline">\frac{d}{dx}x^n=nx^{n-1}</span>).</p>
<p>Let’s code this now:</p>
<div id="3a20bf70" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate toy loss derivative</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> toy_loss_derivative(x):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">2</span><span class="op">*</span>x</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient descent step method</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gd_step(x, d_loss<span class="op">=</span>toy_loss_derivative):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform one step of gradient descent</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">    x : float</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Current value of x</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">    d_loss : function</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">        The derivative of the loss function</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co">    x_new : float</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co">        New value of x after one step of gradient descent</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    x_new <span class="op">=</span> x <span class="op">-</span> d_loss(x)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_new</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a><span class="co"># run gradient descent for 1 step</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>x_hist <span class="op">=</span> np.append(x_hist, gd_step(x_hist[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_loss()</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>plot_loss_gd(ax<span class="op">=</span>ax, x_pts<span class="op">=</span>x_hist)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Toy loss function'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>Text(0.5, 1.0, 'Toy loss function')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-14-output-2.png" width="585" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Oh no, we moved our dot in the right direction, towards <span class="math inline">x=0</span>, but proceeded to go too far and are now on the other side of the quadratic function. We may want to slow down how fast we are moving our dot so that we minimize the risk of overshooting. For that, we will add an additional argument to our <code>gd_step</code> function, <code>lr</code>, which stands for <em>learning rate</em> (also referred to as eta, <span class="math inline">\eta</span>). The learning rate is a factor (usually much less than 1) that we multiply the gradient by to slow down the speed at which the parameters update.</p>
<div id="1da4f993" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># undo the last, ill-advised step</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>x_hist <span class="op">=</span> x_hist[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient descent step method</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gd_step(x, d_loss<span class="op">=</span>toy_loss_derivative, lr<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform one step of gradient descent</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">    x : float</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Current value of x</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co">    d_loss : function</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co">        The derivative of the loss function</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co">    lr : float</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="co">        The learning rate. Default is 0.1</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co">    x_new : float</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="co">        New value of x after one step of gradient descent</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    x_new <span class="op">=</span> x <span class="op">-</span> lr<span class="op">*</span>d_loss(x)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_new</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="co"># run gradient descent for 1</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>x_hist <span class="op">=</span> np.append(x_hist, gd_step(x_hist[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_loss()</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>plot_loss_gd(ax<span class="op">=</span>ax, x_pts<span class="op">=</span>x_hist)</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Toy loss function'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>Text(0.5, 1.0, 'Toy loss function')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-15-output-2.png" width="585" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Much better. We have moved closer to the minimum of our loss function, and without overshooting it. Let’s keep going for several more iterations.</p>
<div id="3f937d35" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run gradient descent for 5 more steps</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    x_hist <span class="op">=</span> np.append(x_hist, gd_step(x_hist[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_loss()</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>plot_loss_gd(ax<span class="op">=</span>ax, x_pts<span class="op">=</span>x_hist)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Toy loss function'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>Text(0.5, 1.0, 'Toy loss function')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-16-output-2.png" width="585" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>After 6 iterations we are closer to the bottom, but still not quite there. Note that the loss changes less and less as we get closer to the minimum. Instead of trying to guess how many iterations are required, we could keep running the gradient descent process until the changes in the loss falls below a predetermined level, known as a <em>tolerance</em>. Here is how we can code that:</p>
<div id="86247817" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># incorporate a stopping criterion into the gradient descent step method</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gd_run(x, d_loss<span class="op">=</span>toy_loss_derivative, lr<span class="op">=</span><span class="fl">0.1</span>, tol<span class="op">=</span><span class="fl">0.01</span>, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform gradient descent</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">    x : float</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Starting value of x</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co">    d_loss : function</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co">        The derivative of the loss function</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co">    lr : float</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co">        The learning rate. Default is 0.1</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co">    tol : float</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co">        The stopping criterion. Default is 0.01</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co">    max_iter : int</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="co">        The maximum number of iterations. Default is 100</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="co">    x_hist : array</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="co">        History of x values</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="co">    i : int</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Number of iterations</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x is the starting point, so we initialize the history with it</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>    x_hist <span class="op">=</span> [x]</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run gradient descent until the stopping criterion is met</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># or until we reach the maximum number of iterations</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate the next step</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>        x_new <span class="op">=</span> gd_step(x_hist[<span class="op">-</span><span class="dv">1</span>], d_loss<span class="op">=</span>d_loss, lr<span class="op">=</span>lr)</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>        x_hist.append(x_new)</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># check if the stopping criterion is met</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if so, break out of the loop</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">abs</span>(x_new <span class="op">-</span> x_hist[<span class="op">-</span><span class="dv">2</span>]) <span class="op">&lt;</span> tol:</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return the history of x values and the number of iterations</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    x_hist <span class="op">=</span> np.array(x_hist).ravel()</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> i<span class="op">+</span><span class="dv">1</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_hist, i</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This method automates our calls to the <code>gd_step</code> function. Given a starting point specified by <code>x</code>, and a learning rate (<code>lr</code>), it will iteratively run <code>gd_step</code> until either our loss is below tolerance (<code>tol</code>) or maximum number of iterations is reached (<code>max_iter</code>).</p>
<div id="b86905f2" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">47</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>x_init <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># run gradient descent with a learning rate of 0.1</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>x_hist, n_iter <span class="op">=</span> gd_run(x_init, lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_loss()</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plot_loss_gd(ax<span class="op">=</span>ax, x_pts<span class="op">=</span>x_hist)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Toy loss function</span><span class="ch">\n</span><span class="st">Learning rate = 0.1</span><span class="ch">\n</span><span class="st">Iterations = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(n_iter))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Text(0.5, 1.0, 'Toy loss function\nLearning rate = 0.1\nIterations = 21')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-18-output-2.png" width="585" height="485" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Just for fun, let’s increase the learning rate and see if we can reach the minimum faster.</p>
<div id="11992afe" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">47</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>x_init <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># run gradient descent with a learning rate of 0.8</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>x_hist, n_iter <span class="op">=</span> gd_run(x_init, lr<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_loss()</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>plot_loss_gd(ax<span class="op">=</span>ax, x_pts<span class="op">=</span>x_hist)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Toy loss function</span><span class="ch">\n</span><span class="st">Learning rate = 0.8</span><span class="ch">\n</span><span class="st">Iterations = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(n_iter))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>Text(0.5, 1.0, 'Toy loss function\nLearning rate = 0.8\nIterations = 14')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-19-output-2.png" width="585" height="485" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice how increasing the learning rate decreased the number of iterations it took to reach the minimum, but at the cost of causing our path towards that minimum to oscillate. This underscores the importance of <em>hyperparameters</em>, such as learning rate, when using machine learning approaches. These often have dramatic impacts whether we are successful in training a decoder.</p>
<p>So far it seems that if we let gradient run for long enough we can get arbitrarily close to the minimum. But this is because we have chosen a function, <span class="math inline">x^2</span> that is well behaved for being solved with gradient descent. On the other hand, there are some functions whose minimum is not guaranteed to be found by gradient descent. On what functions could gradient descent fail? Gradient descent will find the minimum loss if the landscape is <em>convex</em>. A simple geometric intuition for whether a curve is convex is if you can draw a line between any two points on it, no points on the curve exceed that line.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/Convex.png" class="img-fluid figure-img"></p>
<figcaption>Convexity</figcaption>
</figure>
</div>
<p>In the diagram above, the non-convex curve has a bump in the middle that prevents us from drawing a line between <span class="math inline">a</span> and <span class="math inline">b</span> that is not exceeded by the curve (red shaded region). This is commonly expressed mathematically as Jensen’s inequality: <span class="math display"> f\left(\frac{a+b}{2}\right)\le\frac{f(a)+f(b)}{2} </span></p>
<p>When a function is non-convex, gradient descent can fail. It may get trapped on its path to the minimum value of the function, known as the global minimum. These points where it gets trapped are known as local minima. We can see all of this with a new toy model. In this case, we will use the quadratic function $ y=0.2x^4-4x^2-x $.</p>
<div id="26b6da13" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a non-convex toy loss function</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nc_toy_loss(x):</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.2</span><span class="op">*</span>x<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> x</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># the derivative of the non-convex toy loss function</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nc_toy_loss_derivative(x):</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.8</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">8</span><span class="op">*</span>x <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize random seed numpy</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">47</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co"># choose our starting point at random</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>x_hist <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Starting point:'</span>, x_hist[<span class="dv">0</span>])</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>x_hist, n_iter <span class="op">=</span> gd_run(x_init, d_loss<span class="op">=</span>nc_toy_loss_derivative, lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_loss(loss<span class="op">=</span>nc_toy_loss)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>plot_loss_gd(ax<span class="op">=</span>ax, loss<span class="op">=</span>nc_toy_loss, x_pts<span class="op">=</span>x_hist)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Non-convex toy loss function</span><span class="ch">\n</span><span class="st">Learning rate = 0.1</span><span class="ch">\n</span><span class="st">Iterations = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(n_iter))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting point: -3.8651152810635048</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>Text(0.5, 1.0, 'Non-convex toy loss function\nLearning rate = 0.1\nIterations = 7')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-20-output-3.png" width="596" height="485" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If we start our fitting with a negative value of <code>x</code>, we get stuck in a valley that is at a higher level than the global minimum. This demonstrates how valuable it is to know whether your loss function is convex. If it is convex, then gradient descent will find the global minimum if run for long enough.</p>
</section>
<section id="gradient-descent-for-the-logistic-function" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-for-the-logistic-function">Gradient descent for the logistic function</h2>
<p>Now that we have defined the loss of the logistic function (eq. 2), and the gradient descent process that optimizes over it to find the best combination of parameters, we can write our own function from scratch to fit a logistic model! But before we do that, let’s get a little bit more intuition about how the loss landscape for the logistic model behaves. The two parameters we are varying to fit the logistic function are <code>b</code> and <code>w</code>. We will systematically vary both of them over a range of values, and with each set of values we will calculate the loss. The result of this can be visualized with as an image plot. Our x and y-axes will be the parameters <code>b</code> and <code>w</code>, respectively, while the color at each point indicates the loss.</p>
<p>To get an intuition for how the loss function behaves, we will first explore how the number of samples influences the loss landscape? Since the loss function is the mean loss across all samples in our data set, varying the number or types of samples we include should impact it.</p>
<div id="ffe639b4" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a function that plots the loss landscape for the logistic model on our data</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss_landscape(ax<span class="op">=</span><span class="va">None</span>, sel_data<span class="op">=</span><span class="va">None</span>, loss<span class="op">=</span>cross_entropy_loss):</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parameters</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ----------</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax : matplotlib axis object, optional</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     Axis to plot on. If not provided, a new figure and axis will be created.</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sel_data : array-like, optional</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     Indices of the data points to use for plotting the loss landscape. If not provided,</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     all data points will be used.</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss : function, optional</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     Loss function to use. If not provided, cross-entropy loss will be used.</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Returns</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax : matplotlib axis object</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     Axis with the loss landscape plotted.</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss_grid : numpy array</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     Array of loss values for each combination of w and b values.</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sel_data <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>        sel_data <span class="op">=</span> np.arange(<span class="dv">0</span>, X.size)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create a grid of w and b values</span></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    w_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.2</span>, <span class="fl">1.2</span>, <span class="dv">100</span>)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    b_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">15.5</span>, <span class="fl">15.5</span>, <span class="dv">100</span>)</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>    loss_grid <span class="op">=</span> np.zeros((w_grid.size, b_grid.size))</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the loss for each combination of w and b values</span></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, w <span class="kw">in</span> <span class="bu">enumerate</span>(w_grid):</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j, b <span class="kw">in</span> <span class="bu">enumerate</span>(b_grid):</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>            loss_grid[i,j] <span class="op">=</span> loss(y[sel_data], logistic(X.squeeze()[sel_data], w, b))</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove exceptionally small value that won't render well       </span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>    small_inds <span class="op">=</span> loss_grid<span class="op">&lt;</span><span class="fl">10e-10</span></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    loss_grid[small_inds] <span class="op">=</span> <span class="fl">10e-10</span></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot the loss landscape</span></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>    im <span class="op">=</span> ax.imshow(loss_grid, extent<span class="op">=</span>[b_grid[<span class="dv">0</span>], b_grid[<span class="op">-</span><span class="dv">1</span>], w_grid[<span class="dv">0</span>], w_grid[<span class="op">-</span><span class="dv">1</span>]],</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>               norm<span class="op">=</span>colors.LogNorm(), aspect<span class="op">=</span><span class="st">'auto'</span>, origin<span class="op">=</span><span class="st">'lower'</span>)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'$b$'</span>)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'$w$'</span>)</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>    ax.grid()</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax, loss_grid</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a><span class="co"># randomize the order of the samples, since the data is ordered</span></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>sel_data <span class="op">=</span> np.random.choice(np.arange(<span class="dv">0</span>, X.size), size<span class="op">=</span>X.size, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss landscape for the logistic model on our data</span></span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a><span class="co"># using different numbers of samples</span></span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">3</span>))</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ind, count <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">20</span>, X.size]):</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>    _, loss_ce <span class="op">=</span> plot_loss_landscape(ax<span class="op">=</span>ax[ind], sel_data<span class="op">=</span>sel_data[:count])</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>    ax[ind].set_title(<span class="st">'n=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(count))</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Cross-entropy loss landscape varies with number of samples'</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>Text(0.5, 1.05, 'Cross-entropy loss landscape varies with number of samples')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-21-output-2.png" width="1142" height="306" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If we only specify one data point, then the loss landscape is a relatively simple surface that slopes down towards lower values (dark blue) as either <code>w</code> or <code>b</code> are increased. This is because only one of the samples has to be fit, so as long as the orientation and decision point of our logistic function is set to classify that one data point correctly, our fit will be perfect. When four samples are included, we now have a mixture of trial types. This constrains the logistic model because we cannot push <code>w</code> or <code>b</code> towards infinity and still expect perfect performance. A zone starts to emerge with loss minimal for larger values of <code>w</code> and negative values of <code>b</code>. Further increasing the number of samples to 20 and then all of them creates a well defined valley where loss is minimal.</p>
<p>Besides the number of samples, the loss landscape also depends on the <em>kinds</em> of samples one includes. If you only include trials of one type, then the loss function will show a minimum where the parameters <code>w</code> and <code>b</code> are only optimal for that type of trial. However, including another type of trial could dramatically alter the loss landscape. We can explore that here by keeping the number of Cue trials fixed and ramping up the number of No-Cue trials included in the loss calculation.</p>
<div id="2771476f" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss landscape for the logistic model on our data</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co"># using different numbers of samples</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">3</span>))</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ind, count <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">10</span>]):</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    idxs <span class="op">=</span> np.concatenate((np.arange(<span class="dv">20</span>), np.arange(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span>(count<span class="op">+</span><span class="dv">1</span>),<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    _, loss_ce <span class="op">=</span> plot_loss_landscape(ax<span class="op">=</span>ax[ind], sel_data<span class="op">=</span>idxs)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    ax[ind].set_title(<span class="st">'No-cue trials=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(count))</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Cross-entropy loss landscape varies with types of samples included'</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>Text(0.5, 1.05, 'Cross-entropy loss landscape varies with types of samples included')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-22-output-2.png" width="1142" height="306" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If no No-Cue trials are included, then the loss landscape has the simple appearance we saw when only a single sample from a Cue trial was used. This is because all the samples we included are Cue trials, and so if <code>w</code> and <code>b</code> are raised to large enough values then the decoder’s performance will be perfect. Including a single No-Cue trial strongly alters the landscape, causing a mountain to begin rising in the loss landscape for large positive values of <code>b</code>. Further increasing the number of No-Cue trials that are included swings this mountain further towards the left of the graph, giving rise to the thin diagonal blue line of minimal loss.</p>
<section id="determining-the-gradient-in-the-loss-landscape" class="level3">
<h3 class="anchored" data-anchor-id="determining-the-gradient-in-the-loss-landscape">Determining the gradient in the loss landscape</h3>
<p>So far we have mapped the loss landscape by doing a grid search over a range of <code>w</code> and <code>b</code> values. This is a tedious way to find the optimal value. If we want to use gradient descent to find the minimum, we need an equation that describes the gradient of the loss landscape at each pair of <code>w</code> and <code>b</code> values. But first, if gradient descent is going to find the global minimum of the loss landscape, we need to determine if the loss landscape is convex.</p>
<p>On its own, the logistic function is non-convex. This can be demonstrated simply by plotting the logistic function, <span class="math inline">\frac{1}{1+e^{-x}}</span>, and then drawing a line between x=-5 and 5, which the logistic function crosses.</p>
<div id="b9a5c131" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the logistic function</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>ax.plot(x, logistic(x, <span class="dv">1</span>, <span class="dv">0</span>), label<span class="op">=</span><span class="st">'logistic'</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$\sigma(x)$'</span>)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co"># draw a line between the points on the curve at x = -5 and 5</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [logistic(<span class="op">-</span><span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>), logistic(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>)], <span class="st">'r--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'convex violation'</span>)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Logistic function with violation of convexity'</span>)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-23-output-1.png" width="590" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>What if we passed the logistic function through the cross-entropy loss function? Assuming that the predicted value, <span class="math inline">\hat{y}</span>, is 1, we get:</p>
<p><span class="math display"> \begin{align}
    \notag loss &amp;= -ln(\frac{1}{1+e^{-x}})\\
    \notag &amp;= -ln(1) + ln(1+e^{-x})\\
    \end{align}
</span></p>
<p>We can imagine that as <span class="math inline">x \to \infty</span> the term <span class="math inline">e^{-x}</span> approaches 0, heading towards <span class="math inline">-ln(1)+ln(1)=0</span>. And, as <span class="math inline">x \to -\infty</span> the term <span class="math inline">ln(1+e^{-x})</span> becomes a straight line, since the <span class="math inline">1+</span> becomes negligible and <span class="math inline">ln(e^{-x})=-x</span>. Let’s do some plotting to confirm these intuitions.</p>
<div id="81bdafa2" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the cross-entropy of the logistic function</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>ax.plot(x, <span class="op">-</span>np.log(logistic(x, <span class="dv">1</span>, <span class="dv">0</span>)), label<span class="op">=</span><span class="st">'cross-entropy of logistic'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$\ln(\sigma(x))$'</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co"># draw a black dotted line for y=-x</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="dv">10</span>, <span class="op">-</span><span class="dv">10</span>], [<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>], <span class="st">':'</span>, label<span class="op">=</span><span class="st">'y=-x'</span>, color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="co"># draw a black dotted line for y=0</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>], [<span class="dv">0</span>, <span class="dv">0</span>], <span class="st">'k:'</span>, label<span class="op">=</span><span class="st">'y=0'</span>)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="co"># draw a line on the curve between the points -5 and 5</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="op">-</span>np.log(logistic(<span class="op">-</span><span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>)), <span class="op">-</span>np.log(logistic(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>))], <span class="st">'g--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'no convex violation'</span>)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">2.5</span>, <span class="dv">10</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-24-output-1.png" width="588" height="434" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It is readily apparent that the logistic function when passed through the cross-entropy function is made convex. But recall that the cross-entropy is the average response of our logistic function across all samples. What happens when we add many of these curves together? Fortunately, the addition of any two convex functions results in a new function that is also convex!</p>
<p>Moving on, to run gradient descent, we need to know the derivative of the cross-entropy loss function for the logistic model (eq. 2). For the logistic model, we have to calculate the derivative for our two parameters, <span class="math inline">b</span> and <span class="math inline">w</span>. If you are curious how to derive the gradient, check out the note on it. The intercept/bias term, <code>b</code>, is <span class="math inline">\frac{\partial{L}}{\partial{b}}=(\hat{y}-y)</span>, which is the difference between the predicted (probability of) and actual class. The intuition for this is that if an event occurs, <span class="math inline">y=1</span>, then the intercept will be decreased more if the predicted probability of that event was low. On the other hand, if the event did not occur, then the bias will be increased in proportion to the predicted probability. When the predicted probability agrees with the occurrence of the event then the bias does not change. For the slope/weight term, <code>w</code>, the gradient is <span class="math inline">\frac{\partial{L}}{\partial{w}}=(\hat{y}-y)x</span>, which is the difference between the predicted and actual class scaled by the factor <span class="math inline">x</span> that we are using to make our prediction. This has the same behavior as the intercept, except that the change’s proportion and sign is set by the value of <span class="math inline">x</span>. For instance, if <span class="math inline">x&lt;0</span> then the slope will be increased. The larger the magnitude of <span class="math inline">x</span>, the greater the change.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deriving the gradient for logistic regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can grind through the math to get these. To simplify, we will replace the logistic function with the lowercase sigma symbol: <span class="math inline">\sigma(x)=\frac{1}{1+e^{-x}}</span>.</p>
<p>The derivative with respect to <span class="math inline">w</span> is: <span class="math display"> \begin{align}
    \notag \frac{\partial{L}}{\partial{w}}&amp;=\frac{\partial}{\partial{w}}-(y\log(\sigma(wx+b))+(1-y)\log(1-\sigma(wx+b)))\\
    \notag &amp;=-\left(\left(\frac{y}{\sigma(wx+b)}\frac{\partial}{\partial{w}}\sigma(wx+b)\right)+\left(\frac{1-y}{1-\sigma(wx+b)}\frac{\partial}{\partial{w}}1-\sigma(wx+b)\right)\right) \\
    \notag &amp;=-\left(\left(\frac{y}{\sigma(wx+b)}\frac{\partial}{\partial{w}}\sigma(wx+b)\right)+\left(\frac{1-y}{1-\sigma(wx+b)}\frac{\partial}{\partial{w}}-\sigma(wx+b)\right)\right) \\
    \notag &amp;=-\left(\frac{y}{\sigma(wx+b)}-\frac{1-y}{1-\sigma(wx+b)}\right)\frac{\partial}{\partial{w}}\sigma(wx+b) \\
    \notag &amp;=-\left(\frac{y(1-\sigma(wx+b))}{\sigma(wx+b)(1-\sigma(wx+b))}-\frac{(1-y)\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\frac{\partial}{\partial{w}}\sigma(wx+b) \\
    \notag &amp;=-\left(\frac{y-y\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}-\frac{\sigma(wx+b)-y\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\frac{\partial}{\partial{w}}\sigma(wx+b) \\
    \notag &amp;=-\left(\frac{y-\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\frac{\partial}{\partial{w}}\sigma(wx+b) \\
    \notag &amp;=-\left(\frac{y-\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\sigma(wx+b)(1-\sigma(wx+b))\frac{\partial}{\partial{w}}(wx+b) \\
    \notag &amp;=-\left(\frac{y-\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\sigma(wx+b)(1-\sigma(wx+b))x \\
    \notag &amp;=-(y-\sigma(wx+b))x \\
    \notag &amp;= (\sigma(wx+b)-y)x \\
\end{align}
</span></p>
<p>And the derivative with respect to <span class="math inline">b</span> is: <span class="math display"> \begin{align}
    \notag \textcolor{lightgray}{\frac{\partial{L}}{\partial{b}}}&amp;\textcolor{lightgray}{=\frac{\partial}{\partial{b}}-(y\log(\sigma(wx+b))+(1-y)\log(1-\sigma(wx+b)))}\\
    \notag &amp;\textcolor{lightgray}{=-\left(\left(\frac{y}{\sigma(wx+b)}\frac{\partial}{\partial{b}}\sigma(wx+b)\right)+\left(\frac{1-y}{1-\sigma(wx+b)}\frac{\partial}{\partial{b}}1-\sigma(wx+b)\right)\right)} \\
    \notag &amp;\textcolor{lightgray}{=-\left(\left(\frac{y}{\sigma(wx+b)}\frac{\partial}{\partial{b}}\sigma(wx+b)\right)+\left(\frac{1-y}{1-\sigma(wx+b)}\frac{\partial}{\partial{b}}-\sigma(wx+b)\right)\right)} \\
    \notag &amp;\textcolor{lightgray}{=-\left(\frac{y}{\sigma(wx+b)}-\frac{1-y}{1-\sigma(wx+b)}\right)\frac{\partial}{\partial{b}}\sigma(wx+b)} \\
    \notag &amp;\textcolor{lightgray}{=-\left(\frac{y(1-\sigma(wx+b))}{\sigma(wx+b)(1-\sigma(wx+b))}-\frac{(1-y)\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\frac{\partial}{\partial{b}}\sigma(wx+b)} \\
    \notag &amp;\textcolor{lightgray}{=-\left(\frac{y-y\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}-\frac{\sigma(wx+b)-y\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\frac{\partial}{\partial{b}}\sigma(wx+b)} \\
    \notag &amp;\textcolor{lightgray}{=-\left(\frac{y-\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\frac{\partial}{\partial{b}}\sigma(wx+b)} \\
    \notag &amp;\textcolor{lightgray}{=-\left(\frac{y-\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\sigma(wx+b)(1-\sigma(wx+b))}\frac{\partial}{\partial{b}}(wx+b) \\
    \notag &amp;\textcolor{lightgray}{=-\left(\frac{y-\sigma(wx+b)}{\sigma(wx+b)(1-\sigma(wx+b))}\right)\sigma(wx+b)(1-\sigma(wx+b))} \cdot 1 \\
    \notag &amp;=-(y-\sigma(wx+b)) \\
    \notag &amp;= \sigma(wx+b)-y \\
\end{align}
</span></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Alternative loss functions
</div>
</div>
<div class="callout-body-container callout-body">
<p>We chose the cross entropy loss because of its beneficial properties when working with the Bernoulli probability distribution and logistic function. However, what if we had gone with another popular loss function, the <em>mean squared error</em> (MSE). MSE is the mean difference between our predicted and actual results squared. It is expressed mathematically as:</p>
<p><span class="math display"> MSE(y,\hat{y}) = \frac{1}{N}\sum_{i}^{N}(y-\hat{y})^2 </span></p>
<p>This error function is often used when fitting linear systems of equations. If we tried to to use it with our logistic model, what would its loss landscape look like?</p>
<p>Let’s see what happens when we apply the mean squared error loss to the logistic function.</p>
<div id="415fd540" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the mean squared error of the logistic function</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>ax.plot(x, np.power(<span class="dv">1</span><span class="op">-</span>logistic(x, <span class="dv">1</span>, <span class="dv">0</span>),<span class="dv">2</span>), label<span class="op">=</span><span class="st">'MSE of logisitic'</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$(1-\sigma(x))^2$'</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co"># draw a line between the points on the curve at x = -5 and 5</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], np.power(<span class="dv">1</span><span class="op">-</span>np.array([logistic(<span class="op">-</span><span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>), logistic(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>)]),<span class="dv">2</span>), <span class="st">'r--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'convex violation'</span>)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Logistic function with MSE shows violation of convexity'</span>)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-25-output-1.png" width="592" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The mean squared error does not transform the logistic function in such a manner as to remove the convexity. This is also apparent when we look at the loss landscape.</p>
<div id="fa746224" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># function for calculating mean squared error loss</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(y, y_hat):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate the mean squared error loss</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co">    y : array</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="co">        True labels</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co">    y_hat : array</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Predicted probabilities</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="co">    loss : float</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a><span class="co">        The mean squared error loss</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.mean((y<span class="op">-</span>y_hat)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss landscape for the logistic model on our data</span></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a><span class="co"># using different numbers of samples and the mean squared error loss</span></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">3</span>))</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ind, count <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">20</span>, x.size]):</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>    _, loss_mse <span class="op">=</span> plot_loss_landscape(ax<span class="op">=</span>ax[ind], sel_data<span class="op">=</span>sel_data[:count], loss<span class="op">=</span>mse_loss)</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>    ax[ind].set_title(<span class="st">'n=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(count))</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Mean squared error loss landscape varies with number of samples'</span>, y<span class="op">=</span><span class="fl">1.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>Text(0.5, 1.05, 'Mean squared error loss landscape varies with number of samples')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-26-output-2.png" width="1142" height="306" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>While the landscapes for the cross-entropy and MSE loss look broadly similar, there are important differences. Note that as the number of samples increases, the landscape has a faceted appearance, with straight lines running through it. These correspond to edges in the landscape that make it non-convex. This is best visualized by taking a slice of the landscape and comparing it with the same slice from one generated using the cross-entropy loss.</p>
<div id="b69c51bc" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>ax.plot(loss_mse[:,<span class="dv">60</span>])</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>ax_ce <span class="op">=</span> ax.twinx()</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>ax_ce.plot(loss_ce[:,<span class="dv">60</span>], color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'w'</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'MSE loss'</span>, color<span class="op">=</span><span class="st">'tab:blue'</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>ax.tick_params(axis<span class="op">=</span><span class="st">'y'</span>, labelcolor<span class="op">=</span><span class="st">'tab:blue'</span>)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>ax_ce.tick_params(axis<span class="op">=</span><span class="st">'y'</span>, labelcolor<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>ax_ce.set_ylabel(<span class="st">'Cross-entropy loss'</span>, color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Losses along $b=3.1$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>Text(0.5, 1.0, 'Losses along $b=3.1$')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-27-output-2.png" width="633" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice how the cross-entropy loss we use for fitting the logistic model is convex, while the mean squared error loss function is not. This means that gradient descent will work with the cross-entropy loss for a logistic model, but it would not consistently work with the mean squared error loss.</p>
</div>
</div>
</section>
</section>
<section id="creating-a-logistic-regression-class-from-scratch-with-numpy" class="level2">
<h2 class="anchored" data-anchor-id="creating-a-logistic-regression-class-from-scratch-with-numpy">Creating a logistic regression class from scratch with Numpy</h2>
<p>Now that we have all the background and theory under our belt, let’s create an object for fitting a logistic model. We will use the same standards as the scikit learn package when designing it.</p>
<div id="de7cda9b" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticModel():</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, w<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># w : float, optional</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Initial value for w</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># b : float, optional</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Initial value for b</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> w</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> b</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_hist <span class="op">=</span> [w]</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_hist <span class="op">=</span> [b]</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_hist <span class="op">=</span> []</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _logistic(<span class="va">self</span>, x):</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x : array-like</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Input data</span></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y : array-like</span></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Predicted probabilities</span></span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(<span class="va">self</span>.w<span class="op">*</span>x <span class="op">+</span> <span class="va">self</span>.b)))</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _derivative_w(<span class="va">self</span>, x, y, y_hat):</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x : array-like</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Input data</span></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y : array-like</span></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Target values</span></span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y_hat : array-like</span></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Predicted values</span></span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># derivative : float</span></span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Derivative of the loss with respect to w</span></span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean((y_hat <span class="op">-</span> y)<span class="op">*</span>x)</span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _derivative_b(<span class="va">self</span>, y, y_hat):</span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y : array-like</span></span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Target values</span></span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y_hat : array-like</span></span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Predicted values</span></span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># derivative : float</span></span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Derivative of the loss with respect to b</span></span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(y_hat <span class="op">-</span> y)</span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _cross_entropy_loss(<span class="va">self</span>, y, y_hat):</span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y : array-like</span></span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Target values</span></span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y_hat : array-like</span></span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Predicted values</span></span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss : float</span></span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Cross-entropy loss</span></span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a>        y_hat[y_hat<span class="op">==</span><span class="dv">0</span>] <span class="op">=</span> np.finfo(<span class="bu">float</span>).eps</span>
<span id="cb44-75"><a href="#cb44-75" aria-hidden="true" tabindex="-1"></a>        y_hat[y_hat<span class="op">==</span><span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>np.finfo(<span class="bu">float</span>).eps </span>
<span id="cb44-76"><a href="#cb44-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(<span class="op">-</span>(y<span class="op">*</span>np.log(y_hat)<span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span>np.log(<span class="dv">1</span><span class="op">-</span>y_hat)))</span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-78"><a href="#cb44-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, x, y, lr<span class="op">=</span><span class="fl">0.1</span>, tol<span class="op">=</span><span class="fl">0.01</span>, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb44-79"><a href="#cb44-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb44-80"><a href="#cb44-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb44-81"><a href="#cb44-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x : array-like</span></span>
<span id="cb44-82"><a href="#cb44-82" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Input data</span></span>
<span id="cb44-83"><a href="#cb44-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y : array-like</span></span>
<span id="cb44-84"><a href="#cb44-84" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Target values</span></span>
<span id="cb44-85"><a href="#cb44-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lr : float, optional</span></span>
<span id="cb44-86"><a href="#cb44-86" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Learning rate</span></span>
<span id="cb44-87"><a href="#cb44-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tol : float, optional</span></span>
<span id="cb44-88"><a href="#cb44-88" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Tolerance for stopping criterion</span></span>
<span id="cb44-89"><a href="#cb44-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max_iter : int, optional</span></span>
<span id="cb44-90"><a href="#cb44-90" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Maximum number of iterations</span></span>
<span id="cb44-91"><a href="#cb44-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-92"><a href="#cb44-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb44-93"><a href="#cb44-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb44-94"><a href="#cb44-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self : LogisticModel</span></span>
<span id="cb44-95"><a href="#cb44-95" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     The fitted model</span></span>
<span id="cb44-96"><a href="#cb44-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-97"><a href="#cb44-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize the history of w and b values</span></span>
<span id="cb44-98"><a href="#cb44-98" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_hist <span class="op">=</span> [<span class="va">self</span>.w]</span>
<span id="cb44-99"><a href="#cb44-99" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_hist <span class="op">=</span> [<span class="va">self</span>.b]</span>
<span id="cb44-100"><a href="#cb44-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-101"><a href="#cb44-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ensure y and x are formatted as column vectors</span></span>
<span id="cb44-102"><a href="#cb44-102" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb44-103"><a href="#cb44-103" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb44-104"><a href="#cb44-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-105"><a href="#cb44-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run gradient descent until the stopping criterion is met</span></span>
<span id="cb44-106"><a href="#cb44-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># or until we reach the maximum number of iterations</span></span>
<span id="cb44-107"><a href="#cb44-107" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb44-108"><a href="#cb44-108" aria-hidden="true" tabindex="-1"></a>            <span class="co"># predict y values using the current w and b values</span></span>
<span id="cb44-109"><a href="#cb44-109" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> <span class="va">self</span>._logistic(x)</span>
<span id="cb44-110"><a href="#cb44-110" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.loss_hist.append(<span class="va">self</span>._cross_entropy_loss(y, y_hat))</span>
<span id="cb44-111"><a href="#cb44-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-112"><a href="#cb44-112" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w <span class="op">-=</span> lr<span class="op">*</span><span class="va">self</span>._derivative_w(x, y, y_hat)</span>
<span id="cb44-113"><a href="#cb44-113" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.b <span class="op">-=</span> lr<span class="op">*</span><span class="va">self</span>._derivative_b(y, y_hat)</span>
<span id="cb44-114"><a href="#cb44-114" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb44-115"><a href="#cb44-115" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the new w and b values to the history</span></span>
<span id="cb44-116"><a href="#cb44-116" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w_hist.append(<span class="va">self</span>.w)</span>
<span id="cb44-117"><a href="#cb44-117" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.b_hist.append(<span class="va">self</span>.b)</span>
<span id="cb44-118"><a href="#cb44-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-119"><a href="#cb44-119" aria-hidden="true" tabindex="-1"></a>            <span class="co"># check if the stopping criterion is met</span></span>
<span id="cb44-120"><a href="#cb44-120" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if so, break out of the loop</span></span>
<span id="cb44-121"><a href="#cb44-121" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (i<span class="op">&gt;</span><span class="dv">0</span>) <span class="kw">and</span> (np.<span class="bu">abs</span>(<span class="va">self</span>.loss_hist[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> <span class="va">self</span>.loss_hist[<span class="op">-</span><span class="dv">2</span>]) <span class="op">&lt;</span> tol):</span>
<span id="cb44-122"><a href="#cb44-122" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb44-123"><a href="#cb44-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-124"><a href="#cb44-124" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb44-125"><a href="#cb44-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb44-126"><a href="#cb44-126" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb44-127"><a href="#cb44-127" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x : array-like</span></span>
<span id="cb44-128"><a href="#cb44-128" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Input data</span></span>
<span id="cb44-129"><a href="#cb44-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-130"><a href="#cb44-130" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb44-131"><a href="#cb44-131" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb44-132"><a href="#cb44-132" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y_hat : array-like</span></span>
<span id="cb44-133"><a href="#cb44-133" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Predicted probabilities</span></span>
<span id="cb44-134"><a href="#cb44-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-135"><a href="#cb44-135" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._logistic(x).ravel() <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="co"># ravel converts to 1D array</span></span>
<span id="cb44-136"><a href="#cb44-136" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-137"><a href="#cb44-137" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> score(<span class="va">self</span>, x, y):</span>
<span id="cb44-138"><a href="#cb44-138" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb44-139"><a href="#cb44-139" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb44-140"><a href="#cb44-140" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x : array-like</span></span>
<span id="cb44-141"><a href="#cb44-141" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Input data</span></span>
<span id="cb44-142"><a href="#cb44-142" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y : array-like</span></span>
<span id="cb44-143"><a href="#cb44-143" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Target values</span></span>
<span id="cb44-144"><a href="#cb44-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-145"><a href="#cb44-145" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb44-146"><a href="#cb44-146" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb44-147"><a href="#cb44-147" aria-hidden="true" tabindex="-1"></a>        <span class="co"># score : float</span></span>
<span id="cb44-148"><a href="#cb44-148" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Accuracy of the model</span></span>
<span id="cb44-149"><a href="#cb44-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-150"><a href="#cb44-150" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> <span class="va">self</span>.predict(x)</span>
<span id="cb44-151"><a href="#cb44-151" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(y_hat <span class="op">==</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our logistic model class has the same methods as those found in scikit learn, and so it can be used in a similar way. Let’s run it and compare its fitted parameters with the ones returned by the scikit learn LogisticRegression class.</p>
<div id="b0458684" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>lm_scratch <span class="op">=</span> LogisticModel()</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>lm_scratch.fit(X, y, lr<span class="op">=</span><span class="fl">0.01</span>, tol<span class="op">=</span><span class="fl">0.000001</span>, max_iter<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Our fitted model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(lm_scratch.w, lm_scratch.b))</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Scikit learn model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_sk, b_sk))</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Our fitted model score: </span><span class="sc">{:.2f}</span><span class="st">%'</span>.<span class="bu">format</span>(lm_scratch.score(X, y)<span class="op">*</span><span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Our fitted model parameters: w=0.07, b=-1.98
Scikit learn model parameters: w=0.05, b=-1.86
Our fitted model score: 87.04%</code></pre>
</div>
</div>
<p>If we compare the logistic functions, it is apparent that they have have a similar threshold. The slope of the one we fitted is tighter, and this is because our <code>w</code> parameter is larger (0.07 vs 0.05).</p>
<div id="751b0e4d" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>plot_logistic(X, y, lm_scratch.w, lm_scratch.b, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot sample points colored by their predicted class</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> (lm_scratch.predict(X)<span class="op">&gt;=</span><span class="fl">0.5</span>).ravel()</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].axvline(<span class="op">-</span>lm_scratch.b<span class="op">/</span>lm_scratch.w, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Our fitted model'</span>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>plot_logistic(X, y, w_sk, b_sk, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].axvline(<span class="op">-</span>b_sk<span class="op">/</span>w_sk, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Scikit learn fitted model'</span>)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>fig.supylabel(<span class="st">'$\hat</span><span class="sc">{y}</span><span class="st">$'</span>)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-30-output-1.png" width="663" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The parameters are pretty close, and the performance is comparable. Since we have access to the history of values for the <code>b</code> and <code>w</code> parameters during the iterations of gradient descent, we can see how they evolve during the fitting process.</p>
<div id="deb841c4" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>ax,_ <span class="op">=</span> plot_loss_landscape(sel_data<span class="op">=</span>np.arange(<span class="dv">0</span>, X.size))</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>ax.plot(lm_scratch.b_hist, lm_scratch.w_hist, <span class="st">'k'</span>, label<span class="op">=</span><span class="st">'GD path'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>ax.text(lm_scratch.b_hist[<span class="dv">0</span>], lm_scratch.w_hist[<span class="dv">0</span>], <span class="st">'Start'</span>, ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'top'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>ax.text(lm_scratch.b_hist[<span class="op">-</span><span class="dv">1</span>], lm_scratch.w_hist[<span class="op">-</span><span class="dv">1</span>], <span class="st">'End'</span>, ha<span class="op">=</span><span class="st">'left'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>ax.scatter(b_sk, w_sk, color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Scikit-learn fit'</span>)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.5</span>, <span class="dv">1</span>])</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.2</span>])</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">False</span>)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Loss landscape with gradient descent path'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8088/2917809726.py:2: UserWarning:

color is redundantly defined by the 'color' keyword argument and the fmt string "k" (-&gt; color=(0.0, 0.0, 0.0, 1)). The keyword argument will take precedence.
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>Text(0.5, 1.0, 'Loss landscape with gradient descent path')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-31-output-3.png" width="619" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking at the path taken, we can see there is some initial fluctuation before it starts to descend along the path towards the minimum. It does not reach it, but neither did the output of scikit-learn! There are ways to improve the gradient process by modifying the learning rate based on the decrease in the loss, or integrating information from the trajectories past into how we calculate its present one.</p>
<p>Another modification to the gradient descent algorithm is to make it operate on a different random subset of the data for each iteration. This is known as <em>stochastic gradient descent</em>. Since our loss function is the mean loss across all samples, it may be that only a small subset of the samples are required to get a good estimate of the mean. Indeed, looking back on how the loss landscape was affected by the number of samples, we saw that it changed little after 20 samples. This can make the algorithm more computationally efficient, since fewer predictions must be computed on each iteration. It can also lead to better generalization, since a few outlier observations will not skew each update of our model parameters.</p>
<p>To do this, we will add a new parameter to our fitting process, <em>batch size</em>. This specifies how many samples are to be used for each step of gradient descent.</p>
<div id="bf103e2d" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticModelSGD(LogisticModel):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, x, y, lr<span class="op">=</span><span class="fl">0.1</span>, tol<span class="op">=</span><span class="fl">0.01</span>, max_iter<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x : array-like</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Input data</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y : array-like</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Target values</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lr : float, optional</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Learning rate</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tol : float, optional</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Tolerance for stopping criterion</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max_iter : int, optional</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Maximum number of iterations</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_size : int, optional</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Number of samples to use for each step of gradient descent</span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self : LogisticModelSGD</span></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     The fitted model</span></span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize the history of w and b values</span></span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_hist <span class="op">=</span> [<span class="va">self</span>.w]</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_hist <span class="op">=</span> [<span class="va">self</span>.b]</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ensure y and x are formatted as column vectors</span></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run gradient descent until the stopping criterion is met</span></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># or until we reach the maximum number of iterations</span></span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># randomly select batch_size samples</span></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>            sel_data <span class="op">=</span> np.random.choice(np.arange(<span class="dv">0</span>, x.size), size<span class="op">=</span>batch_size, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>            <span class="co"># predict y values using the current w and b values</span></span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> <span class="va">self</span>._logistic(x[sel_data])</span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.loss_hist.append(<span class="va">self</span>._cross_entropy_loss(y[sel_data], y_hat))</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w <span class="op">-=</span> lr<span class="op">*</span><span class="va">self</span>._derivative_w(x[sel_data], y[sel_data], y_hat)</span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.b <span class="op">-=</span> lr<span class="op">*</span><span class="va">self</span>._derivative_b(y[sel_data], y_hat)</span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the new w and b values to the history</span></span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w_hist.append(<span class="va">self</span>.w)</span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.b_hist.append(<span class="va">self</span>.b)</span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># check if the stopping criterion is met</span></span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if so, break out of the loop</span></span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (i<span class="op">&gt;</span><span class="dv">0</span>) <span class="kw">and</span> (np.<span class="bu">abs</span>(<span class="va">self</span>.loss_hist[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> <span class="va">self</span>.loss_hist[<span class="op">-</span><span class="dv">2</span>]) <span class="op">&lt;</span> tol):</span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Inheriting from our <code>LogisticModel</code> class, we modify the <code>fit</code> function to include a <code>batch_size</code> parameter. This sets the number of samples used. How does this version perform?</p>
<div id="a22dda11" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>lm_sgd <span class="op">=</span> LogisticModelSGD()</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>lm_sgd.fit(X, y, lr<span class="op">=</span><span class="fl">0.01</span>, tol<span class="op">=</span><span class="fl">0.000001</span>, max_iter<span class="op">=</span><span class="dv">2000</span>, batch_size<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Our fitted model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(lm_sgd.w, lm_sgd.b))</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Scikit learn model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_sk, b_sk))</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Our fitted model score: </span><span class="sc">{:.2f}</span><span class="st">%'</span>.<span class="bu">format</span>(lm_sgd.score(X, y)<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>ax,_ <span class="op">=</span> plot_loss_landscape(sel_data<span class="op">=</span>np.arange(<span class="dv">0</span>, X.size))</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>ax.plot(lm_sgd.b_hist, lm_sgd.w_hist, <span class="st">'k'</span>, label<span class="op">=</span><span class="st">'GD path'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>ax.text(lm_sgd.b_hist[<span class="dv">0</span>], lm_sgd.w_hist[<span class="dv">0</span>], <span class="st">'Start'</span>, ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'top'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>ax.text(lm_sgd.b_hist[<span class="op">-</span><span class="dv">1</span>], lm_sgd.w_hist[<span class="op">-</span><span class="dv">1</span>], <span class="st">'End'</span>, ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>ax.scatter(b_sk, w_sk, color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Scikit-learn fit'</span>)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.5</span>, <span class="dv">1</span>])</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.2</span>])</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">False</span>)</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Loss landscape with gradient descent path'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Our fitted model parameters: w=0.07, b=-2.02
Scikit learn model parameters: w=0.05, b=-1.86
Our fitted model score: 85.19%</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8088/4246139248.py:8: UserWarning:

color is redundantly defined by the 'color' keyword argument and the fmt string "k" (-&gt; color=(0.0, 0.0, 0.0, 1)). The keyword argument will take precedence.
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>Text(0.5, 1.0, 'Loss landscape with gradient descent path')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-33-output-4.png" width="619" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Using stochastic gradient descent yields comparable performance, and with fewer samples processed per iteration. However, the path it takes is far less smooth, zigging and zagging towards the minimum. To minimize this jitter a momentum term is often included. There are several different ways to implement momentum, and we will use the one that treats it as a moving average of the gradient over successive iterations. The gamma parameter controls the mixture of the currently calculated gradient and the running average of its past values. For <code>w</code> (it will be the same for <code>b</code>), we can write this as:</p>
<p><span class="math display"> \begin{align}
    dw &amp;= \gamma dw + (1-\gamma)\frac{\partial{L}}{\partial{w}} \notag \\
    w &amp;= w - \eta dw \notag \\
    \end{align}
</span></p>
<p>Here the <span class="math inline">\gamma</span> (gamma) parameter sets how much the past change in value of the model parameter contributes to its current update. <span class="math inline">\gamma</span> ranges from 0 to 1, with 0 making the update equivalent to normal gradient descent, and values close to 1 making the update almost wholly dependent on its previous value. Note that <span class="math inline">\gamma=1</span> would result in no descent since the gradient would never update. A physical intuition for <span class="math inline">\gamma</span> is that increasing it makes the path taken by gradient descent more ‘sticky’, less likely to be swayed from batch to batch.</p>
<div id="790467be" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic regression model with momentum added to the gradient descent step</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticModelSGD_Mom(LogisticModel):</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, x, y, lr<span class="op">=</span><span class="fl">0.1</span>, tol<span class="op">=</span><span class="fl">0.01</span>, max_iter<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span><span class="fl">0.9</span>):</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x : array-like</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Input data</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y : array-like</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Target values</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lr : float, optional</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Learning rate</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tol : float, optional</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Tolerance for stopping criterion</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max_iter : int, optional</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Maximum number of iterations</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_size : int, optional</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Number of samples to use for each step of gradient descent</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># gamma : float, optional</span></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Momentum parameter</span></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self : LogisticModelSGD</span></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     The fitted model</span></span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize the history of w and b values</span></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_hist <span class="op">=</span> [<span class="va">self</span>.w]</span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_hist <span class="op">=</span> [<span class="va">self</span>.b]</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ensure y and x are formatted as column vectors</span></span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize the momentum</span></span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>        mw <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>        mb <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run gradient descent until the stopping criterion is met</span></span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># or until we reach the maximum number of iterations</span></span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># randomly select batch_size samples</span></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a>            sel_data <span class="op">=</span> np.random.choice(np.arange(<span class="dv">0</span>, x.size), size<span class="op">=</span>batch_size, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># predict y values using the current w and b values</span></span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> <span class="va">self</span>._logistic(x[sel_data])</span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.loss_hist.append(<span class="va">self</span>._cross_entropy_loss(y[sel_data], y_hat))</span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># calculate the gradient</span></span>
<span id="cb56-49"><a href="#cb56-49" aria-hidden="true" tabindex="-1"></a>            dw <span class="op">=</span> <span class="va">self</span>._derivative_w(x[sel_data], y[sel_data], y_hat)</span>
<span id="cb56-50"><a href="#cb56-50" aria-hidden="true" tabindex="-1"></a>            db <span class="op">=</span> <span class="va">self</span>._derivative_b(y[sel_data], y_hat)</span>
<span id="cb56-51"><a href="#cb56-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-52"><a href="#cb56-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update the momentum</span></span>
<span id="cb56-53"><a href="#cb56-53" aria-hidden="true" tabindex="-1"></a>            mw <span class="op">=</span> gamma<span class="op">*</span>mw <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>gamma)<span class="op">*</span>dw</span>
<span id="cb56-54"><a href="#cb56-54" aria-hidden="true" tabindex="-1"></a>            mb <span class="op">=</span> gamma<span class="op">*</span>mb <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>gamma)<span class="op">*</span>db</span>
<span id="cb56-55"><a href="#cb56-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-56"><a href="#cb56-56" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update the weights and bias</span></span>
<span id="cb56-57"><a href="#cb56-57" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w <span class="op">-=</span> lr<span class="op">*</span>mw</span>
<span id="cb56-58"><a href="#cb56-58" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.b <span class="op">-=</span> lr<span class="op">*</span>mb</span>
<span id="cb56-59"><a href="#cb56-59" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-60"><a href="#cb56-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the new w and b values to the history</span></span>
<span id="cb56-61"><a href="#cb56-61" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w_hist.append(<span class="va">self</span>.w)</span>
<span id="cb56-62"><a href="#cb56-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.b_hist.append(<span class="va">self</span>.b)</span>
<span id="cb56-63"><a href="#cb56-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-64"><a href="#cb56-64" aria-hidden="true" tabindex="-1"></a>            <span class="co"># check if the stopping criterion is met</span></span>
<span id="cb56-65"><a href="#cb56-65" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if so, break out of the loop</span></span>
<span id="cb56-66"><a href="#cb56-66" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (i<span class="op">&gt;</span><span class="dv">0</span>) <span class="kw">and</span> (np.<span class="bu">abs</span>(<span class="va">self</span>.loss_hist[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> <span class="va">self</span>.loss_hist[<span class="op">-</span><span class="dv">2</span>]) <span class="op">&lt;</span> tol):</span>
<span id="cb56-67"><a href="#cb56-67" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s run it and see how it performs!</p>
<div id="605efc87" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>lm_sgd <span class="op">=</span> LogisticModelSGD_Mom()</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>lm_sgd.fit(X, y, lr<span class="op">=</span><span class="fl">0.015</span>, tol<span class="op">=</span><span class="fl">0.000001</span>, max_iter<span class="op">=</span><span class="dv">2000</span>, batch_size<span class="op">=</span><span class="dv">30</span>, gamma<span class="op">=</span><span class="fl">0.99</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Our fitted model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(lm_sgd.w, lm_sgd.b))</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Scikit learn model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_sk, b_sk))</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Our fitted model score: </span><span class="sc">{:.2f}</span><span class="st">%'</span>.<span class="bu">format</span>(lm_sgd.score(X, y)<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>ax,_ <span class="op">=</span> plot_loss_landscape(sel_data<span class="op">=</span>np.arange(<span class="dv">0</span>, X.size))</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>ax.plot(lm_sgd.b_hist, lm_sgd.w_hist, <span class="st">'k'</span>, label<span class="op">=</span><span class="st">'GD path'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>ax.text(lm_sgd.b_hist[<span class="dv">0</span>], lm_sgd.w_hist[<span class="dv">0</span>], <span class="st">'Start'</span>, ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'top'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>ax.text(lm_sgd.b_hist[<span class="op">-</span><span class="dv">1</span>], lm_sgd.w_hist[<span class="op">-</span><span class="dv">1</span>], <span class="st">'End'</span>, ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>ax.scatter(b_sk, w_sk, color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Scikit-learn fit'</span>)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.5</span>, <span class="dv">1</span>])</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.2</span>])</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">False</span>)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Loss landscape with gradient descent path'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Our fitted model parameters: w=0.07, b=-2.49
Scikit learn model parameters: w=0.05, b=-1.86
Our fitted model score: 87.04%</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_8088/2917554854.py:8: UserWarning:

color is redundantly defined by the 'color' keyword argument and the fmt string "k" (-&gt; color=(0.0, 0.0, 0.0, 1)). The keyword argument will take precedence.
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>Text(0.5, 1.0, 'Loss landscape with gradient descent path')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-35-output-4.png" width="619" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Still a ragged path, but certainly smoother than when we did not include momentum. Note also that this is despite having also increased the learning rate term, which generally makes the gradient descent path less stable.</p>
</section>
<section id="building-the-logistic-regression-model-with-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="building-the-logistic-regression-model-with-pytorch">Building the logistic regression model with PyTorch</h2>
<p>Our scratch built logistic model has given us a good understanding of the basic underlying math and algorithms, but it is not how such models are often built. If you want to create a custom decoder, it better to build it from a package of general purpose functions. You could imagine that some functions would handle the chunking of the data, others the operations and transforms applied by the models, and another for the optimization. One such python package that offers all these features is PyTorch.</p>
<p>PyTorch is widely used for the construction of machine learning (especially deep neural network) models. It has a native data object, a <code>Tensor</code>, which is a multidimensional array. Also featured are a collection of standard linear algebra and mathematical operations that can operate on tensors. The parameters of these operations or values within the tensors can be updated with a variety of optimization functions featuring different forms of gradient descent.</p>
<p>Most importantly, as we progress in our use of logistic models to more complex neural decoding problems you may want to use PyTorch. It allows you to program your decoders at a higher level of abstraction (e.g., you don’t have to write your own gradient descent algorithm), which saves time.</p>
<p>To start, let’s create a <code>Dataset</code> object that will contain the neural measures and trial labels. <code>Dataset</code> is a class built in to PyTorch that gives it access to the data we want to train on. Crucially, it is useful when constructing batches of data for running stochastic gradient descent.</p>
<p>There are three methods we must specify when creating a custom <code>Dataset</code> class. The <code>__init__</code> method is used to initialize the object. Its input parameters allow us to specify the paths or variables where predictors and labels are stored, and transforms we want to apply to those before passing on to the decoder. Seperate transform functions can be applied to the predictors and labels.</p>
<p>We then specify the <code>__len__</code> function, whose role is simply to indicate how many samples are part of the dataset.</p>
<p>Finally, we create a <code>__getitem__</code> method, which gives access to samples in the dataset. An index parameter is passed to this method that specifies the sample to return, and it in turn transforms and returns the corresponding predictor and labels.</p>
<div id="aeed9771" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ERPData(Dataset):</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, erp_pred, trial_lbl, transform<span class="op">=</span><span class="va">None</span>, target_transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># erp_pred : array-like</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     ERP data</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># trial_lbl : array-like</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Trial labels</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># transform : callable, optional</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Optional transform to be applied to the ERP data</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># target_transform : callable, optional</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Optional transform to be applied to the trial labels</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.erp_pred <span class="op">=</span> erp_pred</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.trial_lbl <span class="op">=</span> trial_lbl</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_transform <span class="op">=</span> target_transform</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># len : int</span></span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Number of samples in the dataset</span></span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.trial_lbl)</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters</span></span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----------</span></span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># idx : int</span></span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Index of the sample to return</span></span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -------</span></span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># erp : array-like</span></span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     ERP data for the selected sample</span></span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lbl : array-like</span></span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     Trial label for the selected sample</span></span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a>        erp <span class="op">=</span> <span class="va">self</span>.erp_pred[idx, np.newaxis] <span class="co"># get the ERP data for the selected sample</span></span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a>        lbl <span class="op">=</span> <span class="va">self</span>.trial_lbl[idx, np.newaxis] <span class="co"># get the trial label for the selected sample</span></span>
<span id="cb61-42"><a href="#cb61-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: <span class="co"># apply the transform to the ERP data</span></span>
<span id="cb61-43"><a href="#cb61-43" aria-hidden="true" tabindex="-1"></a>            erp <span class="op">=</span> <span class="va">self</span>.transform(erp)</span>
<span id="cb61-44"><a href="#cb61-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.target_transform <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: <span class="co"># apply the transform to the trial label</span></span>
<span id="cb61-45"><a href="#cb61-45" aria-hidden="true" tabindex="-1"></a>            lbl <span class="op">=</span> <span class="va">self</span>.target_transform(lbl)</span>
<span id="cb61-46"><a href="#cb61-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> erp, lbl</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s give it a try!</p>
<div id="c469bc02" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>erp_ds <span class="op">=</span> ERPData(X, y)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print the number of samples in the dataset</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of samples in the dataset: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="bu">len</span>(erp_ds)))</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="co"># print the first sample in the dataset</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>erp, lbl <span class="op">=</span> erp_ds[<span class="dv">0</span>]</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ERP data for the first sample: </span><span class="sc">{}</span><span class="st">, </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(erp, lbl))</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print just the erp for the first sample in the dataset</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>erp <span class="op">=</span> erp_ds[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ERP data for the first sample: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(erp))</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a><span class="co"># print just the label for the first sample in the dataset</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>lbl <span class="op">=</span> erp_ds[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Label for the first sample: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(lbl))</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a><span class="co"># print the first 10 samples in the dataset</span></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>erp, lbl <span class="op">=</span> erp_ds[:<span class="dv">10</span>]</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ERP data for the first 10 samples:'</span>)</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> samp <span class="kw">in</span> <span class="bu">zip</span>(erp, lbl):</span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{}</span><span class="st">, </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(samp[<span class="dv">0</span>], samp[<span class="dv">1</span>]))</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a><span class="co"># print the last sample in the dataset</span></span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>erp, lbl <span class="op">=</span> erp_ds[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ERP data for the last sample: </span><span class="sc">{}</span><span class="st">, </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(erp,lbl))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of samples in the dataset: 54
ERP data for the first sample: [47.8682191], [ True]
ERP data for the first sample: [47.8682191]
Label for the first sample: [ True]
ERP data for the first 10 samples:
[47.8682191], [ True]
[79.21943476], [ True]
[59.77661572], [ True]
[67.42808345], [ True]
[106.59886022], [ True]
[45.76555801], [ True]
[103.45205398], [ True]
[56.59460022], [ True]
[60.77195205], [ True]
[111.57862896], [ True]
ERP data for the last sample: [-53.37683445], [False]</code></pre>
</div>
</div>
<p>Once we have created a <code>Dataset</code> object for our ERP data, we can pass it to the PyTorch <code>DataLoader</code> object. It partitions the data into batches that can be used for stochastic gradient descent later in the training.</p>
<div id="210b435b" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataLoader object</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>erp_dl <span class="op">=</span> DataLoader(erp_ds, batch_size<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a batch of data</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>erps, lbls <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(erp_dl))</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print the dimensions of the erps and label tensors</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ERP data dimensions: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(erps.shape))</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Label dimensions: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(lbls.shape))</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ERP data for the first batch:'</span>)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> samp <span class="kw">in</span> <span class="bu">zip</span>(erps, lbls):</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{}</span><span class="st">, </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(samp[<span class="dv">0</span>], samp[<span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ERP data dimensions: torch.Size([5, 1])
Label dimensions: torch.Size([5, 1])
ERP data for the first batch:
tensor([41.7905], dtype=torch.float64), tensor([True])
tensor([34.6684], dtype=torch.float64), tensor([False])
tensor([52.2659], dtype=torch.float64), tensor([True])
tensor([26.0798], dtype=torch.float64), tensor([True])
tensor([67.4281], dtype=torch.float64), tensor([True])</code></pre>
</div>
</div>
<p>Here we created a <code>DataLoader</code> object and customized its behavior. The <code>batch_size</code> argument specified how many samples go into each batch, in this case 5. Using the <code>shuffle</code> argument set it up so that samples were drawn at random, instead of in the order they are natively stored. This is needed to ensure our samples are a mixture of trial types when evaluating the loss gradient for each batch.</p>
<p>When samples are returned by the <code>DataLoader</code>, they come as the <code>tensor</code> data type. A tensor is a multidimensional array, similar to the <code>array</code> data type in Numpy. Compared with python data types for storing numeric data, such as lists, tensors offer several advantages. These are largely on the low-level backend side. They take up less memory by storing numbers with <em>C</em> data types instead of as python objects (a python number is an object with lots of metadata associated with it that fills up memory). They are faster because they are stored in memory as contiguous blocks, which can be read and written to faster. And equally important, they can be stored or processed on graphical processing units (GPUs) that dramatically speed up the computations for optimization and prediction.</p>
<p>The tensor returned by the <code>DataLoader</code> has a shape of 5x1. The first dimension is the number of samples in the batch, and each subsequent dimension corresponds to the number of features we want to decode with (often referred to as the dimensionality) In the case of the ERP fit, that is only a single number, so the dimension is 1. If we were training an algorithm to classify images, hypothetically, it might have the shape 5x3x256x256, where the dimensions correspond to <em>sample</em> x <em>channel</em> x <em>height</em> x <em>width</em>.</p>
<p>The <code>DataLoader</code> can be used in a for loop to return multiple batches of samples. To do this, use the <code>enumerate</code> function built in to Python that repeatedly calls an objects <code>__getitem__</code> method until the number of samples returned equals the total number of samples in the <code>Dataset</code> referenced by <code>DataLoader</code>.</p>
<div id="f72f92ae" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (erp, lbl) <span class="kw">in</span> <span class="bu">enumerate</span>(erp_dl):</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Batch </span><span class="sc">{}</span><span class="st">: erp shape = </span><span class="sc">{}</span><span class="st">, lbl shape = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(i, erp.shape, lbl.shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batch 0: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 1: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 2: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 3: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 4: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 5: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 6: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 7: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 8: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 9: erp shape = torch.Size([5, 1]), lbl shape = torch.Size([5, 1])
Batch 10: erp shape = torch.Size([4, 1]), lbl shape = torch.Size([4, 1])</code></pre>
</div>
</div>
<p>We have a total of 54 samples in the dataset <code>erp_ds</code>, and you can see that a total of 54 samples are returned when we call <code>erp_dl</code> using enumerate in a for loop. We specified that the <code>DataLoader</code> object should return a batch with 5 samples, and 10 the returned batches had 5 samples. The last batch only has 4 samples because that was the remainder (54 total samples - 5 samples per batch * 10 batches).</p>
<p>Now that we have our data, we need to setup the logistic function. Recall that it has the form:</p>
<p><span class="math display"> p(Event|x)=\frac{1}{(1+e^{-(b+wx)})} </span></p>
<p>We can break it down into two parts. The first is a linear equation, <span class="math inline">b+wx</span>, which scales (with <span class="math inline">w</span>) and shifts (<span class="math inline">b</span>) our ERP value. This rescaled ERP value is then passed to the logistic function, <span class="math inline">\frac{1}{1+e^{-z}}</span>, converts it to a probability between 0 and 1. Note that <span class="math inline">z=b+wx</span>.</p>
<p>Pytorch offers objects to handle both of these functions. The linear equation can be found in <code>torch.nn.linear</code>. The ‘nn’ stands for <em>neural network</em>. Many neural networks make use of a linear transform layer, so that is implemented as part of that module. This object implements a linear equation, essentially a matrix that can be multiplied by an incoming vector. It has the form:</p>
<p><span class="math display"> z_j = b_j+\sum_{i=0}^{n}w_{i,j}x_i </span> where n is the number of inputs, <span class="math inline">x_i</span>, each is multiplied by a distinct weight, <span class="math inline">w_{i,j}</span>. These are summed together and a bias, <span class="math inline">b_j</span>, is added to give an output <span class="math inline">z_j</span>. Since our logistic model has only one input and output, the equation simplifies to:</p>
<p><span class="math display"> z = b + wx </span> To implement this using PyTorch we code:</p>
<div id="eaaad077" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a linear transformation with 1 input, 1 output, and a bias</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>lin_trans <span class="op">=</span> torch.nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first argument of <code>Linear</code> is the number of inputs, while the second is the number of outputs. Settings the <code>bias</code> argument to <code>True</code> adds the bias term, <span class="math inline">b</span>. When the <code>Linear</code> layer is initialized, its weight (<span class="math inline">w</span>) and bias are given random values. You can directly inspect them as properties of the <code>Linear</code> object you created.</p>
<div id="e1bc3518" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print the weight and bias values</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Linear layer weight value is: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(lin_trans.weight))</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Linear layer bias value is: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(lin_trans.bias))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear layer weight value is: Parameter containing:
tensor([[-0.2243]], requires_grad=True)
Linear layer bias value is: Parameter containing:
tensor([0.8938], requires_grad=True)</code></pre>
</div>
</div>
<p>In addition, you can pass a value to the <code>Linear</code> object as if it were a function, with it returning the transformed value. However, make sure the value you pass to it is a tensor, since that is the datatype it expects.</p>
<div id="8cc21eda" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>t0 <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>]) <span class="co"># create a tensor with a single value of 0</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>]) <span class="co"># create a tensor with a single value of 1</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Value of lin_trans at 0: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(lin_trans(t0)))</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Value of lin_trans at 1: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(lin_trans(t1)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Value of lin_trans at 0: tensor([0.8938], grad_fn=&lt;ViewBackward0&gt;)
Value of lin_trans at 1: tensor([0.6694], grad_fn=&lt;ViewBackward0&gt;)</code></pre>
</div>
</div>
<p>If we want to plot the output of <code>lin_trans</code>, things get a little bit more complicated. First we have to create a tensor of <code>x</code> values.</p>
<div id="05d6f7a4" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create numpy array with 100 values between -10 and 10</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="co"># cast the array to float32 data type for PyTorch compatibility</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>).astype(np.float32)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Data type and shape of x: </span><span class="sc">{}</span><span class="st">, </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(x.dtype, x.shape))</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="co"># convert numpy array to a PyTorch tensor</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>x_t <span class="op">=</span> torch.tensor(x).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Data type and shape of x_t: </span><span class="sc">{}</span><span class="st">, </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(x_t.dtype, x_t.shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data type and shape of x: float32, (100,)
Data type and shape of x_t: torch.float32, torch.Size([100, 1])</code></pre>
</div>
</div>
<p>A few things to note here. We assign the numeric data type to be <code>np.float32</code>, which stands for floating point 32-bit. This is the default numeric data type used by tensors in Pytorch. If we don’t assign that, we may get an error due to our bias and weights being of incompatible data types from our inputs. We also have to convert the numpy array <code>x</code> to a tensor, which is done using the <code>torch.tensor</code> function. In addition, it needs to be reshaped to ensure that the first dimension corresponds to the samples (100 in this case), and the second dimension to the number of features (1).</p>
<p>Now that we have a properly formatted input <code>x_t</code>, we can pass it to <code>lin_trans</code>.</p>
<div id="122e3425" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> lin_trans(x_t)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Data type and shape of y: </span><span class="sc">{}</span><span class="st">, </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(y_t.dtype, y_t.shape))</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'y: </span><span class="ch">\n</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(y_t))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data type and shape of y: torch.float32, torch.Size([100, 1])
y: 
tensor([[ 3.1372],
        [ 3.0919],
        [ 3.0466],
        [ 3.0012],
        [ 2.9559],
        [ 2.9106],
        [ 2.8653],
        [ 2.8200],
        [ 2.7746],
        [ 2.7293],
        [ 2.6840],
        [ 2.6387],
        [ 2.5933],
        [ 2.5480],
        [ 2.5027],
        [ 2.4574],
        [ 2.4121],
        [ 2.3667],
        [ 2.3214],
        [ 2.2761],
        [ 2.2308],
        [ 2.1855],
        [ 2.1401],
        [ 2.0948],
        [ 2.0495],
        [ 2.0042],
        [ 1.9588],
        [ 1.9135],
        [ 1.8682],
        [ 1.8229],
        [ 1.7776],
        [ 1.7322],
        [ 1.6869],
        [ 1.6416],
        [ 1.5963],
        [ 1.5510],
        [ 1.5056],
        [ 1.4603],
        [ 1.4150],
        [ 1.3697],
        [ 1.3243],
        [ 1.2790],
        [ 1.2337],
        [ 1.1884],
        [ 1.1431],
        [ 1.0977],
        [ 1.0524],
        [ 1.0071],
        [ 0.9618],
        [ 0.9165],
        [ 0.8711],
        [ 0.8258],
        [ 0.7805],
        [ 0.7352],
        [ 0.6898],
        [ 0.6445],
        [ 0.5992],
        [ 0.5539],
        [ 0.5086],
        [ 0.4632],
        [ 0.4179],
        [ 0.3726],
        [ 0.3273],
        [ 0.2819],
        [ 0.2366],
        [ 0.1913],
        [ 0.1460],
        [ 0.1007],
        [ 0.0553],
        [ 0.0100],
        [-0.0353],
        [-0.0806],
        [-0.1259],
        [-0.1713],
        [-0.2166],
        [-0.2619],
        [-0.3072],
        [-0.3526],
        [-0.3979],
        [-0.4432],
        [-0.4885],
        [-0.5338],
        [-0.5792],
        [-0.6245],
        [-0.6698],
        [-0.7151],
        [-0.7604],
        [-0.8058],
        [-0.8511],
        [-0.8964],
        [-0.9417],
        [-0.9871],
        [-1.0324],
        [-1.0777],
        [-1.1230],
        [-1.1683],
        [-1.2137],
        [-1.2590],
        [-1.3043],
        [-1.3496]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p><code>lin_trans</code> returns another tensor, which cannot be directly passed to matplotlib. The tensor object has a gradient associated with it, <code>grad_fn=&lt;AddmmBackward0&gt;</code>, that must be removed. In addition, we could convert the numeric array inside the tensor object into a numpy array (use <code>.numpy()</code> to do that), but matplotlib will accept a tensor without the gradient as a valid input, so we can forgo that.</p>
<div id="7ff734e9" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Here is the first five values of tensor y_t with the gradient removed: </span><span class="ch">\n</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(y_t[:<span class="dv">5</span>].detach()))</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="co"># remove the gradient and convert tensor to numpy array</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>y_dt <span class="op">=</span> y_t.detach()</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Data type and shape of y_dt: </span><span class="sc">{}</span><span class="st">, </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="bu">type</span>(y_dt), y_dt.shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Here is the first five values of tensor y_t with the gradient removed: 
tensor([[3.1372],
        [3.0919],
        [3.0466],
        [3.0012],
        [2.9559]])
Data type and shape of y_dt: &lt;class 'torch.Tensor'&gt;, torch.Size([100, 1])</code></pre>
</div>
</div>
<p>Now we are ready to plot!</p>
<div id="27263d45" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the linear transformation</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>ax.plot(x_t, y_dt)</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Linear transformation'</span>)</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>ax.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-46-output-1.png" width="587" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now that we have implemented the linear function <span class="math inline">b+wx</span>, we have to create the logistic function, <span class="math inline">\frac{1}{1+e^{-z}}</span>. PyTorch has this function built in under the name <code>sigmoid</code>, which is just another name for the logistic function.</p>
<div id="ab9fb364" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>log_trans <span class="op">=</span> torch.nn.Sigmoid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can plot the sigmoid function to make sure it has the same profile as the logistic function we have been working with.</p>
<div id="260b69d7" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the sigmoid transformation</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>x_t <span class="op">=</span> torch.tensor(np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>).astype(np.float32)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>ax.plot(x_t, log_trans(x_t).detach())</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$\sigma(x)$'</span>)</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Sigmoid transformation'</span>)</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>ax.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-48-output-1.png" width="590" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As with the linear transform, we had to format our Numpy array into a tensor that could be passed to the <code>Sigmoid</code> function, and had to then remove the gradient from the tensor returned by <code>Sigmoid</code> to plot it with matplotlib.</p>
<p>Now we need to chain the linear and sigmoid steps together, to create the logistic model. Pytorch provides the <code>nn.Sequential</code> object for doing this. With it, you pass in order the transforms and functions you want to chain together, and it returns a single object that encapsulates that sequence.</p>
<div id="f3b98efd" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>log_mdl <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    lin_trans,</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>    log_trans</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="co"># print the model</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(log_mdl)</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print the model parameters</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Model parameters:</span><span class="ch">\n</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="bu">dict</span>(log_mdl.named_parameters())))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sequential(
  (0): Linear(in_features=1, out_features=1, bias=True)
  (1): Sigmoid()
)
Model parameters:
{'0.weight': Parameter containing:
tensor([[-0.2243]], requires_grad=True), '0.bias': Parameter containing:
tensor([0.8938], requires_grad=True)}</code></pre>
</div>
</div>
<p>We can pass values to the <code>Sequential</code> object just like we did to the <code>Linear</code> and <code>Sigmoid</code> functions. For instance, if we wanted to plot it we would:</p>
<div id="57a218ae" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>y_t <span class="op">=</span> log_mdl(x_t)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>ax.plot(x_t, y_t.detach())</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$\hat</span><span class="sc">{y}</span><span class="vs">$'</span>)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Logistic model'</span>)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>ax.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-50-output-1.png" width="594" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looks like a logistic model. Since the weights and bias were set randomly, this model will not necessarily perform well on our data set. We have to optimize those model parameters using the cross-entropy loss and gradient descent. PyTorch features functions and objects for handling both of those.</p>
<p>PyTorch has a suite of loss functions to choose from. You may be tempted to use its <code>CrossEntropyLoss</code> function, since that is what we used for our scratch built code above. However, in PyTorch the <code>CrossEntropyLoss</code> function is designed for measuring loss when multiple categories of events are being decoded (more than 1), and so it would not be appropriate for our binary case, which is just detecting whether or not a cue was present. Instead, we will use the <code>BCELoss</code> function, which stands for <em>binary cross entropy loss</em>. This is used when only a single outcome is being predicted, and if you refer to the <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.htmld">PyTorch documentation</a> for it, you will see that it is the same equation for cross-entropy loss used above.</p>
<div id="25c55a10" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.BCELoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We also have to create a function optimize the parameters based on the loss. To do this in our scratch built version, we hand coded stochastic gradient descent with momentum. PyTorch provides this built-in with their <code>optimizer</code> object. It implements a variety of gradient descent algorithms, including the one we used. To invoke it:</p>
<div id="b477215e" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(log_mdl.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To use it, we pass the parameters we want to update, and optionally the learning rate and momentum. An optimizer object features a <code>step</code> method, which updates the parameters. Before calling it, we also have to clear any previous gradients that had been calculated, or else they will keep accumulating and throw off the gradient descent process.</p>
<p>Now that we have the loss function and optimizer, run gradient descent. To do this, we use <code>log_mdl</code> to predict the labels based on the features and then pass them and the true labels to the loss function.</p>
<div id="c01eb2ce" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get a batch of data with the dataloader</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>erps, lbls <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(erp_dl))</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="co"># predict the labels for the batch</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> log_mdl(erps.<span class="bu">float</span>())</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the loss</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> loss_fn(y_hat, lbls.<span class="bu">float</span>().reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Loss: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(loss))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 4.591439723968506</code></pre>
</div>
</div>
<p>Once the loss is calculated, we can use it to update the parameters of our logistic model. In our scratch built version, we had derived the gradient of loss with respect to each parameter (<span class="math inline">\frac{\partial{L}}{\partial{w}}</span> and <span class="math inline">\frac{\partial{L}}{\partial{b}}</span>), which allowed us to explicitly calculate these changes. This is alright for simple decoders, like the logistic model, but becomes prohibitive with more complex ones. Since PyTorch is designed to handle such complex models, it can automatically calculate the gradient for each parameter. The details of this, or the deeper workings of PyTorch as a computation graph engine, are beyond the scope of this course, but for a good overview check out the official PyTorch documentation on <a href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/"><em>autograd</em></a><em>.</em> This process is known as automatic differentiation.</p>
<p>What you should know about automatic differentiation in PyTorch is that each function and tensor can have an associated gradient. Indeed, to plot the tensors above we had to remove this gradient using the <code>.detach()</code> method. As you chain tensors and functions together, like we do with <code>nn.Sequential</code>, these gradients can be used to calculate the derivative of each parameter with respect to the loss. which propagates backward for each model parameter. To see the parameters for our logistic model, you just need to call its <code>.parameters()</code> method.</p>
<div id="48145267" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print the parameters of the logistic model</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(log_mdl.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>[Parameter containing:
 tensor([[-0.2243]], requires_grad=True),
 Parameter containing:
 tensor([0.8938], requires_grad=True)]</code></pre>
</div>
</div>
<p>Note that the tensor for each parameter has a <code>requires_grad=True</code>, so they are subject to updating. You can also inspect the value of their gradients.</p>
<div id="0bc5eeec" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The gradient associated with w: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(log_mdl[<span class="dv">0</span>].weight.grad))</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The gradient associated with b: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(log_mdl[<span class="dv">0</span>].bias.grad))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The gradient associated with w: None
The gradient associated with b: None</code></pre>
</div>
</div>
<p>They show up as <code>None</code> because no loss has been calculated yet and propagated backward towards parameters. To do that, we use the <code>backward</code> method for the loss tensor. This propagates the loss value back through the sigmoid and linear function towards the weight and bias parameters.</p>
<div id="be582efb" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The gradient associated with w: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(log_mdl[<span class="dv">0</span>].weight.grad))</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The gradient associated with b: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(log_mdl[<span class="dv">0</span>].bias.grad))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The gradient associated with w: tensor([[-19.7786]])
The gradient associated with b: tensor([0.0463])</code></pre>
</div>
</div>
<p>Now that we have the gradient of the loss with respect to weight and bias, we can update those parameters.</p>
<div id="1ef06037" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print previous values of the parameters</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model parameters before updating:</span><span class="ch">\n</span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(<span class="bu">list</span>(log_mdl.parameters())))</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co"># update parameters using the optimizer</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print the updated parameters</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model parameters after updating:</span><span class="ch">\n</span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(<span class="bu">list</span>(log_mdl.parameters())))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model parameters before updating:
[Parameter containing:
tensor([[-0.2243]], requires_grad=True), Parameter containing:
tensor([0.8938], requires_grad=True)]
Model parameters after updating:
[Parameter containing:
tensor([[-0.2046]], requires_grad=True), Parameter containing:
tensor([0.8937], requires_grad=True)]</code></pre>
</div>
</div>
<p>Notice that the model parameters change after running <code>optimizer.step</code>. To fit the model, we want to do this repeatedly across many iterations. We can use a for loop for this purpose, and call up different random batches of samples using the <code>DataLoader</code> object.</p>
<p>To start, let’s reinitialize our logistic model, loss function, and optimizer.</p>
<div id="1d0965e0" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a linear transformation with 1 input, 1 output, and a bias</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>lin_trans <span class="op">=</span> torch.nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>log_trans <span class="op">=</span> torch.nn.Sigmoid()</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>log_mdl <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    lin_trans,</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>    log_trans</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize binary cross-entropy loss function and SGD optimizer</span></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.BCELoss()</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(log_mdl.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we used stochastic gradient descent to optimize our logistic model across 200 epochs of our data. Each epoch is a run through all the batches in our dataset.</p>
<div id="292a3317" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> []</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_pred and acc reflect the performance for the entire data set that we originally imported</span></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> log_mdl(torch.tensor(X[:,np.newaxis].astype(np.float32))).detach().numpy()<span class="op">&gt;</span><span class="fl">0.5</span></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>    acc.append(np.mean(y_pred.ravel() <span class="op">==</span> y.ravel())<span class="op">*</span><span class="dv">100</span>)</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> erp, lbl <span class="kw">in</span> erp_dl:</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> log_mdl(erp.<span class="bu">float</span>())</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_hat, lbl.<span class="bu">float</span>().reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">%</span><span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Epoch </span><span class="sc">{}</span><span class="st">: loss = </span><span class="sc">{:.4f}</span><span class="st">, accuracy = </span><span class="sc">{:.2f}</span><span class="st">%'</span>.<span class="bu">format</span>(i, loss, acc[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>ax.plot(acc)</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Accuracy (%)'</span>)</span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Training accuracy'</span>)</span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="dv">0</span>, <span class="dv">105</span>])</span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a>ax.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0: loss = 0.9356, accuracy = 33.33%
Epoch 10: loss = 1.2235, accuracy = 74.07%
Epoch 20: loss = 0.3909, accuracy = 85.19%
Epoch 30: loss = 0.5118, accuracy = 87.04%
Epoch 40: loss = 0.6842, accuracy = 77.78%
Epoch 50: loss = 0.3293, accuracy = 87.04%
Epoch 60: loss = 0.5359, accuracy = 81.48%
Epoch 70: loss = 0.2846, accuracy = 77.78%
Epoch 80: loss = 0.6533, accuracy = 59.26%
Epoch 90: loss = 0.4187, accuracy = 87.04%
Epoch 100: loss = 0.6262, accuracy = 87.04%
Epoch 110: loss = 0.3231, accuracy = 87.04%
Epoch 120: loss = 0.2624, accuracy = 87.04%
Epoch 130: loss = 0.3069, accuracy = 85.19%
Epoch 140: loss = 0.4511, accuracy = 85.19%
Epoch 150: loss = 0.4121, accuracy = 83.33%
Epoch 160: loss = 0.2714, accuracy = 87.04%
Epoch 170: loss = 1.2468, accuracy = 85.19%
Epoch 180: loss = 0.3626, accuracy = 85.19%
Epoch 190: loss = 0.1619, accuracy = 83.33%</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-59-output-2.png" width="593" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Performance at asymptote is comparable to what we got with Scikit learn and our scratch built code. How do the model parameters, <span class="math inline">w</span> and <span class="math inline">b</span>, compare across the different fits?</p>
<div id="e70af9f5" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>w_pt <span class="op">=</span> <span class="bu">float</span>(lin_trans.weight.detach())</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>b_pt <span class="op">=</span> <span class="bu">float</span>(lin_trans.bias.detach())</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Scikit learn model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_sk, b_sk))</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Scatch model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(lm_scratch.w, lm_scratch.b))</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'PyTorch model parameters: w=</span><span class="sc">{:.2f}</span><span class="st">, b=</span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(w_pt, b_pt))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Scikit learn model parameters: w=0.05, b=-1.86
Scatch model parameters: w=0.07, b=-1.98
PyTorch model parameters: w=0.09, b=-2.15</code></pre>
</div>
</div>
<p>Since the decision threshold for decoding whether a trial had a cue presented depends on both <span class="math inline">w</span> and <span class="math inline">b</span>, it should be easier to compare the fits by plotting the curves of their respective logistic models.</p>
<div id="0f262dc1" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a plot with the three logistic fitted models overlaid</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(np.<span class="bu">min</span>(X), np.<span class="bu">max</span>(X), <span class="dv">100</span>)</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>ax.plot(X, y, <span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Data'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>ax.plot(x_vals, logistic(x_vals, w_sk, b_sk), color<span class="op">=</span><span class="st">'tab:blue'</span>,label<span class="op">=</span><span class="st">'Scikit-learn'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>ax.plot(x_vals, logistic(x_vals, lm_scratch.w, lm_scratch.b), color<span class="op">=</span><span class="st">'tab:orange'</span>,label<span class="op">=</span><span class="st">'Scratch'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>ax.plot(x_vals, logistic(x_vals, w_pt, b_pt), color<span class="op">=</span><span class="st">'tab:green'</span>, label<span class="op">=</span><span class="st">'PyTorch'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>ax.axvline(<span class="op">-</span>b_sk<span class="op">/</span>w_sk, color<span class="op">=</span><span class="st">'tab:blue'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>ax.axvline(<span class="op">-</span>lm_scratch.b<span class="op">/</span>lm_scratch.w, color<span class="op">=</span><span class="st">'tab:orange'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>ax.axvline(<span class="op">-</span>b_pt<span class="op">/</span>w_pt, color<span class="op">=</span><span class="st">'tab:green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'$\hat</span><span class="sc">{y}</span><span class="vs">$'</span>)</span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Logistic models'</span>)</span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb102-15"><a href="#cb102-15" aria-hidden="true" tabindex="-1"></a>ax.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notes4_files/figure-html/cell-61-output-1.png" width="600" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Perhaps unsurprisingly, given that the underlying math is the similar across all implementations, they have all converged on approximately the same decision boundary.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>