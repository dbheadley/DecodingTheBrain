{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayesian decoder\n",
    "Generative classifiers\n",
    "\n",
    "Conditional probabilities\n",
    "\n",
    "Tuning curves to probabilities\n",
    "\n",
    "Bayes rule\n",
    "\n",
    "Naive bayes classifier with numpy\n",
    "\n",
    "Naive bayes classifier with sklearn\n",
    "\n",
    "Noise correlation and normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative vs. discriminative classifiers\n",
    "\n",
    "So far we have used logistic regression classifiers to do our decoding. These take a set of features and use them to predict the probability of the event class that generated it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probability\n",
    "\n",
    "We already mentioned conditional probabilities when discussing how to interpret the logistic function. In that case, the logisitic function reflects the probability of a certain event occurring given the features in our data, $p(y|x)$, where $y$ is the event and $x$ are the features. Across all possible classes there will be a different $p(y_k|x)$, and the sum of those probabilities across all possible classes will be 1:\n",
    "\n",
    "$$ \\sum_{k=1}^{K}{p(y_k|x)} = 1 $$\n",
    "\n",
    "When evaluating the probability across all classes, the $x$ is kept constant. \n",
    "\n",
    "The general way to express conditional probability is:\n",
    "\n",
    "$$ p(y|x) = \\frac{p(y,x)}{p(x)} \\tag{eq. 1} $$\n",
    "\n",
    "Here $p(y,x)$ is the probability that particular instances of $y$ and $x$ ever co-occur. If we divide that by the probability of $x$ occurring, $p(x)$, then we have the probability of $y$ *conditional* on $x$. That is, it is the probability of $y$ for all instances when $x$ is of a particular value.\n",
    "\n",
    "When $y$ and $x$ are conditionally independent of one another then $p(y|x)=p(y)p(x)$. This just means that the chance $y$ occurring with $x$ will equal to the probability that either of those happen in general (irrespective of each other). This is what is meant by conditional independence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' theorem\n",
    "\n",
    "Given equation 1 and a little algebra, we can establish a fundamental relationship between conditional probabilities known as Bayes' rule or theorem. Note that swapping $x$ and $y$, we get the equation $p(x|y)=\\frac{p(x,y)}{p(y)}$. Also note that $p(y,x) = p(x,y)$. With these facts, we can write:\n",
    "\n",
    "$$ \\begin{align}\n",
    "    p(y|x)p(x) &= p(y,x) = p(x,y) = p(x|y)p(y) \\notag \\\\\n",
    "    p(y|x)p(x) &= p(x|y)p(y) \\notag \\\\\n",
    "    p(y|x) &= \\frac{p(x|y)p(y)}{p(x)} \\tag{2} \\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "This powerful relationship shows how to go from the conditional probability of $x$ given $y$, to that of $y$ given $x$. \n",
    "\n",
    "Why is this relaiontship important? Returning to decoders, their goal is to estimate the probability of behavioral or stimulus events, $y_k$, given patterns in brain activity, $x$. In the previous lectures we estimated this probability, $p(y_k|x)$ using logistic regression. The event $k$ with the largest probability was then chosen as the one most likely to have occured. However, Bayes' theorem offers an alternative approach. Instead we can derive that by measuring the probability of observing our brain data given that event $y_k$ occured and use that, along with our knowledge about how often $y_k$ occurs.\n",
    "\n",
    "However, we do have "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decode_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
